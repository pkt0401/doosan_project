import streamlit as st
import pandas as pd
import numpy as np
import faiss
import re
import os
import io
from PIL import Image
from sklearn.model_selection import train_test_split
from openai import OpenAI

# ----------------- ì‹œìŠ¤í…œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ -----------------
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_phase": "ìœ„í—˜ì„± í‰ê°€ & ê°œì„ ëŒ€ì±…",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": (
            "Doosan Enerbility AI Risk AssessmentëŠ” êµ­ë‚´ ë° í•´ì™¸ ê±´ì„¤í˜„ì¥ì˜ 'ìˆ˜ì‹œ ìœ„í—˜ì„± í‰ê°€' ë° "
            "'ë…¸ë™ë¶€ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€'ë¥¼ í•™ìŠµí•˜ì—¬ ê°œë°œëœ ìë™ ìœ„í—˜ì„±í‰ê°€ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. "
            "ìƒì„±ëœ ìœ„í—˜ì„± í‰ê°€ëŠ” ë°˜ë“œì‹œ ìˆ˜ì‹œ ìœ„í—˜ì„±í‰ê°€ ì‹¬ì˜íšŒë¥¼ í†µí•´ ê²€ì¦ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        ),
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase_features": (
            "#### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”\n"
            "- ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ\n"
            "- ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡ (ì˜ì–´ë¡œ ë‚´ë¶€ ì‹¤í–‰)\n"
            "- ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ (ì˜ì–´ ë‚´ë¶€ ì²˜ë¦¬ â†’ ìµœì¢… ì¶œë ¥ ë²ˆì—­)\n"
            "- LLM ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì • (ì˜ì–´ ë‚´ë¶€ ì‹¤í–‰)\n"
            "- ìœ„í—˜ë“±ê¸‰(A-E) ìë™ ì‚°ì •\n\n"
            "#### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±\n"
            "- ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (ì˜ì–´ ë‚´ë¶€ ì‹¤í–‰)\n"
            "- ë‹¤êµ­ì–´(í•œêµ­ì–´/ì˜ì–´/ì¤‘êµ­ì–´) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›\n"
            "- ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„\n"
            "- ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"
        ),
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "include_similar_cases": "ìœ ì‚¬ ì‚¬ë¡€ í¬í•¨",
        "result_language": "ê²°ê³¼ ì–¸ì–´",
        "run_assessment": "ğŸš€ ìœ„í—˜ì„± í‰ê°€ ì‹¤í–‰",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "performing_assessment": "ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¤‘...",
        "phase1_results": "ğŸ“‹ Phase 1: ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼",
        "work_activity": "ì‘ì—…í™œë™",
        "predicted_hazard": "ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸",
        "risk_grade_display": "ìœ„í—˜ë“±ê¸‰",
        "t_value_display": "Tê°’",
        "similar_cases_section": "ğŸ” ìœ ì‚¬í•œ ì‚¬ë¡€",
        "case_number": "ì‚¬ë¡€",
        "phase2_results": "ğŸ› ï¸ Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ìœ„í—˜ë„ ê°œì„  ê²°ê³¼",
        "comparison_columns": ["í•­ëª©", "ê°œì„  ì „", "ê°œì„  í›„"],
        "risk_reduction_label": "ìœ„í—˜ ê°ì†Œìœ¨ (RRR)",
        "risk_visualization": "ğŸ“Š ìœ„í—˜ë„ ë³€í™” ì‹œê°í™”",
        "before_improvement": "ê°œì„  ì „",
        "after_improvement": "ê°œì„  í›„",
        "grade_label": "ë“±ê¸‰",
        "download_results": "ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (Excel)",
        # Excel íƒ­
        "excel_export": "ğŸ“¥ ê²°ê³¼ Excel ë‹¤ìš´ë¡œë“œ",
        # ì»¬ëŸ¼ ë¼ë²¨
        "col_activity_header": "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence",
        "col_hazard_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazarous Factors",
        "col_ehs_header": "EHS",
        "col_risk_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_risk_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity",
        "col_control_header": "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures",
        "col_incharge_header": "ê°œì„ ë‹´ë‹¹ì In Charge",
        "col_duedate_header": "ê°œì„ ì¼ì Correction Due Date",
        "col_after_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_after_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity"
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_phase": "Risk Assessment & Improvement",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": (
            "Doosan Enerbility AI Risk Assessment is an automated program trained on on-demand risk-assessment reports "
            "from domestic and overseas construction sites and major-accident cases compiled by Korea's Ministry of Employment "
            "and Labor. Please ensure that every generated assessment is reviewed and approved by the On-Demand Risk Assessment "
            "Committee before it is used."
        ),
        "features_title": "System Features and Components",
        "phase_features": (
            "#### Phase 1: Risk Assessment Automation\n"
            "- Learning risk assessment data per work activity\n"
            "- Automatic hazard prediction when work activities are entered (internal: English)\n"
            "- Similar case search & display (internal: English â†’ final: translation)\n"
            "- LLM-based risk level (frequency, intensity, T) measurement (internal: English)\n"
            "- Automatic risk grade (A-E) calculation\n\n"
            "#### Phase 2: Automatic Generation of Improvement Measures\n"
            "- Customized improvement measures generation (internal: English)\n"
            "- Multilingual (Korean/English/Chinese) improvement measure support\n"
            "- Automatic comparative analysis before/after improvement\n"
            "- Database of optimal improvement measures per process"
        ),
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset",
        "load_data_btn": "Load Data and Configure Index",
        "api_key_warning": "Please enter an OpenAI API key to continue.",
        "data_loading": "Loading data and configuring index...",
        "demo_limit_info": "For demo purposes, only embedding {max_texts} texts. In a real environment, process all data.",
        "data_load_success": "Data load and index configuration complete! (Total {max_texts} items processed)",
        "load_first_warning": "Please click [Load Data and Configure Index] first.",
        "activity_label": "Work Activity:",
        "include_similar_cases": "Include Similar Cases",
        "result_language": "Result Language",
        "run_assessment": "ğŸš€ Run Risk Assessment",
        "activity_warning": "Please enter a work activity.",
        "performing_assessment": "Performing risk assessment...",
        "phase1_results": "ğŸ“‹ Phase 1: Risk Assessment Results",
        "work_activity": "Work Activity",
        "predicted_hazard": "Predicted Hazard",
        "risk_grade_display": "Risk Grade",
        "t_value_display": "T Value",
        "similar_cases_section": "ğŸ” Similar Cases",
        "case_number": "Case",
        "phase2_results": "ğŸ› ï¸ Phase 2: Improvement Measures Results",
        "improvement_plan_header": "Improvement Plan",
        "risk_improvement_header": "Risk Improvement Results",
        "comparison_columns": ["Item", "Before Improvement", "After Improvement"],
        "risk_reduction_label": "Risk Reduction Rate (RRR)",
        "risk_visualization": "ğŸ“Š Risk Level Change Visualization",
        "before_improvement": "Before Improvement",
        "after_improvement": "After Improvement",
        "grade_label": "Grade",
        "download_results": "ğŸ’¾ Download Results (Excel)",
        # Excel íƒ­
        "excel_export": "ğŸ“¥ Download Excel Report",
        # ì»¬ëŸ¼ ë¼ë²¨
        "col_activity_header": "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence",
        "col_hazard_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazarous Factors",
        "col_ehs_header": "EHS",
        "col_risk_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_risk_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity",
        "col_control_header": "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures",
        "col_incharge_header": "ê°œì„ ë‹´ë‹¹ì In Charge",
        "col_duedate_header": "ê°œì„ ì¼ì Correction Due Date",
        "col_after_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_after_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity"
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_phase": "é£é™©è¯„ä¼° & æ”¹è¿›æªæ–½",
        "overview_header": "åŸºäºLLMçš„é£é™©è¯„ä¼°ç³»ç»Ÿ",
        "overview_text": (
            "Doosan Enerbility AI é£é™©è¯„ä¼°ç³»ç»Ÿæ˜¯ä¸€æ¬¾è‡ªåŠ¨åŒ–é£é™©è¯„ä¼°ç¨‹åºï¼ŒåŸºäºå›½å†…å¤–æ–½å·¥ç°åœºçš„'ä¸´æ—¶é£é™©è¯„ä¼°'æ•°æ®åŠéŸ©å›½åŠ³å·¥éƒ¨ "
            "é‡å¤§äº‹æ•…æ¡ˆä¾‹è®­ç»ƒå¼€å‘è€Œæˆã€‚ç”Ÿæˆçš„é£é™©è¯„ä¼°ç»“æœå¿…é¡»ç»è¿‡ä¸´æ—¶é£é™©è¯„ä¼°å®¡è®®å§”å‘˜ä¼šçš„å®¡æ ¸åæ–¹å¯ä½¿ç”¨ã€‚"
        ),
        "features_title": "ç³»ç»Ÿç‰¹ç‚¹å’Œç»„ä»¶",
        "phase_features": (
            "#### ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°è‡ªåŠ¨åŒ–\n"
            "- æŒ‰å·¥ä½œæ´»åŠ¨å­¦ä¹ é£é™©è¯„ä¼°æ•°æ®\n"
            "- è¾“å…¥å·¥ä½œæ´»åŠ¨æ—¶è‡ªåŠ¨é¢„æµ‹å±å®³ (å†…éƒ¨ï¼šè‹±è¯­)\n"
            "- ç›¸ä¼¼æ¡ˆä¾‹æœç´¢ä¸æ˜¾ç¤º (å†…éƒ¨ï¼šè‹±è¯­ â†’ æœ€ç»ˆï¼šç¿»è¯‘)\n"
            "- åŸºäºLLMçš„é£é™©ç­‰çº§ï¼ˆé¢‘ç‡ã€å¼ºåº¦ã€Tï¼‰æµ‹é‡ (å†…éƒ¨ï¼šè‹±è¯­)\n"
            "- è‡ªåŠ¨è®¡ç®—é£é™©ç­‰çº§(A-E)\n\n"
            "#### ç¬¬2é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½\n"
            "- å®šåˆ¶åŒ–æ”¹è¿›æªæ–½è‡ªåŠ¨ç”Ÿæˆ (å†…éƒ¨ï¼šè‹±è¯­)\n"
            "- å¤šè¯­è¨€ (éŸ©/è‹±/ä¸­) æ”¹è¿›æªæ–½æ”¯æŒ\n"
            "- è‡ªåŠ¨æ¯”è¾ƒæ”¹è¿›å‰åé£é™©ç­‰çº§\n"
            "- æŒ‰å·¥åºç®¡ç†æœ€ä¼˜æ”¹è¿›æªæ–½æ•°æ®åº“"
        ),
        "api_key_label": "è¾“å…¥ OpenAI API å¯†é’¥ï¼š",
        "dataset_label": "é€‰æ‹©æ•°æ®é›†",
        "load_data_btn": "åŠ è½½æ•°æ®å¹¶é…ç½®ç´¢å¼•",
        "api_key_warning": "è¯·è¾“å…¥ OpenAI API å¯†é’¥ä»¥ç»§ç»­ã€‚",
        "data_loading": "æ­£åœ¨åŠ è½½æ•°æ®å¹¶é…ç½®ç´¢å¼•...",
        "demo_limit_info": "æ¼”ç¤ºç”¨é€”ä»…åµŒå…¥ {max_texts} ä¸ªæ–‡æœ¬ã€‚å®é™…ç¯å¢ƒåº”å¤„ç†æ‰€æœ‰æ•°æ®ã€‚",
        "data_load_success": "æ•°æ®åŠ è½½ä¸ç´¢å¼•é…ç½®å®Œæˆï¼(å…±å¤„ç† {max_texts} é¡¹ç›®)",
        "load_first_warning": "è¯·å…ˆç‚¹å‡» [åŠ è½½æ•°æ®å¹¶é…ç½®ç´¢å¼•]ã€‚",
        "activity_label": "å·¥ä½œæ´»åŠ¨ï¼š",
        "include_similar_cases": "åŒ…æ‹¬ç›¸ä¼¼æ¡ˆä¾‹",
        "result_language": "ç»“æœè¯­è¨€",
        "run_assessment": "ğŸš€ è¿è¡Œé£é™©è¯„ä¼°",
        "activity_warning": "è¯·è¾“å…¥å·¥ä½œæ´»åŠ¨ã€‚",
        "performing_assessment": "æ­£åœ¨è¿›è¡Œé£é™©è¯„ä¼°...",
        "phase1_results": "ğŸ“‹ ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°ç»“æœ",
        "work_activity": "å·¥ä½œæ´»åŠ¨",
        "predicted_hazard": "é¢„æµ‹å±å®³",
        "risk_grade_display": "é£é™©ç­‰çº§",
        "t_value_display": "T å€¼",
        "similar_cases_section": "ğŸ” ç›¸ä¼¼æ¡ˆä¾‹",
        "case_number": "æ¡ˆä¾‹",
        "phase2_results": "ğŸ› ï¸ ç¬¬2é˜¶æ®µï¼šæ”¹è¿›æªæ–½ç»“æœ",
        "improvement_plan_header": "æ”¹è¿›æªæ–½",
        "risk_improvement_header": "é£é™©æ”¹è¿›ç»“æœ",
        "comparison_columns": ["é¡¹ç›®", "æ”¹è¿›å‰", "æ”¹è¿›å"],
        "risk_reduction_label": "é£é™©é™ä½ç‡ (RRR)",
        "risk_visualization": "ğŸ“Š é£é™©ç­‰çº§å˜åŒ–å¯è§†åŒ–",
        "before_improvement": "æ”¹è¿›å‰",
        "after_improvement": "æ”¹è¿›å",
        "grade_label": "ç­‰çº§",
        "download_results": "ğŸ’¾ ä¸‹è½½ç»“æœ (Excel)",
        # Excel íƒ­
        "excel_export": "ğŸ“¥ ä¸‹è½½ Excel æŠ¥è¡¨",
        # ì»¬ëŸ¼ ë¼ë²¨
        "col_activity_header": "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence",
        "col_hazard_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazarous Factors",
        "col_ehs_header": "EHS",
        "col_risk_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_risk_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity",
        "col_control_header": "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures",
        "col_incharge_header": "ê°œì„ ë‹´ë‹¹ì In Charge",
        "col_duedate_header": "ê°œì„ ì¼ì Correction Due Date",
        "col_after_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_after_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity"
    }
}

# ----------------- í˜ì´ì§€ ìŠ¤íƒ€ì¼ -----------------
st.set_page_config(page_title="AI Risk Assessment", page_icon="ğŸ› ï¸", layout="wide")
st.markdown(
    """
    <style>
    .main-header{font-size:2.5rem;color:#1E88E5;text-align:center;margin-bottom:1rem}
    .sub-header{font-size:1.8rem;color:#0D47A1;margin-top:2rem;margin-bottom:1rem}
    .similar-case{background-color:#f1f8e9;border-radius:8px;padding:12px;margin-bottom:8px;border-left:4px solid #689f38}
    </style>
    """,
    unsafe_allow_html=True
)

# ----------------- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -----------------
ss = st.session_state
for key, default in {
    "language": "Korean",            # í™”ë©´ í‘œì‹œ ì–¸ì–´
    "index": None,                   # FAISS ì¸ë±ìŠ¤
    "embeddings": None,              # ì„ë² ë”© í–‰ë ¬
    "retriever_pool_df": None,       # ìœ ì‚¬ ì‚¬ë¡€ í›„ë³´ ë°ì´í„°í”„ë ˆì„ (ì›ë³¸ í•œêµ­ì–´)
    "last_assessment": None          # ë§ˆì§€ë§‰ í‰ê°€ ê²°ê³¼ ì €ì¥ìš©
}.items():
    if key not in ss:
        ss[key] = default

# ----------------- ì–¸ì–´ ì„ íƒ -----------------
col0, colLang = st.columns([6, 1])
with colLang:
    lang = st.selectbox(
        "ì–¸ì–´ ì„ íƒ",
        list(system_texts.keys()),
        index=list(system_texts.keys()).index(ss.language),
        label_visibility="hidden"
    )
    ss.language = lang
texts = system_texts[ss.language]

# ----------------- í—¤ë” -----------------
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# ----------------- íƒ­ êµ¬ì„± -----------------
tabs = st.tabs([texts["tab_overview"], texts["tab_phase"]])

# -----------------------------------------------------------------------------  
# ---------------- Utility Functions ------------------------------------------
# -----------------------------------------------------------------------------  

def determine_grade(value: int) -> str:
    """ìœ„í—˜ë„ ë“±ê¸‰ ë¶„ë¥˜ (Tê°’ ê¸°ì¤€)"""
    if 16 <= value <= 25:
        return 'A'
    if 10 <= value <= 15:
        return 'B'
    if 5 <= value <= 9:
        return 'C'
    if 3 <= value <= 4:
        return 'D'
    if 1 <= value <= 2:
        return 'E'
    return 'Unknown' if ss.language != 'Korean' else 'ì•Œ ìˆ˜ ì—†ìŒ'

def get_grade_color(grade: str) -> str:
    """ë“±ê¸‰ë³„ ìƒ‰ìƒ ë°˜í™˜"""
    colors = {
        'A': '#ff1744',
        'B': '#ff9800',
        'C': '#ffc107',
        'D': '#4caf50',
        'E': '#2196f3',
    }
    return colors.get(grade, '#808080')

def compute_rrr(original_t: int, improved_t: int) -> float:
    """ìœ„í—˜ ê°ì†Œìœ¨(RRR) ê³„ì‚°"""
    if original_t == 0:
        return 0.0
    return ((original_t - improved_t) / original_t) * 100

@st.cache_data(show_spinner=False)
def load_data(selected_dataset_name: str) -> pd.DataFrame:
    """
    ì—‘ì…€(.xlsx/.xls) íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬ (í•œêµ­ì–´ ì¹¼ëŸ¼ ê¸°ì¤€).
    - ë‚´ë¶€ì—ëŠ” í•œêµ­ì–´ ë°ì´í„°í”„ë ˆì„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    - í•„ìš”í•œ ì¹¼ëŸ¼ëª…(KO)ì„ ì˜ì–´ prompt ì‘ì„± ì‹œ ë²ˆì—­í•˜ì—¬ ì‚¬ìš©í•˜ê±°ë‚˜,
      ì§ì ‘ ì˜ë¬¸ ë§¤í•‘(í›„ìˆ )ì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        dataset_mapping = {
            "ê±´ì¶•": "ê±´ì¶•", "Architecture": "ê±´ì¶•",
            "í† ëª©": "í† ëª©", "Civil": "í† ëª©",
            "í”ŒëœíŠ¸": "í”ŒëœíŠ¸", "Plant": "í”ŒëœíŠ¸"
        }
        actual_filename = dataset_mapping.get(selected_dataset_name, selected_dataset_name)

        # .xlsx ë¨¼ì € ì‹œë„ â†’ .xls
        if os.path.exists(f"{actual_filename}.xlsx"):
            try:
                df = pd.read_excel(f"{actual_filename}.xlsx", engine='openpyxl')
            except Exception as e1:
                try:
                    df = pd.read_excel(f"{actual_filename}.xlsx", engine='xlrd')
                except Exception as e2:
                    st.warning(f"Excel íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {actual_filename}.xlsx")
                    st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    return create_sample_data()
        elif os.path.exists(f"{actual_filename}.xls"):
            try:
                df = pd.read_excel(f"{actual_filename}.xls", engine='xlrd')
            except Exception as e:
                st.warning(f"Excel íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return create_sample_data()
        else:
            st.info(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {actual_filename}.xlsx ë˜ëŠ” {actual_filename}.xls")
            st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return create_sample_data()

        # ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ ì œê±°
        if "ì‚­ì œ Del" in df.columns:
            df.drop(["ì‚­ì œ Del"], axis=1, inplace=True)
        df = df.dropna(how='all')

        # ì¹¼ëŸ¼ëª… í•œê¸€+ì˜ë¬¸ ë§¤í•‘ (ì›ë³¸ì— \nìœ¼ë¡œ í˜¼ì¬ëœ ê²½ìš°ë„ ì²˜ë¦¬)
        column_mapping = {
            "ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect": "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ\nCorrective Action": "ê°œì„ ëŒ€ì±…"
        }
        df.rename(columns=column_mapping, inplace=True)

        # ìˆ«ìí˜• ë³€í™˜
        for col in ["ë¹ˆë„", "ê°•ë„"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # ê¸°ë³¸ê°’ ì±„ìš°ê¸°
        if 'ë¹ˆë„' not in df.columns:
            df['ë¹ˆë„'] = 3
        if 'ê°•ë„' not in df.columns:
            df['ê°•ë„'] = 3

        df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
        df["ë“±ê¸‰"] = df["T"].apply(determine_grade)

        if "ê°œì„ ëŒ€ì±…" not in df.columns:
            alt_cols = [c for c in df.columns if "ê°œì„ " in c or "ëŒ€ì±…" in c or "Corrective" in c]
            if alt_cols:
                df.rename(columns={alt_cols[0]: "ê°œì„ ëŒ€ì±…"}, inplace=True)
            else:
                df["ê°œì„ ëŒ€ì±…"] = "ì•ˆì „ êµìœ¡ ì‹¤ì‹œ ë° ë³´í˜¸êµ¬ ì°©ìš©"

        # í•„ìš”í•œ ì¹¼ëŸ¼ë§Œ ì¶”ì¶œ
        required_cols = [
            "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ë¹ˆë„",
            "ê°•ë„",
            "T",
            "ë“±ê¸‰",
            "ê°œì„ ëŒ€ì±…"
        ]
        final_cols = [col for col in required_cols if col in df.columns]
        df = df[final_cols]

        df = df.fillna({
            "ì‘ì—…í™œë™ ë° ë‚´ìš©": "ì¼ë°˜ ì‘ì—…",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": "ì¼ë°˜ì  ìœ„í—˜",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": "ë¶€ìƒ",
            "ê°œì„ ëŒ€ì±…": "ì•ˆì „ ì¡°ì¹˜ ìˆ˜í–‰"
        })
        return df

    except Exception as e:
        st.warning(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return create_sample_data()

def create_sample_data() -> pd.DataFrame:
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±(í…ŒìŠ¤íŠ¸ìš©)"""
    data = {
        "ì‘ì—…í™œë™ ë° ë‚´ìš©": [
            "ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "ì½˜í¬ë¦¬íŠ¸/CMU ë¸”ë¡ ì„¤ì¹˜ ì‘ì—…",
            "êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…",
            "ê³ ì†Œ ì‘ì—…ëŒ€ë¥¼ ì´ìš©í•œ ì™¸ë²½ ì‘ì—…",
            "ìš©ì ‘ ì‘ì—…"
        ],
        "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": [
            "ë‹¤ì¤‘ ì¸ì–‘ìœ¼ë¡œ ì¸í•œ ì ì¬ë¬¼ ë‚™í•˜",
            "ë¶ˆì¶©ë¶„í•œ ì‘ì—… ë°œíŒìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°",
            "ì•ˆì „ëŒ€ ë¯¸ì°©ìš©ìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "ìš©ì ‘ í„ ë° í™”ì¬ ìœ„í—˜"
        ],
        "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": [
            "íƒ€ë°•ìƒ", "ê³¨ì ˆ", "ë§¤ëª°", "ì¶”ë½ì‚¬", "í™”ìƒ"
        ],
        "ë¹ˆë„": [3, 3, 2, 4, 2],
        "ê°•ë„": [5, 4, 5, 5, 3],
        "ê°œì„ ëŒ€ì±…": [
            "1) ë‹¤ìˆ˜ì˜ ì² ê³¨ì¬ë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬\n2) ì¹˜ìˆ˜, ì¤‘ëŸ‰, í˜•ìƒì´ ë‹¤ë¥¸ ì¬ë£Œë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬",
            "1) ë¹„ê³„ëŒ€ ëˆ„ë½ëœ ëª©íŒ ì„¤ì¹˜\n2) ì•ˆì „ëŒ€ ë¶€ì°©ì„¤ë¹„ ì„¤ì¹˜ ë° ì‚¬ìš©\n3) ë¹„ê³„ ë³€ê²½ ì‹œ íƒ€ê³µì¢… ì™¸ ì‘ì—…ì ì‘ì—… ê¸ˆì§€",
            "1) ì ì ˆí•œ ì‚¬ë©´ ê¸°ìš¸ê¸° ìœ ì§€\n2) êµ´ì°©ë©´ ë³´ê°•\n3) ì •ê¸°ì  ì§€ë°˜ ìƒíƒœ ì ê²€",
            "1) ì•ˆì „ëŒ€ ì°©ìš© ì˜ë¬´í™”\n2) ì‘ì—… ì „ ì•ˆì „êµìœ¡ ì‹¤ì‹œ\n3) ì¶”ë½ë°©ì§€ë§ ì„¤ì¹˜",
            "1) ì ì ˆí•œ í™˜ê¸°ì‹œì„¤ ì„¤ì¹˜\n2) í™”ì¬ ì˜ˆë°© ì¡°ì¹˜\n3) ë³´í˜¸êµ¬ ì°©ìš©"
        ]
    }
    df = pd.DataFrame(data)
    df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
    df["ë“±ê¸‰"] = df["T"].apply(determine_grade)
    return df

def embed_texts_with_openai(texts: list[str], api_key: str, model: str="text-embedding-3-large") -> list[list[float]]:
    """
    OpenAI APIë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ì˜ì–´ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©).
    - api_keyê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    - ì˜¤ë¥˜ ì‹œ 0 ë²¡í„°ë¡œ íŒ¨ë”©.
    """
    if not api_key:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []

    client = OpenAI(api_key=api_key)
    embeddings = []
    batch_size = 10
    for i in range(0, len(texts), batch_size):
        batch = texts[i : i + batch_size]
        processed = [str(t).replace("\n", " ").strip() for t in batch]
        try:
            resp = client.embeddings.create(
                model=model,
                input=processed
            )
            for item in resp.data:
                embeddings.append(item.embedding)
        except Exception as e:
            st.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ë°°ì¹˜ {i}): {e}")
            for _ in processed:
                embeddings.append([0.0] * 1536)
    return embeddings

def generate_with_gpt(prompt: str, api_key: str, model: str="gpt-4o", max_retries: int=3) -> str:
    """
    OpenAI APIë¥¼ ì´ìš©í•œ GPT ìƒì„±. ë‚´ë¶€ëŠ” ì˜ì–´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - prompt: ë°˜ë“œì‹œ ì˜ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ê²°ê³¼ëŠ” ì˜ì–´ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
    """
    if not api_key:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return ""
    client = OpenAI(api_key=api_key)

    sys_prompt = "You are a construction site risk assessment expert. Provide accurate and practical responses in English."
    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system",  "content": sys_prompt},
                    {"role": "user",    "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500,
                top_p=0.9
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            if attempt == max_retries - 1:
                st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜ ({attempt+1}/{max_retries}): {e}")
                return ""
            else:
                st.warning(f"GPT ì¬ì‹œë„ ì¤‘... ({attempt+1}/{max_retries})")
                continue

def translate_similar_cases(sim_docs: pd.DataFrame, target_language: str, api_key: str) -> pd.DataFrame:
    """
    ìœ ì‚¬ì‚¬ë¡€ ë°ì´í„°í”„ë ˆì„(sim_docs)ì˜ ë‘ ì¹¼ëŸ¼(ì‘ì—…í™œë™, ìœ í•´ìœ„í—˜ìš”ì¸)ì„
    â†’ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ìƒˆë¡œìš´ ì»¬ëŸ¼ 'activity_en', 'hazard_en'ì— ì €ì¥í•œ ë’¤,
    ì˜ì–´ ì „ìš© sim_docs_en ë°ì´í„°í”„ë ˆì„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - sim_docsëŠ” í•œêµ­ì–´ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì…ë‹ˆë‹¤.
    - target_languageìœ¼ë¡œ "English" ì§€ì •í•˜ë©´ ì˜ì–´ ë²ˆì—­ì„ ìˆ˜í–‰.
    """
    sim_docs_en = sim_docs.copy().reset_index(drop=True)
    sim_docs_en["activity_en"] = sim_docs_en["ì‘ì—…í™œë™ ë° ë‚´ìš©"]
    sim_docs_en["hazard_en"] = sim_docs_en["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"]

    if target_language != "English" or not api_key:
        return sim_docs_en  # ì´ë¯¸ ì˜ì–´ ì»¬ëŸ¼ì— ì›ë³¸ í•œêµ­ì–´ê°€ ë³µì‚¬ë˜ì–´ ìˆìŒ.

    # ê° í–‰ë§ˆë‹¤ GPTë¡œ ë²ˆì—­
    for idx, row in sim_docs_en.iterrows():
        try:
            # 1) ì‘ì—…í™œë™ â†’ ì˜ì–´
            act_ko = row["ì‘ì—…í™œë™ ë° ë‚´ìš©"]
            prompt_act = (
                "Translate the following construction work activity into English. "
                "Only provide the translation:\n\n" + act_ko
            )
            act_en = generate_with_gpt(prompt_act, api_key)
            if act_en:
                sim_docs_en.at[idx, "activity_en"] = act_en

            # 2) ìœ í•´ìœ„í—˜ìš”ì¸ â†’ ì˜ì–´
            haz_ko = row["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"]
            prompt_haz = (
                "Translate the following construction hazard into English. "
                "Only provide the translation:\n\n" + haz_ko
            )
            haz_en = generate_with_gpt(prompt_haz, api_key)
            if haz_en:
                sim_docs_en.at[idx, "hazard_en"] = haz_en

        except Exception:
            continue

    return sim_docs_en

def translate_output(content: str, target_language: str, api_key: str, max_retries: int=2) -> str:
    """
    ì˜ì–´ ì½˜í…ì¸ (content)ë¥¼ ì£¼ì–´ì§„ target_languageë¡œ ë²ˆì—­í•˜ì—¬ ë°˜í™˜.
    - target_languageì´ "English"ì´ë©´ ì›ë³¸ ë°˜í™˜.
    - target_languageì´ "Korean" ë˜ëŠ” "Chinese"ì´ë©´ GPTë¥¼ í˜¸ì¶œí•˜ì—¬ ë²ˆì—­.
    """
    if target_language == "English" or not api_key:
        return content

    lang_map = {"Korean": "Korean", "Chinese": "Chinese"}
    if target_language not in lang_map:
        return content

    for attempt in range(max_retries):
        try:
            prompt = f"Translate the following into {target_language}. Only provide the translation:\n\n{content}"
            translated = generate_with_gpt(prompt, api_key)
            if translated:
                return translated
        except Exception:
            if attempt == max_retries - 1:
                return content
            else:
                continue
    return content

def construct_prompt_phase1_hazard(sim_docs_en: pd.DataFrame, activity_en: str) -> str:
    """
    Phase 1: ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ì„ ìœ„í•œ ì˜ì–´ í”„ë¡¬í”„íŠ¸ ìƒì„±
    - sim_docs_en: 'activity_en', 'hazard_en' ì»¬ëŸ¼ì´ ìˆëŠ” ì˜ì–´ ë°ì´í„°í”„ë ˆì„
    - activity_en: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì‘ì—…í™œë™(ì˜ì–´ ë²ˆì—­) ë¬¸ìì—´
    """
    intro = "Below are examples of work activities and associated hazards at construction sites:\n\n"
    example_fmt = "Example {i}:\n- Work Activity: {act}\n- Hazard: {haz}\n\n"
    query_fmt = (
        "Based on the above examples, predict the main hazards for the following work activity:\n\n"
        f"Work Activity: {activity_en}\n\nPredicted Hazard: "
    )

    prompt = intro
    for i, (_, row) in enumerate(sim_docs_en.head(5).iterrows(), start=1):
        act = row["activity_en"]
        haz = row["hazard_en"]
        if pd.notna(act) and pd.notna(haz):
            prompt += example_fmt.format(i=i, act=act, haz=haz)

    prompt += query_fmt
    return prompt

def construct_prompt_phase1_risk(sim_docs_en: pd.DataFrame, activity_en: str, hazard_en: str) -> str:
    """
    Phase 1: ìœ„í—˜ë„ í‰ê°€ë¥¼ ìœ„í•œ ì˜ì–´ í”„ë¡¬í”„íŠ¸ ìƒì„±
    - sim_docs_en: 'activity_en', 'hazard_en', 'ë¹ˆë„', 'ê°•ë„' ì»¬ëŸ¼ ì¡´ì¬
    - activity_en, hazard_en: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í™œë™Â·ìœ„í—˜(ì˜ì–´)
    """
    intro = (
        "Construction site risk assessment criteria:\n"
        "- Frequency(1-5): 1=Very Rare, 2=Rare, 3=Occasional, 4=Frequent, 5=Very Frequent\n"
        "- Intensity(1-5): 1=Minor Injury, 2=Light Injury, 3=Moderate Injury, 4=Serious Injury, 5=Fatality\n"
        "- T-value = Frequency Ã— Intensity\n\n"
        "Reference Cases:\n\n"
    )
    example_fmt = "Case {i}:\nInput: {inp}\nAssessment: Frequency={freq}, Intensity={intensity}, T-value={t}\n\n"
    json_format = '{"frequency": number, "intensity": number, "T": number}'
    query_fmt = (
        "Based on the above criteria and cases, assess the following:\n\n"
        f"Work Activity: {activity_en}\n"
        f"Hazard: {hazard_en}\n\n"
        f"Respond exactly in this JSON format:\n{json_format}"
    )

    prompt = intro
    # ìµœëŒ€ 3ê°œ ì˜ˆì‹œ ì‚¬ìš©
    count = 0
    for _, row in sim_docs_en.head(5).iterrows():
        try:
            inp = f"{row['activity_en']} - {row['hazard_en']}"
            freq = int(row["ë¹ˆë„"])
            intensity = int(row["ê°•ë„"])
            t_val = freq * intensity
            count += 1
            prompt += example_fmt.format(i=count, inp=inp, freq=freq, intensity=intensity, t=t_val)
            if count >= 3:
                break
        except Exception:
            continue

    prompt += query_fmt
    return prompt

def parse_gpt_output_phase1(gpt_output: str) -> tuple[int, int, int]:
    """
    Phase 1 ìœ„í—˜ë„ í‰ê°€ JSON íŒŒì‹± (ì˜ì–´ ê²°ê³¼)
    - ì˜ˆì‹œ: {"frequency": 3, "intensity": 4, "T": 12}
    - ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ, ì¶œë ¥ ë‚´ë¶€ì˜ ìˆ«ì 2ê°œ ì´ìƒì„ freq, intensityë¡œ ê°„ì£¼í•˜ì—¬ T ê³„ì‚°
    """
    pattern = r'\{"frequency":\s*([1-5]),\s*"intensity":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    match = re.search(pattern, gpt_output)
    if match:
        freq = int(match.group(1))
        intensity = int(match.group(2))
        t_val = int(match.group(3))
        return freq, intensity, t_val

    # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ìˆ«ì 2ê°œ ì´ìƒ ì¶”ì¶œ
    nums = re.findall(r'\b([1-5])\b', gpt_output)
    if len(nums) >= 2:
        freq = int(nums[0])
        intensity = int(nums[1])
        return freq, intensity, freq * intensity

    return None

def construct_prompt_phase2(sim_docs_en: pd.DataFrame, activity_en: str, hazard_en: str,
                             freq: int, intensity: int, t_val: int) -> str:
    """
    Phase 2: ê°œì„ ëŒ€ì±… ìƒì„±ì„ ìœ„í•œ ì˜ì–´ í”„ë¡¬í”„íŠ¸ ìƒì„±
    - sim_docs_en: 'activity_en','hazard_en','ê°œì„ ëŒ€ì±…'(í•œêµ­ì–´)ì„ í¬í•¨. ë‹¨, ê°œì„ ëŒ€ì±… ìì²´ë„ ë²ˆì—­í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.
    - activity_en, hazard_en: ì˜ì–´
    - freq, intensity, t_val: Phase1 ê²°ê³¼ (ì˜ì–´)
    """
    example_section = ""
    count = 0
    for _, row in sim_docs_en.head(5).iterrows():
        try:
            # ì˜ì–´ë¡œ ëœ ê°œì„ ëŒ€ì±…ì´ ì—†ìœ¼ë¯€ë¡œ, "ê°œì„ ëŒ€ì±…" ì¹¼ëŸ¼(í•œêµ­ì–´)ì„ ë²ˆì—­í•˜ì—¬ ì‚¬ìš©
            plan_ko = row["ê°œì„ ëŒ€ì±…"]
            prompt_plan_trans = (
                "Translate the following safety improvement measures into English. "
                "Keep the numbered format. Only provide the translation:\n\n" + plan_ko
            )
            plan_en = generate_with_gpt(prompt_plan_trans, api_key)
            if not plan_en:
                plan_en = plan_ko  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸(Korean) ì‚¬ìš©

            orig_freq = int(row["ë¹ˆë„"])
            orig_intensity = int(row["ê°•ë„"])
            orig_t = orig_freq * orig_intensity
            new_freq = max(1, orig_freq - 1)
            new_intensity = max(1, orig_intensity - 1)
            new_t = new_freq * new_intensity
            count += 1
            example_section += (
                f"Example {count}:\n"
                f"Input Work Activity: {row['activity_en']}\n"
                f"Input Hazard: {row['hazard_en']}\n"
                f"Original Frequency: {orig_freq}\n"
                f"Original Intensity: {orig_intensity}\n"
                f"Original T-value: {orig_t}\n"
                "Output (Improvement Plan and Risk Reduction) in JSON:\n"
                "{\n"
                f'  "improvement_plan": "{plan_en}",\n'
                f'  "improved_frequency": {new_freq},\n'
                f'  "improved_intensity": {new_intensity},\n'
                f'  "improved_T": {new_t},\n'
                f'  "reduction_rate": {compute_rrr(orig_t, new_t):.2f}\n'
                "}\n\n"
            )
            if count >= 2:
                break
        except Exception:
            continue

    # ì˜ˆì‹œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì˜ˆì‹œ ì œê³µ
    if count == 0:
        example_section = (
            "Example 1:\n"
            "Input Work Activity: Excavation and backfilling\n"
            "Input Hazard: Collapse of excavation wall\n"
            "Original Frequency: 3\n"
            "Original Intensity: 4\n"
            "Original T-value: 12\n"
            "Output (Improvement Plan and Risk Reduction) in JSON:\n"
            "{\n"
            '  "improvement_plan": "1) Maintain proper slope according to soil classification\\n'
            '2) Reinforce excavation walls\\n3) Conduct regular ground condition inspections",\n'
            '  "improved_frequency": 1,\n'
            '  "improved_intensity": 2,\n'
            '  "improved_T": 2,\n'
            '  "reduction_rate": 83.33\n'
            "}\n\n"
        )

    prompt = (
        example_section +
        "Now here is a new input:\n"
        f"Work Activity: {activity_en}\n"
        f"Hazard: {hazard_en}\n"
        f"Original Frequency: {freq}\n"
        f"Original Intensity: {intensity}\n"
        f"Original T-value: {t_val}\n\n"
        "Please provide practical and specific improvement measures in the following JSON format:\n"
        "{\n"
        '  "improvement_plan": "numbered list of specific measures",\n'
        '  "improved_frequency": (integer 1-5),\n'
        '  "improved_intensity": (integer 1-5),\n'
        '  "improved_T": (improved_frequency Ã— improved_intensity),\n'
        '  "reduction_rate": (percentage)\n'
        "}\n\n"
        "Improvement measures should include at least 3 field-applicable methods."
    )
    return prompt

def parse_gpt_output_phase2(gpt_output: str) -> dict:
    """
    Phase 2 GPT ì¶œë ¥(JSON)ì„ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (ì˜ì–´ í‚¤ ê¸°ì¤€).
    - {"improvement_plan": "...", "improved_frequency": 1, "improved_intensity": 2, "improved_T": 2, "reduction_rate": 83.33}
    """
    try:
        json_match = re.search(r'\{.*\}', gpt_output, re.DOTALL)
        if not json_match:
            raise ValueError("JSON match not found")
        import json
        parsed = json.loads(json_match.group(0))
        # ê¸°ë³¸ í‚¤ ë§¤í•‘
        return {
            "improvement_plan": parsed.get("improvement_plan", ""),
            "improved_freq": parsed.get("improved_frequency", 1),
            "improved_intensity": parsed.get("improved_intensity", 1),
            "improved_T": parsed.get("improved_T", parsed.get("improved_frequency", 1) * parsed.get("improved_intensity", 1)),
            "reduction_rate": parsed.get("reduction_rate", 0.0)
        }
    except Exception as e:
        st.error(f"Phase 2 íŒŒì‹± ì˜¤ë¥˜: {e}")
        # ê¸°ë³¸ê°’ ë¦¬í„´
        return {
            "improvement_plan": "1) Educate workers and mandate PPE usage",
            "improved_freq": 1,
            "improved_intensity": 1,
            "improved_T": 1,
            "reduction_rate": 50.0
        }

def create_excel_download(result_dict: dict, similar_records: list[dict]) -> bytes:
    """
    ìµœì¢… ê²°ê³¼ë¥¼ Excel ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜.
    - result_dict: {'activity': ..., 'hazard': ..., 'freq': ..., 'intensity': ..., 'T': ..., 'grade': ...,
                   'improvement_plan': ..., 'improved_freq': ..., 'improved_intensity': ..., 'improved_T': ...,
                   'rrr': ...}
    - similar_records: ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ, ê° í•­ëª©ì€ {"ì‘ì—…í™œë™":..., "ìœ í•´ìœ„í—˜ìš”ì¸":..., "ë¹ˆë„":..., "ê°•ë„":..., "T":..., "ë“±ê¸‰":..., "ê°œì„ ëŒ€ì±…":...}
    """
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            workbook = writer.book
            # â”€â”€â”€ Phase1 ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            phase1_df = pd.DataFrame({
                "í•­ëª©": ["ì‘ì—…í™œë™", "ìœ í•´ìœ„í—˜ìš”ì¸", "ë¹ˆë„", "ê°•ë„", "Tê°’", "ìœ„í—˜ë“±ê¸‰"],
                "ê°’": [
                    result_dict["activity"],
                    result_dict["hazard"],
                    result_dict["freq"],
                    result_dict["intensity"],
                    result_dict["T"],
                    result_dict["grade"]
                ]
            })
            phase1_df.to_excel(writer, sheet_name="Phase1_ê²°ê³¼", index=False)
            ws1 = writer.sheets["Phase1_ê²°ê³¼"]
            for col_idx in range(len(phase1_df.columns)):
                ws1.set_column(col_idx, col_idx, 20)

            # â”€â”€â”€ Phase2 ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            phase2_df = pd.DataFrame({
                "í•­ëª©": ["ê°œì„ ëŒ€ì±…", "ê°œì„  í›„ ë¹ˆë„", "ê°œì„  í›„ ê°•ë„", "ê°œì„  í›„ Tê°’", "ê°œì„  í›„ ë“±ê¸‰", "ìœ„í—˜ ê°ì†Œìœ¨"],
                "ê°’": [
                    result_dict["improvement_plan"],
                    result_dict["improved_freq"],
                    result_dict["improved_intensity"],
                    result_dict["improved_T"],
                    determine_grade(result_dict["improved_T"]),
                    f"{result_dict['rrr']:.2f}%"
                ]
            })
            phase2_df.to_excel(writer, sheet_name="Phase2_ê²°ê³¼", index=False)
            ws2 = writer.sheets["Phase2_ê²°ê³¼"]
            for col_idx in range(len(phase2_df.columns)):
                ws2.set_column(col_idx, col_idx, 20)

            # â”€â”€â”€ ë¹„êµë¶„ì„ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            comparison_df = pd.DataFrame({
                "í•­ëª©": ["ë¹ˆë„", "ê°•ë„", "Tê°’", "ìœ„í—˜ë“±ê¸‰"],
                "ê°œì„  ì „": [result_dict["freq"], result_dict["intensity"], result_dict["T"], result_dict["grade"]],
                "ê°œì„  í›„": [
                    result_dict["improved_freq"],
                    result_dict["improved_intensity"],
                    result_dict["improved_T"],
                    determine_grade(result_dict["improved_T"])
                ],
                "ê°œì„ ìœ¨": [
                    f"{(result_dict['freq'] - result_dict['improved_freq']) / result_dict['freq'] * 100:.1f}%"
                    if result_dict["freq"] > 0 else "0%",
                    f"{(result_dict['intensity'] - result_dict['improved_intensity']) / result_dict['intensity'] * 100:.1f}%"
                    if result_dict["intensity"] > 0 else "0%",
                    f"{result_dict['rrr']:.1f}%",
                    f"{result_dict['grade']} â†’ {determine_grade(result_dict['improved_T'])}"
                ]
            })
            comparison_df.to_excel(writer, sheet_name="ë¹„êµë¶„ì„", index=False)
            ws3 = writer.sheets["ë¹„êµë¶„ì„"]
            for col_idx in range(len(comparison_df.columns)):
                ws3.set_column(col_idx, col_idx, 20)

            # â”€â”€â”€ ìœ ì‚¬ì‚¬ë¡€ ì‹œíŠ¸ (PIMS ì–‘ì‹: í•œêµ­ì–´+ì˜ì–´ í˜¼í•© í—¤ë”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if similar_records:
                sim_df = pd.DataFrame(similar_records)
                # ì˜ì–´ ë‚´ë¶€ ê³„ì‚° í›„, 'ê°œì„  í›„ ë¹ˆë„', 'ê°œì„  í›„ ê°•ë„' ì»¬ëŸ¼ ì¶”ê°€
                sim_df["ê°œì„  í›„ ë¹ˆë„"] = sim_df["ë¹ˆë„"].astype(int).apply(lambda x: max(1, x - 1))
                sim_df["ê°œì„  í›„ ê°•ë„"] = sim_df["ê°•ë„"].astype(int).apply(lambda x: max(1, x - 1))

                # PIMS ì»¬ëŸ¼ëª…ìœ¼ë¡œ êµ¬ì„±
                export_df = pd.DataFrame({
                    texts["col_activity_header"]:   sim_df["ì‘ì—…í™œë™"],
                    texts["col_hazard_header"]:     sim_df["ìœ í•´ìœ„í—˜ìš”ì¸"],
                    texts["col_ehs_header"]:        ["" for _ in range(len(sim_df))],  # EHS ë¹ˆì¹¸
                    texts["col_risk_likelihood_header"]: sim_df["ë¹ˆë„"],
                    texts["col_risk_severity_header"]:   sim_df["ê°•ë„"],
                    texts["col_control_header"]:     sim_df["ê°œì„ ëŒ€ì±…"],
                    texts["col_incharge_header"]:    ["" for _ in range(len(sim_df))],  # In Charge ë¹ˆì¹¸
                    texts["col_duedate_header"]:     ["" for _ in range(len(sim_df))],  # Due Date ë¹ˆì¹¸
                    texts["col_after_likelihood_header"]: sim_df["ê°œì„  í›„ ë¹ˆë„"],
                    texts["col_after_severity_header"]:   sim_df["ê°œì„  í›„ ê°•ë„"]
                })
                export_df.to_excel(writer, sheet_name="ìœ ì‚¬ì‚¬ë¡€", index=False)
                ws4 = writer.sheets["ìœ ì‚¬ì‚¬ë¡€"]
                for col_idx in range(len(export_df.columns)):
                    ws4.set_column(col_idx, col_idx, 25)

        return output.getvalue()
    except ImportError:
        # xlsxwriterê°€ ì—†ìœ¼ë©´ CSV í¬ë§·ìœ¼ë¡œ ë°˜í™˜
        st.warning("Excel ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. CSVë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
        csv_buffer = io.StringIO()
        sim_df = pd.DataFrame(similar_records)
        sim_df["ê°œì„  í›„ ë¹ˆë„"] = sim_df["ë¹ˆë„"].astype(int).apply(lambda x: max(1, x - 1))
        sim_df["ê°œì„  í›„ ê°•ë„"] = sim_df["ê°•ë„"].astype(int).apply(lambda x: max(1, x - 1))

        export_df = pd.DataFrame({
            texts["col_activity_header"]:   sim_df["ì‘ì—…í™œë™"],
            texts["col_hazard_header"]:     sim_df["ìœ í•´ìœ„í—˜ìš”ì¸"],
            texts["col_ehs_header"]:        ["" for _ in range(len(sim_df))],
            texts["col_risk_likelihood_header"]: sim_df["ë¹ˆë„"],
            texts["col_risk_severity_header"]:   sim_df["ê°•ë„"],
            texts["col_control_header"]:     sim_df["ê°œì„ ëŒ€ì±…"],
            texts["col_incharge_header"]:    ["" for _ in range(len(sim_df))],
            texts["col_duedate_header"]:     ["" for _ in range(len(sim_df))],
            texts["col_after_likelihood_header"]: sim_df["ê°œì„  í›„ ë¹ˆë„"],
            texts["col_after_severity_header"]:   sim_df["ê°œì„  í›„ ê°•ë„"]
        })
        export_df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
        return csv_buffer.getvalue().encode("utf-8-sig")

# -----------------------------------------------------------------------------  
# ---------------------- Overview íƒ­ ------------------------------------------  
# -----------------------------------------------------------------------------  
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)
    col_overview, col_features = st.columns([3, 2])
    with col_overview:
        st.markdown(texts["overview_text"])
        st.markdown(f"**{texts['features_title']}**")
        st.markdown(texts["phase_features"])
    with col_features:
        # ì„ì˜ë¡œ ë©”íŠ¸ë¦­ì„ 3ê°œ ì—´ë¡œ í‘œì‹œ (ì§€ì› ì–¸ì–´ 3ê°œ, í‰ê°€ 2ë‹¨ê³„, ìœ„í—˜ë“±ê¸‰ 5ë‹¨ê³„)
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì§€ì› ì–¸ì–´", "3ê°œ", "í•œ/ì˜/ì¤‘")
        with col2:
            st.metric("í‰ê°€ ë‹¨ê³„", "2ë‹¨ê³„", "Phase1+Phase2")
        with col3:
            st.metric("ìœ„í—˜ë“±ê¸‰", "5ë“±ê¸‰", "A~E")

# -----------------------------------------------------------------------------  
# -------------------- Risk Assessment & Improvement íƒ­ ------------------------
# -----------------------------------------------------------------------------  
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["tab_phase"]}</div>', unsafe_allow_html=True)

    # API í‚¤ ì…ë ¥ ë° ë°ì´í„°ì…‹ ì„ íƒ
    col_api, col_dataset = st.columns([2, 1])
    with col_api:
        api_key = st.text_input(texts["api_key_label"], type="password", key="api_key_all")
    with col_dataset:
        dataset_name = st.selectbox(
            texts["dataset_label"],
            texts["dataset_label"] in texts.keys() and texts["dataset_options"] if "dataset_options" in texts else ["ê±´ì¶•", "í† ëª©", "í”ŒëœíŠ¸"],
            key="dataset_all"
        )

    # ë°ì´í„° ë¡œë“œ ë° ì¸ë±ì‹±
    if ss.retriever_pool_df is None or st.button(texts["load_data_btn"], type="primary"):
        if not api_key:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["data_loading"]):
                try:
                    df = load_data(dataset_name)
                    if len(df) > 10:
                        train_df, _ = train_test_split(df, test_size=0.1, random_state=42)
                    else:
                        train_df = df.copy()

                    pool_df = train_df.copy()
                    pool_df["content"] = pool_df.apply(lambda r: " ".join(r.values.astype(str)), axis=1)

                    to_embed = pool_df["content"].tolist()
                    max_texts = min(len(to_embed), 30)
                    st.info(texts["demo_limit_info"].format(max_texts=max_texts))

                    embeds = embed_texts_with_openai(to_embed[:max_texts], api_key=api_key)
                    vecs = np.array(embeds, dtype="float32")
                    dim = vecs.shape[1]
                    index = faiss.IndexFlatL2(dim)
                    index.add(vecs)

                    ss.index = index
                    ss.embeddings = vecs
                    ss.retriever_pool_df = pool_df.iloc[:max_texts]
                    st.success(texts["data_load_success"].format(max_texts=max_texts))
                    with st.expander("ğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                        st.dataframe(df.head(), use_container_width=True)
                except Exception as e:
                    st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")

    st.divider()
    st.markdown(f"### {texts['performing_assessment'].split('.')[0]}")

    # ì‚¬ìš©ì ì…ë ¥
    activity = st.text_area(
        texts["activity_label"],
        placeholder={
            "Korean": "ì˜ˆ: ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "English": "e.g.: Unloading steel structural materials using forklift at temporary site storage",
            "Chinese": "ä¾‹: åœ¨ä¸´æ—¶ç°åœºä»“åº“ä½¿ç”¨å‰è½¦å¸è½½é’¢ç»“æ„ææ–™"
        }.get(ss.language),
        height=100,
        key="user_activity"
    )
    col_opt1, col_opt2 = st.columns(2)
    with col_opt1:
        include_similar_cases = st.checkbox(texts["include_similar_cases"], value=True)
    with col_opt2:
        result_language = st.selectbox(
            texts["result_language"],
            ["Korean", "English", "Chinese"],
            index=["Korean", "English", "Chinese"].index(ss.language)
        )

    run_button = st.button(texts["run_assessment"], type="primary", use_container_width=True)

    if run_button:
        # ì…ë ¥ ê²€ì¦
        if not activity:
            st.warning(texts["activity_warning"])
        elif not api_key:
            st.warning(texts["api_key_warning"])
        elif ss.index is None:
            st.warning(texts["load_first_warning"])
        else:
            with st.spinner(texts["performing_assessment"]):
                try:
                    # ===== Phase 1 =====
                    # 1) ì‚¬ìš©ìê°€ í•œêµ­ì–´ë¡œ ì…ë ¥í–ˆìœ¼ë¯€ë¡œ, ì˜ì–´ë¡œ ë²ˆì—­
                    prompt_to_english = f"Translate the following construction work activity into English. Only provide the translation:\n\n{activity}"
                    activity_en = generate_with_gpt(prompt_to_english, api_key)
                    if not activity_en:
                        activity_en = activity  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©

                    # 2) sim_docs (í•œêµ­ì–´) â†’ sim_docs_en (ì˜ì–´ ë²ˆì—­)
                    sim_docs = ss.retriever_pool_df.copy().reset_index(drop=True)
                    sim_docs_en = translate_similar_cases(sim_docs, "English", api_key)

                    # 3) ìœ ì‚¬ ì‚¬ë¡€ì˜ contentë§Œ embedí•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ â†’ I, D ë°˜í™˜
                    q_emb_list = embed_texts_with_openai([activity_en], api_key=api_key)
                    if not q_emb_list:
                        st.error("ìœ„í—˜ì„± í‰ê°€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()
                    q_emb = q_emb_list[0]

                    D, I = ss.index.search(np.array([q_emb], dtype="float32"), k=min(10, len(sim_docs_en)))
                    if I is None or len(I[0]) == 0:
                        st.error("ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    sim_docs_subset = sim_docs_en.iloc[I[0]].reset_index(drop=True)

                    # 4) Phase1: ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡(prompt_en ìƒì„± â†’ GPT ì‹¤í–‰)
                    hazard_prompt_en = construct_prompt_phase1_hazard(sim_docs_subset, activity_en)
                    hazard_en = generate_with_gpt(hazard_prompt_en, api_key)
                    if not hazard_en:
                        st.error("ìœ„í—˜ì„± í‰ê°€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    # 5) Phase1: ìœ„í—˜ë„ í‰ê°€(prompt_en ìƒì„± â†’ GPT ì‹¤í–‰ â†’ íŒŒì‹±)
                    risk_prompt_en = construct_prompt_phase1_risk(sim_docs_subset, activity_en, hazard_en)
                    risk_json_en = generate_with_gpt(risk_prompt_en, api_key)
                    parse_result = parse_gpt_output_phase1(risk_json_en)
                    if not parse_result:
                        st.error("ìœ„í—˜ì„± í‰ê°€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.expander("GPT ì›ë³¸ ì‘ë‹µ").write(risk_json_en)
                        st.stop()

                    freq, intensity, T_val = parse_result
                    grade = determine_grade(T_val)

                    # ===== Phase 2 =====
                    prompt_phase2_en = construct_prompt_phase2(sim_docs_subset, activity_en, hazard_en, freq, intensity, T_val)
                    improvement_json_en = generate_with_gpt(prompt_phase2_en, api_key)
                    parsed_improvement = parse_gpt_output_phase2(improvement_json_en)
                    improvement_plan_en = parsed_improvement.get("improvement_plan", "")
                    improved_freq = parsed_improvement.get("improved_freq", 1)
                    improved_intensity = parsed_improvement.get("improved_intensity", 1)
                    improved_T = parsed_improvement.get("improved_T", improved_freq * improved_intensity)
                    rrr_value = compute_rrr(T_val, improved_T)

                    # ===== ìµœì¢… ì¶œë ¥ìš© ë²ˆì—­ =====
                    # 1) í™œë™, ìœ„í—˜, ê°œì„  ê³„íš ë“±
                    hazard_user = translate_output(hazard_en, result_language, api_key)
                    improvement_user = translate_output(improvement_plan_en, result_language, api_key)

                    # 2) sim_cases ì‚¬ìš©ì í‘œì‹œ ë°ì´í„° ìƒì„±
                    display_sim_records = []
                    for idx, row in sim_docs_subset.iterrows():
                        # í•œêµ­ì–´ ì›ë³¸ ë°ì´í„°ì—ì„œ idx ìœ„ì¹˜ í–‰ì„ ì°¾ì•„ì•¼ í•˜ë¯€ë¡œ, I[0][idx] ì¸ë±ìŠ¤ë¥¼ ì´ìš©
                        orig_row = sim_docs.iloc[I[0][idx]]  # sim_docs: í•œêµ­ì–´ ì›ë³¸
                        if result_language == "English":
                            act_disp = row["activity_en"]
                            haz_disp = row["hazard_en"]
                            plan_disp = translate_output(orig_row["ê°œì„ ëŒ€ì±…"], "English", api_key)
                        elif result_language == "Chinese":
                            act_disp = translate_output(orig_row["ì‘ì—…í™œë™ ë° ë‚´ìš©"], "Chinese", api_key)
                            haz_disp = translate_output(orig_row["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"], "Chinese", api_key)
                            plan_disp = translate_output(orig_row["ê°œì„ ëŒ€ì±…"], "Chinese", api_key)
                        else:  # Korean
                            act_disp = orig_row["ì‘ì—…í™œë™ ë° ë‚´ìš©"]
                            haz_disp = orig_row["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"]
                            plan_disp = orig_row["ê°œì„ ëŒ€ì±…"]

                        display_sim_records.append({
                            "ì‘ì—…í™œë™": act_disp,
                            "ìœ í•´ìœ„í—˜ìš”ì¸": haz_disp,
                            "ë¹ˆë„": orig_row["ë¹ˆë„"],
                            "ê°•ë„": orig_row["ê°•ë„"],
                            "T": orig_row["T"],
                            "ë“±ê¸‰": orig_row["ë“±ê¸‰"],
                            "ê°œì„ ëŒ€ì±…": plan_disp
                        })

                    # ===== í™”ë©´ ì¶œë ¥ =====
                    st.markdown(f"## {texts['phase1_results']}")
                    col_r1, col_r2 = st.columns([2, 1])
                    with col_r1:
                        # ì‘ì—…í™œë™ ì‚¬ìš©ì ì–¸ì–´ë¡œ í‘œì‹œ
                        activity_user = (
                            translate_output(activity_en, result_language, api_key)
                            if result_language != "English"
                            else activity_en
                        )
                        st.markdown(f"**{texts['work_activity']}:** {activity_user}")
                        st.markdown(f"**{texts['predicted_hazard']}:** {hazard_user}")

                        result_df_user = pd.DataFrame({
                            texts["comparison_columns"][0] if ss.language=="Korean" else texts["comparison_columns"][0]: ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
                            "ê°’": [str(freq), str(intensity), str(T_val), grade]
                        })
                        # hide_index=True ì˜µì…˜
                        st.dataframe(result_df_user.astype(str), use_container_width=True, hide_index=True)
                    with col_r2:
                        grade_color = get_grade_color(grade)
                        st.markdown(f"""
                        <div style="text-align:center; padding:20px; background-color:{grade_color};
                                    color:white; border-radius:10px; margin:10px 0;">
                            <h2 style="margin:0;">{texts['risk_grade_display']}</h2>
                            <h1 style="margin:10px 0; font-size:3rem;">{grade}</h1>
                            <p style="margin:0;">{texts['t_value_display']}: {T_val}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    # ===== ìœ ì‚¬ì‚¬ë¡€ í‘œì‹œ =====
                    if include_similar_cases and display_sim_records:
                        st.markdown(f"### {texts['similar_cases_section']}")
                        for idx, rec in enumerate(display_sim_records[:5]):  # ìµœëŒ€ 5ê°œ
                            with st.expander(f"{texts['case_number']} {idx+1}: {rec['ì‘ì—…í™œë™'][:30]}â€¦"):
                                c1, c2 = st.columns(2)
                                with c1:
                                    st.write(f"**{texts['work_activity']} :** {rec['ì‘ì—…í™œë™']}")
                                    st.write(f"**{texts['predicted_hazard']} :** {rec['ìœ í•´ìœ„í—˜ìš”ì¸']}")
                                    st.write(f"**ìœ„í—˜ë„ :** ë¹ˆë„ {rec['ë¹ˆë„']}, ê°•ë„ {rec['ê°•ë„']}, T {rec['T']} ({rec['ë“±ê¸‰']})")
                                with c2:
                                    st.write(f"**ê°œì„ ëŒ€ì±… :**")
                                    plan_md = rec["ê°œì„ ëŒ€ì±…"].replace("\n", "  \n")
                                    st.markdown(plan_md)

                    # ===== Phase 2 ì¶œë ¥ =====
                    st.markdown(f"## {texts['phase2_results']}")
                    c_imp, c_riskimp = st.columns([3, 2])
                    with c_imp:
                        st.markdown(f"### {texts['improvement_plan_header']}")
                        if improvement_user:
                            plan_md2 = improvement_user.replace("\n", "  \n")
                            st.markdown(plan_md2)
                        else:
                            st.write("ê°œì„ ëŒ€ì±… ìƒì„± ì‹¤íŒ¨")

                    with c_riskimp:
                        st.markdown(f"### {texts['risk_improvement_header']}")
                        comp_df_user = pd.DataFrame({
                            texts["comparison_columns"][0]: ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
                            texts["comparison_columns"][1]: [str(freq), str(intensity), str(T_val), grade],
                            texts["comparison_columns"][2]: [str(improved_freq), str(improved_intensity), str(improved_T), determine_grade(improved_T)]
                        })
                        st.dataframe(comp_df_user.astype(str), use_container_width=True, hide_index=True)
                        st.metric(
                            label=texts["risk_reduction_label"],
                            value=f"{rrr_value:.1f}%",
                            delta=f"-{T_val - improved_T} T"
                        )

                    # ===== ìœ„í—˜ë„ ì‹œê°í™” =====
                    st.markdown(f"### {texts['risk_visualization']}")
                    vis1, vis2 = st.columns(2)
                    with vis1:
                        st.markdown(f"**{texts['before_improvement']}**")
                        col_before = get_grade_color(grade)
                        st.markdown(f"""
                        <div style="background-color:{col_before}; color:white; padding:15px; 
                                    border-radius:10px; text-align:center; margin:10px 0;">
                            <h3 style="margin:0;">{texts['grade_label']} {grade}</h3>
                            <p style="margin:5px 0; font-size:1.2em;">{texts['t_value_display']}: {T_val}</p>
                        </div>
                        """, unsafe_allow_html=True)
                    with vis2:
                        st.markdown(f"**{texts['after_improvement']}**")
                        grade_after = determine_grade(improved_T)
                        col_after = get_grade_color(grade_after)
                        st.markdown(f"""
                        <div style="background-color:{col_after}; color:white; padding:15px; 
                                    border-radius:10px; text-align:center; margin:10px 0;">
                            <h3 style="margin:0;">{texts['grade_label']} {grade_after}</h3>
                            <p style="margin:5px 0; font-size:1.2em;">{texts['t_value_display']}: {improved_T}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    # ===== ì„¸ì…˜ì— ì €ì¥ =====
                    ss.last_assessment = {
                        "activity": activity_user,
                        "hazard": hazard_user,
                        "freq": freq,
                        "intensity": intensity,
                        "T": T_val,
                        "grade": grade,
                        "improvement_plan": improvement_user,
                        "improved_freq": improved_freq,
                        "improved_intensity": improved_intensity,
                        "improved_T": improved_T,
                        "rrr": rrr_value,
                        "similar_cases": display_sim_records
                    }

                    # ===== ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ =====
                    st.markdown(f"### {texts['download_results']}")
                    excel_bytes = create_excel_download(ss.last_assessment, display_sim_records)
                    st.download_button(
                        label=texts["excel_export"],
                        data=excel_bytes,
                        file_name="risk_assessment_report.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                except Exception as e:
                    st.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
                    st.stop()

    # íƒ­ í•˜ë‹¨ ê°„ë‹¨ í‘¸í„° (ë¡œê³  ë“±)
    st.markdown('<hr style="margin-top: 3rem;">', unsafe_allow_html=True)
    footer_c1, footer_c2, footer_c3 = st.columns([1, 1, 1])
    with footer_c1:
        if os.path.exists("cau.png"):
            st.image("cau.png", width=140)
    with footer_c2:
        st.markdown(
            """
            <div style="text-align: center; padding: 20px;">
                <h4>ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°</h4>
                <p>AI ê¸°ë°˜ ìœ„í—˜ì„± í‰ê°€ ì‹œìŠ¤í…œ</p>
                <p style="font-size: 0.8rem; color: #666;">
                    Â© 2025 Doosan Enerbility. All rights reserved.
                </p>
            </div>
            """,
            unsafe_allow_html=True
        )
    with footer_c3:
        if os.path.exists("doosan.png"):
            st.image("doosan.png", width=160)
