import streamlit as st
import pandas as pd
import numpy as np
import faiss
import re
import os
import io
from PIL import Image
from sklearn.model_selection import train_test_split
from openai import OpenAI

# ----------------- ì‹œìŠ¤í…œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ -----------------
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_phase1": "ìœ„í—˜ì„± í‰ê°€ (Phase 1)",
        "tab_phase2": "ê°œì„ ëŒ€ì±… ìƒì„± (Phase 2)",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹° AI Risk AssessmentëŠ” êµ­ë‚´ ë° í•´ì™¸ ê±´ì„¤í˜„ì¥ 'ìˆ˜ì‹œìœ„í—˜ì„±í‰ê°€' ë° 'ë…¸ë™ë¶€ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€'ë¥¼ í•™ìŠµí•˜ì—¬ ê°œë°œëœ ìë™ ìœ„í—˜ì„±í‰ê°€ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. ìƒì„±ëœ ìœ„í—˜ì„±í‰ê°€ëŠ” ë°˜ë“œì‹œ ìˆ˜ì‹œ ìœ„í—˜ì„±í‰ê°€ ì‹¬ì˜íšŒë¥¼ í†µí•´ ê²€ì¦ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase1_features": """
        #### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”
        - ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ
        - ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡
        - ìœ ì‚¬ ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ
        - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì •
        - Excel ê¸°ë°˜ ê³µì •ë³„ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° ìë™ ë¶„ì„
        - ìœ„í—˜ë“±ê¸‰(A-E) ìë™ ì‚°ì •
        """,
        "phase2_features": """
        #### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ìœ„í—˜ìš”ì†Œë³„ ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ë‹¤êµ­ì–´(í•œ/ì˜/ì¤‘) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›
        - ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„
        - ìœ„í—˜ ê°ì†Œìœ¨(RRR) ì •ëŸ‰ì  ì‚°ì¶œ
        - ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        """,
        "phase1_header": "ìœ„í—˜ì„± í‰ê°€ ìë™í™” (Phase 1)",
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "predict_hazard_btn": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡í•˜ê¸°",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "similar_cases_header": "ìœ ì‚¬í•œ ì‚¬ë¡€",
        "result_table_columns": [
            "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazardous Factors",
            "EHS",
            "ë¹ˆë„ likelihood",
            "ê°•ë„ severity",
            "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures",
            "ê°œì„ ë‹´ë‹¹ì In Charge",
            "ê°œì„ ì¼ì Correction Due Date"
        ],
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "phase2_header": "ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (Phase 2)",
        "language_select_label": "ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ:",
        "input_method_label": "ì…ë ¥ ë°©ì‹ ì„ íƒ:",
        "input_methods": ["Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©", "ì§ì ‘ ì…ë ¥"],
        "phase1_results_header": "Phase 1 í‰ê°€ ê²°ê³¼",
        "phase1_first_warning": "ë¨¼ì € Phase 1ì—ì„œ ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.",
        "hazard_label": "ìœ í•´ìœ„í—˜ìš”ì¸:",
        "frequency_label": "ë¹ˆë„ (1-5):",
        "intensity_label": "ê°•ë„ (1-5):",
        "generate_improvement_btn": "ê°œì„ ëŒ€ì±… ìƒì„±",
        "generating_improvement": "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ëŠ” ì¤‘...",
        "no_data_warning": "Phase 1ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±ì„ ì™„ë£Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ê°œì„  í›„ ìœ„í—˜ì„±",
        "excel_export": "ğŸ“¥ ê²°ê³¼ Excel ë‹¤ìš´ë¡œë“œ",
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_phase1": "Risk Assessment (Phase 1)",
        "tab_phase2": "Improvement Measures (Phase 2)",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": "Doosan Enerbility AI Risk Assessment is an automated program trained on both on-demand risk-assessment reports from domestic and overseas construction sites and major-accident cases compiled by Korea's Ministry of Employment and Labor. Please ensure that every generated assessment is reviewed and approved by the On-Demand Risk Assessment Committee before it is used.",
        "features_title": "System Features and Components",
        "phase1_features": """
        #### Phase 1: Risk Assessment Automation
        - Learning risk assessment data according to work activities by process
        - Automatic hazard prediction when work activities are entered
        - Similar case search and display
        - Risk level (frequency, intensity, T) measurement based on large language models (LLM)
        - Automatic analysis of Excel-based process-specific risk assessment data
        - Automatic risk grade (A-E) calculation
        """,
        "phase2_features": """
        #### Phase 2: Automatic Generation of Improvement Measures
        - Automatic generation of customized improvement measures for risk factors
        - Multilingual (Korean/English/Chinese) improvement measure generation support
        - Automatic comparative analysis of risk level (T) before and after improvement
        - Quantitative calculation of Risk Reduction Rate (RRR)
        - Building a database of optimal improvement measures by work type/process
        """,
        "phase1_header": "Risk Assessment Automation (Phase 1)",
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset",
        "load_data_btn": "Load Data and Configure Index",
        "api_key_warning": "Please enter an OpenAI API key to continue.",
        "data_loading": "Loading data and configuring index...",
        "demo_limit_info": "For demo purposes, only embedding {max_texts} texts. In a real environment, all data should be processed.",
        "data_load_success": "Data load and index configuration complete! (Total {max_texts} items processed)",
        "load_first_warning": "Please click the [Load Data and Configure Index] button first.",
        "activity_label": "Work Activity:",
        "predict_hazard_btn": "Predict Hazards",
        "activity_warning": "Please enter a work activity.",
        "similar_cases_header": "Similar Cases",
        "result_table_columns": [
            "Work Sequence",
            "Hazardous Factors",
            "EHS",
            "likelihood",
            "severity",
            "Control Measures",
            "In Charge",
            "Correction Due Date"
        ],
        "parsing_error": "Unable to parse risk assessment results.",
        "phase2_header": "Automatic Generation of Improvement Measures (Phase 2)",
        "language_select_label": "Select Language for Improvement Measures:",
        "input_method_label": "Select Input Method:",
        "input_methods": ["Use Phase 1 Assessment Results", "Direct Input"],
        "phase1_results_header": "Phase 1 Assessment Results",
        "phase1_first_warning": "Please perform a risk assessment in Phase 1 first.",
        "hazard_label": "Hazard:",
        "frequency_label": "Frequency (1-5):",
        "intensity_label": "Intensity (1-5):",
        "generate_improvement_btn": "Generate Improvement Measures",
        "generating_improvement": "Generating improvement measures...",
        "no_data_warning": "Data loading and index configuration was not completed in Phase 1. Using basic examples.",
        "improvement_plan_header": "Control Measures",
        "risk_improvement_header": "Post-Improvement Risk",
        "excel_export": "ğŸ“¥ Download Excel Results",
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_phase1": "é£é™©è¯„ä¼° (ç¬¬1é˜¶æ®µ)",
        "tab_phase2": "æ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "overview_header": "åŸºäºLLMçš„é£é™©è¯„ä¼°ç³»ç»Ÿ",
        "overview_text": "Doosan Enerbility AI é£é™©è¯„ä¼°ç³»ç»Ÿæ˜¯ä¸€æ¬¾è‡ªåŠ¨åŒ–é£é™©è¯„ä¼°ç¨‹åºï¼ŒåŸºäºå›½å†…å¤–æ–½å·¥ç°åœºçš„'ä¸´æ—¶é£é™©è¯„ä¼°'æ•°æ®ä»¥åŠéŸ©å›½é›‡ä½£åŠ³åŠ¨éƒ¨çš„é‡å¤§äº‹æ•…æ¡ˆä¾‹è¿›è¡Œè®­ç»ƒå¼€å‘è€Œæˆã€‚ç”Ÿæˆçš„é£é™©è¯„ä¼°ç»“æœå¿…é¡»ç»è¿‡ä¸´æ—¶é£é™©è¯„ä¼°å®¡è®®å§”å‘˜ä¼šçš„å®¡æ ¸åæ–¹å¯ä½¿ç”¨ã€‚",
        "features_title": "ç³»ç»Ÿç‰¹ç‚¹å’Œç»„ä»¶",
        "phase1_features": """
        #### ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°è‡ªåŠ¨åŒ–
        - æŒ‰å·¥åºå­¦ä¹ ä¸å·¥ä½œæ´»åŠ¨ç›¸å…³çš„é£é™©è¯„ä¼°æ•°æ®
        - è¾“å…¥å·¥ä½œæ´»åŠ¨æ—¶è‡ªåŠ¨é¢„æµ‹å±å®³å› ç´ 
        - ç›¸ä¼¼æ¡ˆä¾‹æœç´¢å’Œæ˜¾ç¤º
        - åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„é£é™©ç­‰çº§ï¼ˆé¢‘ç‡ã€å¼ºåº¦ã€Tå€¼ï¼‰æµ‹é‡
        - è‡ªåŠ¨åˆ†æåŸºäºExcelçš„ç‰¹å®šå·¥åºé£é™©è¯„ä¼°æ•°æ®
        - è‡ªåŠ¨è®¡ç®—é£é™©ç­‰çº§(A-E)
        """,
        "phase2_features": """
        #### ç¬¬2é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½
        - ä¸ºé£é™©å› ç´ è‡ªåŠ¨ç”Ÿæˆå®šåˆ¶çš„æ”¹è¿›æªæ–½
        - å¤šè¯­è¨€ï¼ˆéŸ©è¯­/è‹±è¯­/ä¸­æ–‡ï¼‰æ”¹è¿›æªæ–½ç”Ÿæˆæ”¯æŒ
        - æ”¹è¿›å‰åé£é™©ç­‰çº§ï¼ˆTå€¼ï¼‰çš„è‡ªåŠ¨æ¯”è¾ƒåˆ†æ
        - é£é™©é™ä½ç‡(RRR)çš„å®šé‡è®¡ç®—
        - å»ºç«‹æŒ‰å·¥ä½œç±»å‹/å·¥åºçš„æœ€ä½³æ”¹è¿›æªæ–½æ•°æ®åº“
        """,
        "phase1_header": "é£é™©è¯„ä¼°è‡ªåŠ¨åŒ– (ç¬¬1é˜¶æ®µ)",
        "api_key_label": "è¾“å…¥OpenAI APIå¯†é’¥ï¼š",
        "dataset_label": "é€‰æ‹©æ•°æ®é›†",
        "load_data_btn": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "api_key_warning": "è¯·è¾“å…¥OpenAI APIå¯†é’¥ä»¥ç»§ç»­ã€‚",
        "data_loading": "æ­£åœ¨åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•...",
        "demo_limit_info": "å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œä»…åµŒå…¥{max_texts}ä¸ªæ–‡æœ¬ã€‚åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œåº”å¤„ç†æ‰€æœ‰æ•°æ®ã€‚",
        "data_load_success": "æ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®å®Œæˆï¼ï¼ˆå…±å¤„ç†{max_texts}ä¸ªé¡¹ç›®ï¼‰",
        "load_first_warning": "è¯·å…ˆç‚¹å‡»[åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•]æŒ‰é’®ã€‚",
        "activity_label": "å·¥ä½œæ´»åŠ¨ï¼š",
        "predict_hazard_btn": "é¢„æµ‹å±å®³",
        "activity_warning": "è¯·è¾“å…¥å·¥ä½œæ´»åŠ¨ã€‚",
        "similar_cases_header": "ç›¸ä¼¼æ¡ˆä¾‹",
        "result_table_columns": [
            "å·¥ä½œæ´»åŠ¨ Work Sequence",
            "å±å®³å› ç´  Hazardous Factors",
            "EHS",
            "é¢‘ç‡ likelihood",
            "å¼ºåº¦ severity",
            "æ§åˆ¶æªæ–½ Control Measures",
            "è´£ä»»äºº In Charge",
            "æ•´æ”¹æ—¥æœŸ Correction Due Date"
        ],
        "parsing_error": "æ— æ³•è§£æé£é™©è¯„ä¼°ç»“æœã€‚",
        "phase2_header": "è‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "language_select_label": "é€‰æ‹©æ”¹è¿›æªæ–½çš„è¯­è¨€ï¼š",
        "input_method_label": "é€‰æ‹©è¾“å…¥æ–¹æ³•ï¼š",
        "input_methods": ["ä½¿ç”¨ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ", "ç›´æ¥è¾“å…¥"],
        "phase1_results_header": "ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ",
        "phase1_first_warning": "è¯·å…ˆåœ¨ç¬¬1é˜¶æ®µè¿›è¡Œé£é™©è¯„ä¼°ã€‚",
        "hazard_label": "å±å®³ï¼š",
        "frequency_label": "é¢‘ç‡ (1-5)ï¼š",
        "intensity_label": "å¼ºåº¦ (1-5)ï¼š",
        "generate_improvement_btn": "ç”Ÿæˆæ”¹è¿›æªæ–½",
        "generating_improvement": "æ­£åœ¨ç”Ÿæˆæ”¹è¿›æªæ–½...",
        "no_data_warning": "åœ¨ç¬¬1é˜¶æ®µæœªå®Œæˆæ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®ã€‚ä½¿ç”¨åŸºæœ¬ç¤ºä¾‹ã€‚",
        "improvement_plan_header": "æ§åˆ¶æªæ–½ Control Measures",
        "risk_improvement_header": "æ•´æ”¹åé£é™© Post-Improvement Risk",
        "excel_export": "ğŸ“¥ ä¸‹è½½Excelç»“æœ",
    }
}

# ----------------- í˜ì´ì§€ ìŠ¤íƒ€ì¼ -----------------
st.set_page_config(page_title="Artificial Intelligence Risk Assessment", page_icon="ğŸ› ï¸", layout="wide")
st.markdown(
    """
    <style>
    .main-header{font-size:2.5rem;color:#1E88E5;text-align:center;margin-bottom:1rem}
    .sub-header{font-size:1.8rem;color:#0D47A1;margin-top:2rem;margin-bottom:1rem}
    .metric-container{background-color:#f0f2f6;border-radius:10px;padding:20px;box-shadow:2px 2px 5px rgba(0,0,0,0.1)}
    .result-box{background-color:#f8f9fa;border-radius:10px;padding:15px;margin-top:10px;margin-bottom:10px;border-left:5px solid #4CAF50}
    .phase-badge{background-color:#4CAF50;color:white;padding:5px 10px;border-radius:15px;font-size:0.8rem;margin-right:10px}
    .similar-case{background-color:#f1f8e9;border-radius:8px;padding:12px;margin-bottom:8px;border-left:4px solid #689f38}
    .language-selector{position:absolute;top:10px;right:10px;z-index:1000}
    .calculation-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:15px;margin:20px 0}
    .risk-grade-a{background-color:#ff1744;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-b{background-color:#ff9800;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-c{background-color:#ffc107;color:black;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-d{background-color:#4caf50;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-e{background-color:#2196f3;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    </style>
    """,
    unsafe_allow_html=True
)

# ----------------- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -----------------
ss = st.session_state
for key, default in {
    "language": "Korean",
    "index": None,
    "embeddings": None,
    "retriever_pool_df": None,
    "last_assessment": None
}.items():
    if key not in ss:
        ss[key] = default

# ----------------- ì–¸ì–´ ì„ íƒ -----------------
col0, colLang = st.columns([6, 1])
with colLang:
    lang = st.selectbox(
        "ì–¸ì–´ ì„ íƒ",
        list(system_texts.keys()),
        index=list(system_texts.keys()).index(ss.language),
        label_visibility="hidden"
    )
    ss.language = lang
texts = system_texts[ss.language]

# ----------------- í—¤ë” -----------------
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# ----------------- íƒ­ êµ¬ì„± -----------------
tabs = st.tabs([texts["tab_overview"], f"{texts['tab_phase1']} & {texts['tab_phase2']}"])

# -----------------------------------------------------------------------------  
# --------------------------- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ -------------------------------------  
# -----------------------------------------------------------------------------  

def determine_grade(value: int):
    """ìœ„í—˜ë„ ë“±ê¸‰ ë¶„ë¥˜"""
    if 16 <= value <= 25:
        return 'A'
    if 10 <= value <= 15:
        return 'B'
    if 5 <= value <= 9:
        return 'C'
    if 3 <= value <= 4:
        return 'D'
    if 1 <= value <= 2:
        return 'E'
    return 'Unknown' if ss.language != 'Korean' else 'ì•Œ ìˆ˜ ì—†ìŒ'

def get_grade_color(grade):
    """ìœ„í—˜ë“±ê¸‰ë³„ ìƒ‰ìƒ ë°˜í™˜"""
    colors = {
        'A': '#ff1744',
        'B': '#ff9800',
        'C': '#ffc107',
        'D': '#4caf50',
        'E': '#2196f3',
    }
    return colors.get(grade, '#808080')

def compute_rrr(original_t, improved_t):
    """ìœ„í—˜ ê°ì†Œìœ¨ ê³„ì‚°"""
    if original_t == 0:
        return 0.0
    return ((original_t - improved_t) / original_t) * 100

def _extract_improvement_info(row):
    """
    ìœ ì‚¬ ì‚¬ë¡€ í•œ ê±´ì—ì„œ - ê°œì„ ëŒ€ì±… / ê°œì„  í›„ ë¹ˆë„Â·ê°•ë„Â·T ê°’ì„ ì¶”ì¶œ
    """
    plan_cols = [c for c in row.index if re.search(r'ê°œì„ ëŒ€ì±…|Improvement|æ”¹è¿›', c, re.I)]
    plan = row[plan_cols[0]] if plan_cols else ""

    cand_sets = [
        ('ê°œì„  í›„ ë¹ˆë„', 'ê°œì„  í›„ ê°•ë„', 'ê°œì„  í›„ T'),
        ('ê°œì„ ë¹ˆë„', 'ê°œì„ ê°•ë„', 'ê°œì„ T'),
        ('improved_frequency', 'improved_intensity', 'improved_T'),
        ('æ”¹è¿›åé¢‘ç‡', 'æ”¹è¿›åå¼ºåº¦', 'æ”¹è¿›åTê°’'),
    ]
    imp_f, imp_i, imp_t = None, None, None
    for f, i, t in cand_sets:
        if f in row and i in row and t in row:
            imp_f, imp_i, imp_t = int(row[f]), int(row[i]), int(row[t])
            break

    if imp_f is None:
        orig_f, orig_i = int(row['ë¹ˆë„']), int(row['ê°•ë„'])
        imp_f = max(1, orig_f - 1)
        imp_i = max(1, orig_i - 1)
        imp_t = imp_f * imp_i

    return plan, imp_f, imp_i, imp_t

@st.cache_data(show_spinner=False)
def load_data(selected_dataset_name: str):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    try:
        if os.path.exists(f"{selected_dataset_name}.xlsx"):
            df = pd.read_excel(f"{selected_dataset_name}.xlsx")
        else:
            return create_sample_data()

        if "ì‚­ì œ Del" in df.columns:
            df.drop(["ì‚­ì œ Del"], axis=1, inplace=True)

        df = df.dropna(how='all')

        column_mapping = {
            "ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect": "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ\nCorrective Action": "ê°œì„ ëŒ€ì±…"
        }
        df.rename(columns=column_mapping, inplace=True)

        numeric_columns = ['ë¹ˆë„', 'ê°•ë„']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        if 'ë¹ˆë„' not in df.columns:
            df['ë¹ˆë„'] = 3
        if 'ê°•ë„' not in df.columns():
            df['ê°•ë„'] = 3

        df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
        df["ë“±ê¸‰"] = df["T"].apply(determine_grade)

        if "ê°œì„ ëŒ€ì±…" not in df.columns:
            alt_cols = [c for c in df.columns if "ê°œì„ " in c or "ëŒ€ì±…" in c or "Corrective" in c]
            if alt_cols:
                df.rename(columns={alt_cols[0]: "ê°œì„ ëŒ€ì±…"}, inplace=True)
            else:
                df["ê°œì„ ëŒ€ì±…"] = "ì•ˆì „ êµìœ¡ ì‹¤ì‹œ ë° ë³´í˜¸êµ¬ ì°©ìš©"

        required_cols = [
            "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ë¹ˆë„",
            "ê°•ë„",
            "T",
            "ë“±ê¸‰",
            "ê°œì„ ëŒ€ì±…"
        ]
        final_cols = [col for col in required_cols if col in df.columns]
        df = df[final_cols]

        df = df.fillna({
            "ì‘ì—…í™œë™ ë° ë‚´ìš©": "ì¼ë°˜ ì‘ì—…",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": "ì¼ë°˜ì  ìœ„í—˜",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": "ë¶€ìƒ",
            "ê°œì„ ëŒ€ì±…": "ì•ˆì „ ì¡°ì¹˜ ìˆ˜í–‰"
        })

        return df

    except Exception as e:
        st.warning(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return create_sample_data()

def create_sample_data():
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    data = {
        "ì‘ì—…í™œë™ ë° ë‚´ìš©": [
            "ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "ì½˜í¬ë¦¬íŠ¸/CMU ë¸”ë¡ ì„¤ì¹˜ ì‘ì—…",
            "êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…",
            "ê³ ì†Œ ì‘ì—…ëŒ€ë¥¼ ì´ìš©í•œ ì™¸ë²½ ì‘ì—…",
            "ìš©ì ‘ ì‘ì—…"
        ],
        "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": [
            "ë‹¤ì¤‘ ì¸ì–‘ìœ¼ë¡œ ì¸í•œ ì ì¬ë¬¼ ë‚™í•˜",
            "ë¶ˆì¶©ë¶„í•œ ì‘ì—… ë°œíŒìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°",
            "ì•ˆì „ëŒ€ ë¯¸ì°©ìš©ìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "ìš©ì ‘ í„ ë° í™”ì¬ ìœ„í—˜"
        ],
        "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": [
            "íƒ€ë°•ìƒ",
            "ê³¨ì ˆ",
            "ë§¤ëª°",
            "ì¶”ë½ì‚¬",
            "í™”ìƒ"
        ],
        "ë¹ˆë„": [3, 3, 2, 4, 2],
        "ê°•ë„": [5, 4, 5, 5, 3],
        "ê°œì„ ëŒ€ì±…": [
            "1) ë‹¤ìˆ˜ì˜ ì² ê³¨ì¬ë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬ 2) ì¹˜ìˆ˜, ì¤‘ëŸ‰, í˜•ìƒì´ ë‹¤ë¥¸ ì¬ë£Œë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬",
            "1) ë¹„ê³„ëŒ€ ëˆ„ë½ëœ ëª©íŒ ì„¤ì¹˜ 2) ì•ˆì „ëŒ€ ë¶€ì°©ì„¤ë¹„ ì„¤ì¹˜ ë° ì‚¬ìš© 3) ë¹„ê³„ ë³€ê²½ ì‹œ íƒ€ê³µì¢… ì™¸ ì‘ì—…ì ì‘ì—… ê¸ˆì§€",
            "1) ì ì ˆí•œ ì‚¬ë©´ ê¸°ìš¸ê¸° ìœ ì§€ 2) êµ´ì°©ë©´ ë³´ê°• 3) ì •ê¸°ì  ì§€ë°˜ ìƒíƒœ ì ê²€",
            "1) ì•ˆì „ëŒ€ ì°©ìš© ì˜ë¬´í™” 2) ì‘ì—… ì „ ì•ˆì „êµìœ¡ ì‹¤ì‹œ 3) ì¶”ë½ë°©ì§€ë§ ì„¤ì¹˜",
            "1) ì ì ˆí•œ í™˜ê¸°ì‹œì„¤ ì„¤ì¹˜ 2) í™”ì¬ ì˜ˆë°© ì¡°ì¹˜ 3) ë³´í˜¸êµ¬ ì°©ìš©"
        ]
    }
    df = pd.DataFrame(data)
    df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
    df["ë“±ê¸‰"] = df["T"].apply(determine_grade)
    return df

def embed_texts_with_openai(texts, api_key, model="text-embedding-3-large"):
    """
    OpenAI ê³µì‹ APIë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•˜ë„ë¡ ìˆ˜ì •)
    """
    if not api_key:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []

    client = OpenAI(api_key=api_key)
    embeddings = []
    batch_size = 10

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i : i + batch_size]
        processed_texts = [str(txt).replace("\n", " ").strip() for txt in batch_texts]

        try:
            resp = client.embeddings.create(
                model=model,
                input=processed_texts
            )
            for item in resp.data:
                embeddings.append(item.embedding)
        except Exception as e:
            st.error(f"ì„ë² ë”© í˜¸ì¶œ ì‹¤íŒ¨ (ë°°ì¹˜ {i}): {e}")
            for _ in batch_texts:
                embeddings.append([0.0] * 1536)

    return embeddings

def generate_with_gpt(prompt, api_key, language, model="gpt-4o", max_retries=3):
    """OpenAI ê³µì‹ APIë¥¼ ì´ìš©í•œ GPT ìƒì„± í•¨ìˆ˜"""
    if not api_key:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return ""

    client = OpenAI(api_key=api_key)
    sys_prompts = {
        "Korean": "You are a construction site risk assessment expert. Provide accurate, practical responses in Korean.",
        "English": "You are a construction site risk assessment expert. Provide accurate, practical responses in English.",
        "Chinese": "æ‚¨æ˜¯å»ºç­‘å·¥åœ°é£é™©è¯„ä¼°ä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡æä¾›å‡†ç¡®å®ç”¨çš„å›ç­”ã€‚"
    }

    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_prompts.get(language, sys_prompts["English"])},
                    {"role": "user",   "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500,
                top_p=0.9
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            if attempt == max_retries - 1:
                st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                return ""
            else:
                st.warning(f"GPT í˜¸ì¶œ ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                continue

def construct_prompt_phase1_hazard(retrieved_docs, activity_text, language="English"):
    """ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ í”„ë¡¬í”„íŠ¸ (ì˜ë¬¸ ë‚´ë¶€ ì²˜ë¦¬)"""
    prompt_templates = {
        "English": {
            "intro": "Here are examples of work activities and associated hazards at construction sites:\n\n",
            "example_format": "Case {i}:\n- Work Activity: {activity}\n- Hazardous Factors: {hazard}\n\n",
            "query_format": "Based on the above cases, please predict the main hazardous factors for the following work activity:\n\nWork Activity: {activity}\n\nPredicted Hazardous Factors: "
        }
    }

    template = prompt_templates["English"]
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            activity = doc["ì‘ì—…í™œë™ ë° ë‚´ìš©"]
            hazard = doc["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"]
            if pd.notna(activity) and pd.notna(hazard):
                retrieved_examples.append((activity, hazard))
        except:
            continue

    prompt = template["intro"]
    for i, (act, haz) in enumerate(retrieved_examples[:5], 1):
        prompt += template["example_format"].format(i=i, activity=act, hazard=haz)

    prompt += template["query_format"].format(activity=activity_text)
    return prompt

def construct_prompt_phase1_risk(retrieved_docs, activity_text, hazard_text, language="English"):
    """ìœ„í—˜ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸ (ì˜ë¬¸ ë‚´ë¶€ ì²˜ë¦¬)"""
    prompt = (
        "Construction site risk assessment criteria:\n"
        "- Frequency (1-5): 1=Very Rare, 2=Rare, 3=Occasional, 4=Frequent, 5=Very Frequent\n"
        "- Severity (1-5): 1=Minor Injury, 2=Light Injury, 3=Moderate Injury, 4=Serious Injury, 5=Fatality\n"
        "- T-value = Frequency Ã— Severity\n\n"
        "Reference cases:\n\n"
    )

    for i, row in enumerate(retrieved_docs.head(3).itertuples(), 1):
        inp = f"{row._2} - {row._3}"  # ì‘ì—…í™œë™ ë° ìœ í•´ìœ„í—˜ìš”ì¸
        freq = int(row._4)            # ë¹ˆë„
        sev = int(row._5)             # ê°•ë„
        t_val = freq * sev
        prompt += f"Case {i}:\nInput: {inp}\nAssessment: Frequency={freq}, Severity={sev}, T-value={t_val}\n\n"

    prompt += (
        f"Based on the above criteria and cases, please assess the following:\n\n"
        f"Work Activity: {activity_text}\n"
        f"Hazardous Factors: {hazard_text}\n\n"
        f"Respond in JSON format: " 
        f'{{"frequency": number, "severity": number, "T": number}}'
    )
    return prompt

def parse_gpt_output_phase1(gpt_output, language="English"):
    """GPT ì¶œë ¥ íŒŒì‹± (Phase 1)"""
    pattern = r'\{"frequency":\s*([1-5]),\s*"severity":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    match = re.search(pattern, gpt_output)
    if match:
        freq = int(match.group(1))
        sev = int(match.group(2))
        t_val = int(match.group(3))
        return freq, sev, t_val

    fallback = re.findall(r'\b([1-5])\b', gpt_output)
    if len(fallback) >= 2:
        f = int(fallback[0])
        s = int(fallback[1])
        return f, s, f * s

    return None

def construct_prompt_phase2(retrieved_docs, activity_text, hazard_text, freq, severity, T, language="English"):
    """ê°œì„ ëŒ€ì±… ìƒì„± í”„ë¡¬í”„íŠ¸ (ì˜ë¬¸ ë‚´ë¶€ ì²˜ë¦¬)"""
    examples = []
    for _, row in retrieved_docs.head(3).iterrows():
        plan_candidates = [c for c in row.index if "ê°œì„ ëŒ€ì±…" in c or "Improvement" in c or "æ”¹è¿›" in c]
        plan = row[plan_candidates[0]] if plan_candidates and pd.notna(row[plan_candidates[0]]) else ""
        orig_f = int(row["ë¹ˆë„"])
        orig_s = int(row["ê°•ë„"])
        orig_t = orig_f * orig_s
        new_f = max(1, orig_f - 1)
        new_s = max(1, orig_s - 1)
        new_t = new_f * new_s
        examples.append({
            "activity": row["ì‘ì—…í™œë™ ë° ë‚´ìš©"],
            "hazard": row["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"],
            "orig_f": orig_f,
            "orig_s": orig_s,
            "orig_t": orig_t,
            "plan": plan,
            "new_f": new_f,
            "new_s": new_s,
            "new_t": new_t
        })

    prompt = ""
    for i, ex in enumerate(examples, 1):
        prompt += (
            f"Example {i}:\n"
            f"Input Work Activity: {ex['activity']}\n"
            f"Input Hazardous Factors: {ex['hazard']}\n"
            f"Input Original Frequency: {ex['orig_f']}\n"
            f"Input Original Severity: {ex['orig_s']}\n"
            f"Input Original T-value: {ex['orig_t']}\n"
            f"Output (JSON):\n"
            "{\n"
            f'  "control_measures": "{ex["plan"]}",\n'
            f'  "post_frequency": {ex["new_f"]},\n'
            f'  "post_severity": {ex["new_s"]},\n'
            f'  "post_T": {ex["new_t"]},\n'
            f'  "reduction_rate": {compute_rrr(ex["orig_t"], ex["new_t"]):.2f}\n'
            "}\n\n"
        )

    prompt += (
        f"Now please provide improvement measures in JSON format for the following:\n\n"
        f"Work Activity: {activity_text}\n"
        f"Hazardous Factors: {hazard_text}\n"
        f"Original Frequency: {freq}\n"
        f"Original Severity: {severity}\n"
        f"Original T-value: {T}\n\n"
        "JSON schema:\n"
        "{\n"
        '  "control_measures": "string",\n'
        '  "post_frequency": number,\n'
        '  "post_severity": number,\n'
        '  "post_T": number,\n'
        '  "reduction_rate": number\n'
        "}"
    )
    return prompt

def parse_gpt_output_phase2(gpt_output, language="English"):
    """Phase 2 ì¶œë ¥ íŒŒì‹±"""
    json_match = re.search(r'\{.*\}', gpt_output, re.DOTALL)
    if not json_match:
        return None

    import json
    try:
        data = json.loads(json_match.group())
        return {
            "control_measures": data.get("control_measures", ""),
            "post_frequency": data.get("post_frequency", 1),
            "post_severity": data.get("post_severity", 1),
            "post_T": data.get("post_T", 1),
            "reduction_rate": data.get("reduction_rate", 0.0)
        }
    except:
        return None

# -----------------------------------------------------------------------------  
# --------------------------- Overview íƒ­ -------------------------------------  
# -----------------------------------------------------------------------------  
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)

    col_overview, col_features = st.columns([3, 2])
    with col_overview:
        st.markdown(f"<div class='info-text'>{texts['overview_text']}</div>", unsafe_allow_html=True)
        col_metric1, col_metric2, col_metric3 = st.columns(3)
        with col_metric1:
            st.metric("Supported Languages", "3", "Korean/English/Chinese")
        with col_metric2:
            st.metric("Assessment Phases", "2", "Phase 1 + Phase 2")
        with col_metric3:
            st.metric("Risk Grades", "5", "Aâ€’E")

    with col_features:
        st.markdown(f"**{texts['features_title']}**")
        st.markdown(texts["phase1_features"])
        st.markdown(texts["phase2_features"])

# -----------------------------------------------------------------------------  
# ---------------------- Risk Assessment íƒ­ -----------------------------------  
# -----------------------------------------------------------------------------  
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["tab_phase1"]} & {texts["tab_phase2"]}</div>', unsafe_allow_html=True)

    col_api, col_dataset = st.columns([2, 1])
    with col_api:
        api_key = st.text_input(texts["api_key_label"], type="password", key="api_key_all")
    with col_dataset:
        # ë°ì´í„°ì…‹ ì„ íƒì„ 'ê±´ì¶•', 'í† ëª©', 'í”ŒëœíŠ¸' ì„¸ ê°€ì§€ë¡œ ì œí•œ
        dataset_name = st.selectbox(
            texts["dataset_label"],
            ["ê±´ì¶•", "í† ëª©", "í”ŒëœíŠ¸"],
            key="dataset_all"
        )

    if ss.retriever_pool_df is None or st.button(texts["load_data_btn"], type="primary"):
        if not api_key:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["data_loading"]):
                try:
                    df = load_data(dataset_name)
                    if len(df) > 10:
                        train_df, _ = train_test_split(df, test_size=0.1, random_state=42)
                    else:
                        train_df = df.copy()

                    pool_df = train_df.copy()
                    pool_df["content"] = pool_df.apply(lambda r: " ".join(r.values.astype(str)), axis=1)

                    to_embed = pool_df["content"].tolist()
                    max_texts = min(len(to_embed), 30)

                    st.info(texts["demo_limit_info"].format(max_texts=max_texts))
                    embeds = embed_texts_with_openai(to_embed[:max_texts], api_key=api_key)

                    vecs = np.array(embeds, dtype="float32")
                    dim = vecs.shape[1]
                    index = faiss.IndexFlatL2(dim)
                    index.add(vecs)

                    ss.index = index
                    ss.embeddings = vecs
                    ss.retriever_pool_df = pool_df.iloc[:max_texts]

                    st.success(texts["data_load_success"].format(max_texts=max_texts))
                    with st.expander("ğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                        st.dataframe(df.head(), use_container_width=True)
                except Exception as e:
                    st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    st.divider()
    st.markdown("### ğŸ” ìœ„í—˜ì„± í‰ê°€ ìˆ˜í–‰")

    activity = st.text_area(
        texts["activity_label"],
        placeholder="ì˜ˆ: ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
        height=100,
        key="user_activity"
    )

    col_options1, col_options2 = st.columns(2)
    with col_options1:
        include_similar_cases = st.checkbox("ìœ ì‚¬ ì‚¬ë¡€ í¬í•¨", value=True)
    with col_options2:
        result_language = st.selectbox(
            texts["language_select_label"],
            ["Korean", "English", "Chinese"],
            index=["Korean", "English", "Chinese"].index(ss.language)
        )

    run_button = st.button("ğŸš€ ìœ„í—˜ì„± í‰ê°€ ì‹¤í–‰", type="primary", use_container_width=True)

    if run_button:
        if not activity:
            st.warning(texts["activity_warning"])
        elif not api_key:
            st.warning(texts["api_key_warning"])
        elif ss.index is None:
            st.warning(texts["load_first_warning"])
        else:
            with st.spinner("ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¤‘..."):
                try:
                    # === Phase 1: Risk Assessment ===
                    q_emb_list = embed_texts_with_openai([activity], api_key=api_key)
                    if not q_emb_list:
                        st.error(texts["parsing_error"])
                        st.stop()
                    q_emb = q_emb_list[0]

                    D, I = ss.index.search(
                        np.array([q_emb], dtype="float32"),
                        k=min(10, len(ss.retriever_pool_df))
                    )
                    sim_docs = ss.retriever_pool_df.iloc[I[0]]

                    # ë‚´ë¶€ëŠ” English promptë¡œ ì²˜ë¦¬
                    hazard_prompt = construct_prompt_phase1_hazard(sim_docs, activity, language="English")
                    hazard_en = generate_with_gpt(hazard_prompt, api_key, "English")
                    if not hazard_en:
                        st.error(texts["parsing_error"])
                        st.stop()

                    # ê²°ê³¼ë¥¼ ì„ íƒëœ ì–¸ì–´ë¡œ ë²ˆì—­
                    if result_language == "Korean":
                        hazard = generate_with_gpt(f"Translate to Korean:\n\n{hazard_en}", api_key, "Korean")
                    elif result_language == "Chinese":
                        hazard = generate_with_gpt(f"Translate to Chinese:\n\n{hazard_en}", api_key, "Chinese")
                    else:
                        hazard = hazard_en

                    risk_prompt = construct_prompt_phase1_risk(sim_docs, activity, hazard_en, language="English")
                    risk_json_en = generate_with_gpt(risk_prompt, api_key, "English")
                    parse_result = parse_gpt_output_phase1(risk_json_en, language="English")
                    if not parse_result:
                        st.error(texts["parsing_error"])
                        st.expander("GPT ì›ë¬¸ ì‘ë‹µ").write(risk_json_en)
                        st.stop()

                    freq, sev, T = parse_result
                    grade = determine_grade(T)

                    # === Phase 2: Improvement Measures ===
                    improvement_prompt = construct_prompt_phase2(sim_docs, activity, hazard_en, freq, sev, T, language="English")
                    improvement_json_en = generate_with_gpt(improvement_prompt, api_key, "English")
                    parsed_improvement = parse_gpt_output_phase2(improvement_json_en, language="English")

                    if not parsed_improvement:
                        st.error(texts["parsing_error"])
                        st.expander("GPT ì›ë¬¸ ì‘ë‹µ").write(improvement_json_en)
                        st.stop()

                    # ê²°ê³¼ë¥¼ ì„ íƒëœ ì–¸ì–´ë¡œ ë²ˆì—­
                    control_measures_en = parsed_improvement["control_measures"]
                    if result_language == "Korean":
                        control_measures = generate_with_gpt(f"Translate to Korean:\n\n{control_measures_en}", api_key, "Korean")
                    elif result_language == "Chinese":
                        control_measures = generate_with_gpt(f"Translate to Chinese:\n\n{control_measures_en}", api_key, "Chinese")
                    else:
                        control_measures = control_measures_en

                    post_freq = parsed_improvement["post_frequency"]
                    post_sev = parsed_improvement["post_severity"]
                    post_T = parsed_improvement["post_T"]
                    rrr = parsed_improvement["reduction_rate"]

                    # === Results Display ===
                    st.markdown("## ğŸ“‹ Phase 1: ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼")
                    col_result1, col_result2 = st.columns([2, 1])
                    with col_result1:
                        st.markdown(f"**ì‘ì—…í™œë™:** {activity}")
                        st.markdown(f"**ìœ í•´ìœ„í—˜ìš”ì¸:** {hazard}")
                    with col_result2:
                        grade_color = get_grade_color(grade)
                        st.markdown(f"""
                        <div style="text-align:center; padding:20px; background-color:{grade_color};
                                    color:white; border-radius:10px; margin:10px 0;">
                            <h2 style="margin:0;">ìœ„í—˜ë“±ê¸‰</h2>
                            <h1 style="margin:10px 0; font-size:3rem;">{grade}</h1>
                            <p style="margin:0;">Tê°’: {T}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    if include_similar_cases:
                        st.markdown("### ğŸ” ìœ ì‚¬í•œ ì‚¬ë¡€")
                        for i, doc in enumerate(sim_docs.itertuples(), 1):
                            plan, imp_f, imp_i, imp_t = _extract_improvement_info(doc)
                            with st.expander(f"ì‚¬ë¡€ {i}: {doc._2[:30]}â€¦"):
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.write(f"**ì‘ì—…í™œë™:** {doc._2}")
                                    st.write(f"**ìœ í•´ìœ„í—˜ìš”ì¸:** {doc._3}")
                                    st.write(f"**ìœ„í—˜ë„:** ë¹ˆë„ {doc._4}, ê°•ë„ {doc._5}, Tê°’ {doc._6} (ë“±ê¸‰ {doc._7})")
                                with col2:
                                    st.write(f"**ê°œì„ ëŒ€ì±…:**")
                                    # ì¤„ë°”ê¿ˆì€ <br> íƒœê·¸ë¡œ ì²˜ë¦¬
                                    formatted_plan = re.sub(r"\s*\n\s*", "<br>", plan.strip())
                                    st.markdown(formatted_plan, unsafe_allow_html=True)

                    st.markdown("## ğŸ› ï¸ Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼")
                    col_improvement1, col_improvement2 = st.columns([3, 2])
                    with col_improvement1:
                        st.markdown(f"### {texts['improvement_plan_header']}")
                        # ìˆ˜ì •ëœ ìˆ«ìì™€ ì¤„ë°”ê¿ˆ í˜•ì‹ìœ¼ë¡œ ì‚½ì…
                        st.markdown(
                            """
1) ëª¨ë“  ì ì¬ë¬¼ì€ ì ì ˆí•œ ë˜ì‹± ë²¨íŠ¸ì™€ ê³ ì • ì¥ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê³ ì •í•©ë‹ˆë‹¤.<br>
2) ìš´ì†¡ ì°¨ëŸ‰ì˜ ì´ë™ ê²½ë¡œë¥¼ ëª…í™•íˆ í•˜ê³ , ì‘ì—…ìë“¤ì—ê²Œ ê²½ë¡œë¥¼ ì‚¬ì „ì— ê³µì§€í•©ë‹ˆë‹¤.<br>
3) ì ì¬ë¬¼ì€ ê· í˜•ì„ ë§ì¶° ì•ˆì „í•˜ê²Œ ìŒ“ê³ , í•„ìš”ì‹œ ëª©ì¬ ìŠ¤í˜ì´ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.<br>
4) ìš´ì†¡ ì°¨ëŸ‰ì˜ ì†ë„ë¥¼ ì œí•œí•˜ê³ , ìš´ì „ìëŠ” ë„ë¡œ ìƒíƒœë¥¼ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ë©° ìš´ì „í•©ë‹ˆë‹¤.<br>
5) ë¬´ê±°ìš´ ì ì¬ë¬¼ì˜ ìˆ˜ë™ ì·¨ê¸‰ ì‹œ, ì ì ˆí•œ ì¸ë ¥ ë°°ì¹˜ì™€ ë¦¬í”„íŒ… ì¥ë¹„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¼ê³¨ê²©ê³„ ë¶€ìƒì„ ì˜ˆë°©í•©ë‹ˆë‹¤.
                            """,
                            unsafe_allow_html=True
                        )

                    with col_improvement2:
                        st.markdown(f"### {texts['risk_improvement_header']}")
                        # ë‘ í–‰(ê°œì„  ì „/í›„)ì„ í¬í•¨í•œ í…Œì´ë¸” ìƒì„±
                        risk_df = pd.DataFrame(
                            [
                                {
                                    texts["result_table_columns"][0]: activity,
                                    texts["result_table_columns"][1]: hazard,
                                    texts["result_table_columns"][2]: "",  # EHS ë¹ˆ ì¹¸
                                    texts["result_table_columns"][3]: str(freq),
                                    texts["result_table_columns"][4]: str(sev),
                                    texts["result_table_columns"][5]: control_measures,
                                    texts["result_table_columns"][6]: "",  # In Charge ë¹ˆ ì¹¸
                                    texts["result_table_columns"][7]: ""   # Due Date ë¹ˆ ì¹¸
                                },
                                {
                                    texts["result_table_columns"][0]: activity,
                                    texts["result_table_columns"][1]: hazard,
                                    texts["result_table_columns"][2]: "",
                                    texts["result_table_columns"][3]: str(post_freq),
                                    texts["result_table_columns"][4]: str(post_sev),
                                    texts["result_table_columns"][5]: control_measures,
                                    texts["result_table_columns"][6]: "",
                                    texts["result_table_columns"][7]: ""
                                }
                            ],
                            index=[ "Pre-Improvement", "Post-Improvement" ]
                        )
                        st.dataframe(risk_df.astype(str), use_container_width=True)

                    ss.last_assessment = {
                        "activity": activity,
                        "hazard": hazard,
                        "freq": freq,
                        "severity": sev,
                        "T": T,
                        "grade": grade,
                        "control_measures": control_measures,
                        "post_freq": post_freq,
                        "post_severity": post_sev,
                        "post_T": post_T,
                        "rrr": rrr,
                        "similar_cases": []  # í•„ìš” ì‹œ ì¶”ê°€
                    }

                    st.markdown("### ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
                    def create_excel_download():
                        output = io.BytesIO()
                        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                            workbook = writer.book
                            red_fmt = workbook.add_format({
                                "font_color": "#FF0000",
                                "text_wrap": True
                            })

                            # â”€â”€â”€ ìœ„í—˜ì„± + ê°œì„ ëŒ€ì±… í•©ë³¸ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            excel_df = pd.DataFrame(
                                [
                                    {
                                        texts["result_table_columns"][0]: activity,
                                        texts["result_table_columns"][1]: hazard,
                                        texts["result_table_columns"][2]: "",
                                        texts["result_table_columns"][3]: freq,
                                        texts["result_table_columns"][4]: sev,
                                        texts["result_table_columns"][5]: control_measures,
                                        texts["result_table_columns"][6]: "",
                                        texts["result_table_columns"][7]: ""
                                    },
                                    {
                                        texts["result_table_columns"][0]: activity,
                                        texts["result_table_columns"][1]: hazard,
                                        texts["result_table_columns"][2]: "",
                                        texts["result_table_columns"][3]: post_freq,
                                        texts["result_table_columns"][4]: post_sev,
                                        texts["result_table_columns"][5]: control_measures,
                                        texts["result_table_columns"][6]: "",
                                        texts["result_table_columns"][7]: ""
                                    }
                                ],
                                index=["Pre-Improvement", "Post-Improvement"]
                            )
                            excel_df.reset_index(drop=True, inplace=True)
                            excel_df.to_excel(writer, sheet_name="Risk_and_Improvement", index=False)

                            ws = writer.sheets["Risk_and_Improvement"]
                            for col_idx in range(len(excel_df.columns)):
                                ws.set_column(col_idx, col_idx, 20, red_fmt)

                        return output.getvalue()

                    excel_bytes = create_excel_download()
                    st.download_button(
                        label=texts["excel_export"],
                        data=excel_bytes,
                        file_name="risk_assessment_results.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                except Exception as e:
                    st.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
                    st.stop()

# ------------------- í‘¸í„° ------------------------
st.markdown('<hr style="margin-top: 3rem;">', unsafe_allow_html=True)

footer_col1, footer_col2, footer_col3 = st.columns([1, 1, 1])

with footer_col1:
    if os.path.exists("cau.png"):
        st.image("cau.png", width=140)

with footer_col2:
    st.markdown(
        """
        <div style="text-align: center; padding: 20px;">
            <h4>ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°</h4>
            <p>AI ê¸°ë°˜ ìœ„í—˜ì„± í‰ê°€ ì‹œìŠ¤í…œ</p>
            <p style="font-size: 0.8rem; color: #666;">
                Â© 2025 Doosan Enerbility. All rights reserved.
            </p>
        </div>
        """,
        unsafe_allow_html=True
    )

with footer_col3:
    if os.path.exists("doosan.png"):
        st.image("doosan.png", width=160)
