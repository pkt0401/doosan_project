import streamlit as st
import pandas as pd
import numpy as np
import faiss
import openai
import re
import os
import io
from PIL import Image
from sklearn.model_selection import train_test_split

# ------------- ì‹œìŠ¤í…œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ -----------------
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_phase1": "ìœ„í—˜ì„± í‰ê°€ (Phase 1)",
        "tab_phase2": "ê°œì„ ëŒ€ì±… ìƒì„± (Phase 2)",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹° AI Risk AssessmentëŠ” êµ­ë‚´ ë° í•´ì™¸ ê±´ì„¤í˜„ì¥ 'ìˆ˜ì‹œìœ„í—˜ì„±í‰ê°€' ë° 'ë…¸ë™ë¶€ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€'ë¥¼ í•™ìŠµí•˜ì—¬ ê°œë°œëœ ìë™ ìœ„í—˜ì„±í‰ê°€ í”„ë¡œê·¸ë¨ ì…ë‹ˆë‹¤. ìƒì„±ëœ ìœ„í—˜ì„±í‰ê°€ëŠ” ë°˜ë“œì‹œ ìˆ˜ì‹œ ìœ„í—˜ì„±í‰ê°€ ì‹¬ì˜íšŒë¥¼ í†µí•´ ê²€ì¦ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase1_features": """
        #### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”
        - ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ
        - ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡
        - ìœ ì‚¬ ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ
        - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì •
        - Excel ê¸°ë°˜ ê³µì •ë³„ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° ìë™ ë¶„ì„
        - ìœ„í—˜ë“±ê¸‰(A-E) ìë™ ì‚°ì •
        """,
        "phase2_features": """
        #### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ìœ„í—˜ìš”ì†Œë³„ ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ë‹¤êµ­ì–´(í•œ/ì˜/ì¤‘) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›
        - ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„
        - ìœ„í—˜ ê°ì†Œìœ¨(RRR) ì •ëŸ‰ì  ì‚°ì¶œ
        - ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        """,
        "phase1_header": "ìœ„í—˜ì„± í‰ê°€ ìë™í™” (Phase 1)",
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_label": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "hazard_prediction_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "predict_hazard_btn": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡í•˜ê¸°",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "predicting_hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” ì¤‘...",
        "similar_cases_header": "ìœ ì‚¬í•œ ì‚¬ë¡€",
        "similar_case_text": """
        <div class="similar-case">
            <strong>ì‚¬ë¡€ {i}</strong><br>
            <strong>ì‘ì—…í™œë™:</strong> {activity}<br>
            <strong>ìœ í•´ìœ„í—˜ìš”ì¸:</strong> {hazard}<br>
            <strong>ìœ„í—˜ë„:</strong> ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})
        </div>
        """,
        "prediction_result_header": "ì˜ˆì¸¡ ê²°ê³¼",
        "activity_result": "ì‘ì—…í™œë™: {activity}",
        "hazard_result": "ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸: {hazard}",
        "result_table_columns": ["í•­ëª©", "ê°’"],
        "result_table_rows": ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "gpt_response": "GPT ì›ë¬¸ ì‘ë‹µ: {response}",
        "phase2_header": "ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (Phase 2)",
        "language_select_label": "ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ:",
        "input_method_label": "ì…ë ¥ ë°©ì‹ ì„ íƒ:",
        "input_methods": ["Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©", "ì§ì ‘ ì…ë ¥"],
        "phase1_results_header": "Phase 1 í‰ê°€ ê²°ê³¼",
        "risk_level_text": "ìœ„í—˜ë„: ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})",
        "phase1_first_warning": "ë¨¼ì € Phase 1ì—ì„œ ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.",
        "hazard_label": "ìœ í•´ìœ„í—˜ìš”ì¸:",
        "frequency_label": "ë¹ˆë„ (1-5):",
        "intensity_label": "ê°•ë„ (1-5):",
        "t_value_text": "Tê°’: {t_value} (ë“±ê¸‰: {grade})",
        "generate_improvement_btn": "ê°œì„ ëŒ€ì±… ìƒì„±",
        "generating_improvement": "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ëŠ” ì¤‘...",
        "no_data_warning": "Phase 1ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±ì„ ì™„ë£Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "improvement_result_header": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ìœ„í—˜ë„ ê°œì„  ê²°ê³¼",
        "comparison_columns": ["í•­ëª©", "ê°œì„  ì „", "ê°œì„  í›„"],
        "risk_reduction_label": "ìœ„í—˜ ê°ì†Œìœ¨ (RRR)",
        "t_value_change_header": "ìœ„í—˜ë„(Tê°’) ë³€í™”",
        "before_improvement": "ê°œì„  ì „ Tê°’:",
        "after_improvement": "ê°œì„  í›„ Tê°’:",
        "parsing_error_improvement": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "excel_export": "ğŸ“¥ ê²°ê³¼ Excel ë‹¤ìš´ë¡œë“œ",
        "risk_classification": "ìœ„í—˜ë„ ë¶„ë¥˜",
        "supported_languages": "ì§€ì› ì–¸ì–´",
        "languages_count": "3ê°œ",
        "languages_detail": "í•œ/ì˜/ì¤‘",
        "assessment_phases": "í‰ê°€ ë‹¨ê³„",
        "phases_count": "2ë‹¨ê³„",
        "phases_detail": "í‰ê°€+ê°œì„ ",
        "risk_grades": "ìœ„í—˜ë“±ê¸‰",
        "grades_count": "5ë“±ê¸‰",
        "grades_detail": "A~E"
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_phase1": "Risk Assessment (Phase 1)",
        "tab_phase2": "Improvement Measures (Phase 2)",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": "Doosan Enerbility AI Risk Assessment is an automated program trained on both on-demand risk-assessment reports from domestic and overseas construction sites and major-accident cases compiled by Korea's Ministry of Employment and Labor. Please ensure that every generated assessment is reviewed and approved by the On-Demand Risk Assessment Committee before it is used.",
        "features_title": "System Features and Components",
        "phase1_features": """
        #### Phase 1: Risk Assessment Automation
        - Learning risk assessment data according to work activities by process
        - Automatic hazard prediction when work activities are entered
        - Similar case search and display
        - Risk level (frequency, intensity, T) measurement based on large language models (LLM)
        - Automatic analysis of Excel-based process-specific risk assessment data
        - Automatic risk grade (A-E) calculation
        """,
        "phase2_features": """
        #### Phase 2: Automatic Generation of Improvement Measures
        - Automatic generation of customized improvement measures for risk factors
        - Multilingual (Korean/English/Chinese) improvement measure generation support
        - Automatic comparative analysis of risk level (T) before and after improvement
        - Quantitative calculation of Risk Reduction Rate (RRR)
        - Building a database of optimal improvement measures by work type/process
        """,
        "phase1_header": "Risk Assessment Automation (Phase 1)",
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset",
        "load_data_label": "Load Data and Configure Index",
        "load_data_btn": "Load Data and Configure Index",
        "api_key_warning": "Please enter an OpenAI API key to continue.",
        "data_loading": "Loading data and configuring index...",
        "demo_limit_info": "For demo purposes, only embedding {max_texts} texts. In a real environment, all data should be processed.",
        "data_load_success": "Data load and index configuration complete! (Total {max_texts} items processed)",
        "hazard_prediction_header": "Hazard Prediction",
        "load_first_warning": "Please click the [Load Data and Configure Index] button first.",
        "activity_label": "Work Activity:",
        "predict_hazard_btn": "Predict Hazards",
        "activity_warning": "Please enter a work activity.",
        "predicting_hazard": "Predicting hazards...",
        "similar_cases_header": "Similar Cases",
        "similar_case_text": """
        <div class="similar-case">
            <strong>Case {i}</strong><br>
            <strong>Work Activity:</strong> {activity}<br>
            <strong>Hazard:</strong> {hazard}<br>
            <strong>Risk Level:</strong> Frequency {freq}, Intensity {intensity}, T-value {t_value} (Grade {grade})
        </div>
        """,
        "prediction_result_header": "Prediction Results",
        "activity_result": "Work Activity: {activity}",
        "hazard_result": "Predicted Hazard: {hazard}", 
        "result_table_columns": ["Item", "Value"],
        "result_table_rows": ["Frequency", "Intensity", "T Value", "Risk Grade"],
        "parsing_error": "Unable to parse risk assessment results.",
        "gpt_response": "Original GPT Response: {response}",
        "phase2_header": "Automatic Generation of Improvement Measures (Phase 2)",
        "language_select_label": "Select Language for Improvement Measures:",
        "input_method_label": "Select Input Method:",
        "input_methods": ["Use Phase 1 Assessment Results", "Direct Input"],
        "phase1_results_header": "Phase 1 Assessment Results",
        "risk_level_text": "Risk Level: Frequency {freq}, Intensity {intensity}, T-value {t_value} (Grade {grade})",
        "phase1_first_warning": "Please perform a risk assessment in Phase 1 first.",
        "hazard_label": "Hazard:",
        "frequency_label": "Frequency (1-5):",
        "intensity_label": "Intensity (1-5):",
        "t_value_text": "T-value: {t_value} (Grade: {grade})",
        "generate_improvement_btn": "Generate Improvement Measures",
        "generating_improvement": "Generating improvement measures...",
        "no_data_warning": "Data loading and index configuration was not completed in Phase 1. Using basic examples.",
        "improvement_result_header": "Improvement Measure Generation Results",
        "improvement_plan_header": "Improvement Measures",
        "risk_improvement_header": "Risk Level Improvement Results",
        "comparison_columns": ["Item", "Before Improvement", "After Improvement"],
        "risk_reduction_label": "Risk Reduction Rate (RRR)",
        "t_value_change_header": "Risk Level (T-value) Change",
        "before_improvement": "T-value Before Improvement:",
        "after_improvement": "T-value After Improvement:",
        "parsing_error_improvement": "Unable to parse improvement measure generation results.",
        "excel_export": "ğŸ“¥ Download Excel Results",
        "risk_classification": "Risk Classification",
        "supported_languages": "Supported Languages",
        "languages_count": "3 Languages",
        "languages_detail": "KOR/ENG/CHN",
        "assessment_phases": "Assessment Phases",
        "phases_count": "2 Phases",
        "phases_detail": "Assessment+Improvement",
        "risk_grades": "Risk Grades",
        "grades_count": "5 Grades", 
        "grades_detail": "A~E"
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_phase1": "é£é™©è¯„ä¼° (ç¬¬1é˜¶æ®µ)",
        "tab_phase2": "æ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "overview_header": "åŸºäºLLMçš„é£é™©è¯„ä¼°ç³»ç»Ÿ",
        "overview_text": "Doosan Enerbility AI é£é™©è¯„ä¼°ç³»ç»Ÿæ˜¯ä¸€æ¬¾è‡ªåŠ¨åŒ–é£é™©è¯„ä¼°ç¨‹åºï¼ŒåŸºäºå›½å†…å¤–æ–½å·¥ç°åœºçš„'ä¸´æ—¶é£é™©è¯„ä¼°'æ•°æ®ä»¥åŠéŸ©å›½é›‡ä½£åŠ³åŠ¨éƒ¨çš„é‡å¤§äº‹æ•…æ¡ˆä¾‹è¿›è¡Œè®­ç»ƒå¼€å‘è€Œæˆã€‚ç”Ÿæˆçš„é£é™©è¯„ä¼°ç»“æœå¿…é¡»ç»è¿‡ä¸´æ—¶é£é™©è¯„ä¼°å®¡è®®å§”å‘˜ä¼šçš„å®¡æ ¸åæ–¹å¯ä½¿ç”¨",
        "features_title": "ç³»ç»Ÿç‰¹ç‚¹å’Œç»„ä»¶",
        "phase1_features": """
        #### ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°è‡ªåŠ¨åŒ–
        - æŒ‰å·¥åºå­¦ä¹ ä¸å·¥ä½œæ´»åŠ¨ç›¸å…³çš„é£é™©è¯„ä¼°æ•°æ®
        - è¾“å…¥å·¥ä½œæ´»åŠ¨æ—¶è‡ªåŠ¨é¢„æµ‹å±å®³å› ç´ 
        - ç›¸ä¼¼æ¡ˆä¾‹æœç´¢å’Œæ˜¾ç¤º
        - åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„é£é™©ç­‰çº§ï¼ˆé¢‘ç‡ã€å¼ºåº¦ã€Tå€¼ï¼‰æµ‹é‡
        - è‡ªåŠ¨åˆ†æåŸºäºExcelçš„ç‰¹å®šå·¥åºé£é™©è¯„ä¼°æ•°æ®
        - è‡ªåŠ¨è®¡ç®—é£é™©ç­‰çº§(A-E)
        """,
        "phase2_features": """
        #### ç¬¬2é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½
        - ä¸ºé£é™©å› ç´ è‡ªåŠ¨ç”Ÿæˆå®šåˆ¶çš„æ”¹è¿›æªæ–½
        - å¤šè¯­è¨€ï¼ˆéŸ©è¯­/è‹±è¯­/ä¸­æ–‡ï¼‰æ”¹è¿›æªæ–½ç”Ÿæˆæ”¯æŒ
        - æ”¹è¿›å‰åé£é™©ç­‰çº§ï¼ˆTå€¼ï¼‰çš„è‡ªåŠ¨æ¯”è¾ƒåˆ†æ
        - é£é™©é™ä½ç‡(RRR)çš„å®šé‡è®¡ç®—
        - å»ºç«‹æŒ‰å·¥ä½œç±»å‹/å·¥åºçš„æœ€ä½³æ”¹è¿›æªæ–½æ•°æ®åº“
        """,
        "phase1_header": "é£é™©è¯„ä¼°è‡ªåŠ¨åŒ– (ç¬¬1é˜¶æ®µ)",
        "api_key_label": "è¾“å…¥OpenAI APIå¯†é’¥ï¼š",
        "dataset_label": "é€‰æ‹©æ•°æ®é›†",
        "load_data_label": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "load_data_btn": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "api_key_warning": "è¯·è¾“å…¥OpenAI APIå¯†é’¥ä»¥ç»§ç»­ã€‚",
        "data_loading": "æ­£åœ¨åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•...",
        "demo_limit_info": "å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œä»…åµŒå…¥{max_texts}ä¸ªæ–‡æœ¬ã€‚åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œåº”å¤„ç†æ‰€æœ‰æ•°æ®ã€‚",
        "data_load_success": "æ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®å®Œæˆï¼ï¼ˆå…±å¤„ç†{max_texts}ä¸ªé¡¹ç›®ï¼‰",
        "hazard_prediction_header": "å±å®³é¢„æµ‹",
        "load_first_warning": "è¯·å…ˆç‚¹å‡»[åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•]æŒ‰é’®ã€‚",
        "activity_label": "å·¥ä½œæ´»åŠ¨ï¼š",
        "predict_hazard_btn": "é¢„æµ‹å±å®³",
        "activity_warning": "è¯·è¾“å…¥å·¥ä½œæ´»åŠ¨ã€‚",
        "predicting_hazard": "æ­£åœ¨é¢„æµ‹å±å®³...",
        "similar_cases_header": "ç›¸ä¼¼æ¡ˆä¾‹",
        "similar_case_text": """
        <div class="similar-case">
            <strong>æ¡ˆä¾‹ {i}</strong><br>
            <strong>å·¥ä½œæ´»åŠ¨ï¼š</strong> {activity}<br>
            <strong>å±å®³ï¼š</strong> {hazard}<br>
            <strong>é£é™©ç­‰çº§ï¼š</strong> é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, Tå€¼ {t_value} (ç­‰çº§ {grade})
        </div>
        """,
        "prediction_result_header": "é¢„æµ‹ç»“æœ",
        "activity_result": "å·¥ä½œæ´»åŠ¨: {activity}",
        "hazard_result": "é¢„æµ‹çš„å±å®³: {hazard}",
        "result_table_columns": ["é¡¹ç›®", "å€¼"],
        "result_table_rows": ["é¢‘ç‡", "å¼ºåº¦", "Tå€¼", "é£é™©ç­‰çº§"],
        "parsing_error": "æ— æ³•è§£æé£é™©è¯„ä¼°ç»“æœã€‚",
        "gpt_response": "åŸå§‹GPTå“åº”: {response}",
        "phase2_header": "è‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "language_select_label": "é€‰æ‹©æ”¹è¿›æªæ–½çš„è¯­è¨€ï¼š",
        "input_method_label": "é€‰æ‹©è¾“å…¥æ–¹æ³•ï¼š",
        "input_methods": ["ä½¿ç”¨ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ", "ç›´æ¥è¾“å…¥"],
        "phase1_results_header": "ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ",
        "risk_level_text": "é£é™©ç­‰çº§: é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, Tå€¼ {t_value} (ç­‰çº§ {grade})",
        "phase1_first_warning": "è¯·å…ˆåœ¨ç¬¬1é˜¶æ®µè¿›è¡Œé£é™©è¯„ä¼°ã€‚",
        "hazard_label": "å±å®³ï¼š",
        "frequency_label": "é¢‘ç‡ (1-5)ï¼š",
        "intensity_label": "å¼ºåº¦ (1-5)ï¼š",
        "t_value_text": "Tå€¼: {t_value} (ç­‰çº§: {grade})",
        "generate_improvement_btn": "ç”Ÿæˆæ”¹è¿›æªæ–½",
        "generating_improvement": "æ­£åœ¨ç”Ÿæˆæ”¹è¿›æªæ–½...",
        "no_data_warning": "åœ¨ç¬¬1é˜¶æ®µæœªå®Œæˆæ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®ã€‚ä½¿ç”¨åŸºæœ¬ç¤ºä¾‹ã€‚",
        "improvement_result_header": "æ”¹è¿›æªæ–½ç”Ÿæˆç»“æœ",
        "improvement_plan_header": "æ”¹è¿›æªæ–½",
        "risk_improvement_header": "é£é™©ç­‰çº§æ”¹è¿›ç»“æœ",
        "comparison_columns": ["é¡¹ç›®", "æ”¹è¿›å‰", "æ”¹è¿›å"],
        "risk_reduction_label": "é£é™©é™ä½ç‡ (RRR)",
        "t_value_change_header": "é£é™©ç­‰çº§ (Tå€¼) å˜åŒ–",
        "before_improvement": "æ”¹è¿›å‰Tå€¼ï¼š",
        "after_improvement": "æ”¹è¿›åTå€¼ï¼š",
        "parsing_error_improvement": "æ— æ³•è§£ææ”¹è¿›æªæ–½ç”Ÿæˆç»“æœã€‚",
        "excel_export": "ğŸ“¥ ä¸‹è½½Excelç»“æœ",
        "risk_classification": "é£é™©åˆ†ç±»",
        "supported_languages": "æ”¯æŒè¯­è¨€",
        "languages_count": "3ç§è¯­è¨€",
        "languages_detail": "éŸ©/è‹±/ä¸­",
        "assessment_phases": "è¯„ä¼°é˜¶æ®µ", 
        "phases_count": "2ä¸ªé˜¶æ®µ",
        "phases_detail": "è¯„ä¼°+æ”¹è¿›",
        "risk_grades": "é£é™©ç­‰çº§",
        "grades_count": "5ä¸ªç­‰çº§",
        "grades_detail": "A~E"
    }
}

# ----------------- í˜ì´ì§€ / ìŠ¤íƒ€ì¼ -----------------
st.set_page_config(page_title="Artificial Intelligence Risk Assessment", page_icon="ğŸ› ï¸", layout="wide")

st.markdown("""
<style>
/* ê¸°ì¡´ ìŠ¤íƒ€ì¼ì— ì¶”ê°€ ê°œì„  */
.main-header{font-size:2.5rem;color:#1E88E5;text-align:center;margin-bottom:1rem}
.sub-header{font-size:1.8rem;color:#0D47A1;margin-top:2rem;margin-bottom:1rem}
.metric-container{background-color:#f0f2f6;border-radius:10px;padding:20px;box-shadow:2px 2px 5px rgba(0,0,0,0.1)}
.result-box{background-color:#f8f9fa;border-radius:10px;padding:15px;margin-top:10px;margin-bottom:10px;border-left:5px solid #4CAF50}
.phase-badge{background-color:#4CAF50;color:white;padding:5px 10px;border-radius:15px;font-size:0.8rem;margin-right:10px}
.similar-case{background-color:#f1f8e9;border-radius:8px;padding:12px;margin-bottom:8px;border-left:4px solid #689f38}
.language-selector{position:absolute;top:10px;right:10px;z-index:1000}
.calculation-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:15px;margin:20px 0}
.risk-grade-a{background-color:#ff1744;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
.risk-grade-b{background-color:#ff9800;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
.risk-grade-c{background-color:#ffc107;color:black;padding:5px 10px;border-radius:5px;font-weight:bold}
.risk-grade-d{background-color:#4caf50;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
.risk-grade-e{background-color:#2196f3;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
</style>
""", unsafe_allow_html=True)

# ----------------- ì„¸ì…˜ ìƒíƒœ -----------------
ss = st.session_state
for key, default in {
    "language":"Korean","index":None,"embeddings":None,
    "retriever_pool_df":None,"last_assessment":None
}.items():
    if key not in ss: ss[key]=default

# ----------------- ì–¸ì–´ ì„ íƒ -----------------
col0, colLang = st.columns([6,1])
with colLang:
    lang = st.selectbox("", list(system_texts.keys()), index=list(system_texts.keys()).index(ss.language))
    ss.language = lang
texts = system_texts[ss.language]

# ----------------- í—¤ë” -----------------
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# ----------------- íƒ­ êµ¬ì„± -----------------
tabs = st.tabs([texts["tab_overview"], "Risk Assessment âœ¨"])

# -----------------------------------------------------------------------------
# ---------------------------  ê³µìš© ìœ í‹¸ë¦¬í‹° -----------------------------------
# -----------------------------------------------------------------------------

def determine_grade(value: int):
    """ìœ„í—˜ë„ ë“±ê¸‰ ë¶„ë¥˜ ê°œì„ """
    if 16<=value<=25: return 'A'
    if 10<=value<=15: return 'B'
    if 5<=value<=9: return 'C'
    if 3<=value<=4: return 'D'
    if 1<=value<=2: return 'E'
    return 'Unknown' if ss.language!='Korean' else 'ì•Œ ìˆ˜ ì—†ìŒ'

def get_grade_color(grade):
    """ìœ„í—˜ë“±ê¸‰ë³„ ìƒ‰ìƒ ë°˜í™˜"""
    colors = {
        'A': '#ff1744',  # ë¹¨ê°•
        'B': '#ff9800',  # ì£¼í™©
        'C': '#ffc107',  # ë…¸ë‘
        'D': '#4caf50',  # ì´ˆë¡
        'E': '#2196f3',  # íŒŒë‘
    }
    return colors.get(grade, '#gray')

def compute_rrr(original_t, improved_t):
    """ìœ„í—˜ ê°ì†Œìœ¨ ê³„ì‚°"""
    if original_t == 0:
        return 0
    return ((original_t - improved_t) / original_t) * 100

def _extract_improvement_info(row):
    """
    ìœ ì‚¬ ì‚¬ë¡€ í•œ ê±´ì—ì„œ - ê°œì„ ëŒ€ì±… / ê°œì„  í›„ ë¹ˆë„Â·ê°•ë„Â·T ê°’ì„ ìµœëŒ€í•œ ì°¾ì•„ ë°˜í™˜
    """
    # â‘  ê°œì„ ëŒ€ì±…
    plan_cols = [c for c in row.index if re.search(r'ê°œì„ ëŒ€ì±…|Improvement|æ”¹è¿›', c, re.I)]
    plan = row[plan_cols[0]] if plan_cols else ""

    # â‘¡ ê°œì„  í›„ ë¹ˆë„Â·ê°•ë„Â·T
    cand_sets = [
        ('ê°œì„  í›„ ë¹ˆë„', 'ê°œì„  í›„ ê°•ë„', 'ê°œì„  í›„ T'),
        ('ê°œì„ ë¹ˆë„', 'ê°œì„ ê°•ë„', 'ê°œì„ T'),
        ('improved_frequency', 'improved_intensity', 'improved_T'),
        ('æ”¹è¿›åé¢‘ç‡', 'æ”¹è¿›åå¼ºåº¦', 'æ”¹è¿›åTå€¼'),
    ]
    imp_f, imp_i, imp_t = None, None, None
    for f,i,t in cand_sets:
        if f in row and i in row and t in row:
            imp_f, imp_i, imp_t = int(row[f]), int(row[i]), int(row[t])
            break

    # ê°’ì´ ì—†ìœ¼ë©´ ì›ë˜ ê°’ì—ì„œ ê°ì†Œëœ ê°’ìœ¼ë¡œ ì¶”ì •
    if imp_f is None:
        orig_f, orig_i = int(row['ë¹ˆë„']), int(row['ê°•ë„'])
        # ê°œì„  í›„ ì¶”ì •: ì›ë˜ ê°’ì˜ 50-70% ë²”ìœ„
        imp_f = max(1, orig_f - 1)
        imp_i = max(1, orig_i - 1)
        imp_t = imp_f * imp_i

    return plan, imp_f, imp_i, imp_t

@st.cache_data(show_spinner=False)
def load_data(selected_dataset_name: str):
    """í–¥ìƒëœ ë°ì´í„° ë¡œë”© í•¨ìˆ˜"""
    try:
        # Excel íŒŒì¼ ì½ê¸° ì‹œ ì—¬ëŸ¬ ì‹œíŠ¸ ì²˜ë¦¬ ê°€ëŠ¥
        if os.path.exists(f"{selected_dataset_name}.xlsx"):
            df = pd.read_excel(f"{selected_dataset_name}.xlsx")
        else:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            return create_sample_data()

        # 1) ë¶ˆí•„ìš” ì—´ ì œê±°
        if "ì‚­ì œ Del" in df.columns:
            df.drop(["ì‚­ì œ Del"], axis=1, inplace=True)

        # 2) ë¹ˆ í–‰ ì œê±° (ëª¨ë“  ê°’ì´ NaNì¸ í–‰)
        df = df.dropna(how='all')

        # 3) í•µì‹¬ ì—´ ì´ë¦„ í†µì¼ ë° ì •ë¦¬
        column_mapping = {
            "ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect": "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ\nCorrective Action": "ê°œì„ ëŒ€ì±…"
        }
        
        df.rename(columns=column_mapping, inplace=True)
        
        # ë¹ˆë„, ê°•ë„ ì—´ ì‹ë³„ ë° ë³€í™˜
        numeric_columns = ['ë¹ˆë„', 'ê°•ë„']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # 4) í•„ìˆ˜ ì—´ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
        if 'ë¹ˆë„' not in df.columns:
            df['ë¹ˆë„'] = 3
        if 'ê°•ë„' not in df.columns:
            df['ê°•ë„'] = 3

        # 5) T, ë“±ê¸‰ ê³„ì‚°
        df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
        df["ë“±ê¸‰"] = df["T"].apply(determine_grade)

        # 6) ê°œì„ ëŒ€ì±… ì—´ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
        if "ê°œì„ ëŒ€ì±…" not in df.columns:
            alt_cols = [c for c in df.columns if "ê°œì„ " in c or "ëŒ€ì±…" in c or "Corrective" in c]
            if alt_cols:
                df.rename(columns={alt_cols[0]: "ê°œì„ ëŒ€ì±…"}, inplace=True)
            else:
                df["ê°œì„ ëŒ€ì±…"] = "ì•ˆì „ êµìœ¡ ì‹¤ì‹œ ë° ë³´í˜¸êµ¬ ì°©ìš©"

        # 7) ìµœì¢… ì—´ ì •ë¦¬
        required_cols = [
            "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥", 
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ë¹ˆë„",
            "ê°•ë„", 
            "T",
            "ë“±ê¸‰",
            "ê°œì„ ëŒ€ì±…"
        ]
        
        # ì¡´ì¬í•˜ëŠ” ì—´ë§Œ ì„ íƒ
        final_cols = [col for col in required_cols if col in df.columns]
        df = df[final_cols]

        # 8) ë¹ˆ ê°’ ì²˜ë¦¬
        df = df.fillna({
            "ì‘ì—…í™œë™ ë° ë‚´ìš©": "ì¼ë°˜ ì‘ì—…",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": "ì¼ë°˜ì  ìœ„í—˜",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": "ë¶€ìƒ",
            "ê°œì„ ëŒ€ì±…": "ì•ˆì „ ì¡°ì¹˜ ìˆ˜í–‰"
        })

        return df

    except Exception as e:
        st.warning(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return create_sample_data()

def create_sample_data():
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    data = {
        "ì‘ì—…í™œë™ ë° ë‚´ìš©": [
            "ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "ì½˜í¬ë¦¬íŠ¸/CMU ë¸”ë¡ ì„¤ì¹˜ ì‘ì—…",
            "êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…",
            "ê³ ì†Œ ì‘ì—…ëŒ€ë¥¼ ì´ìš©í•œ ì™¸ë²½ ì‘ì—…",
            "ìš©ì ‘ ì‘ì—…"
        ],
        "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": [
            "ë‹¤ì¤‘ ì¸ì–‘ìœ¼ë¡œ ì¸í•œ ì ì¬ë¬¼ ë‚™í•˜",
            "ë¶ˆì¶©ë¶„í•œ ì‘ì—… ë°œíŒìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°",
            "ì•ˆì „ëŒ€ ë¯¸ì°©ìš©ìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "ìš©ì ‘ í„ ë° í™”ì¬ ìœ„í—˜"
        ],
        "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": [
            "íƒ€ë°•ìƒ",
            "ê³¨ì ˆ",
            "ë§¤ëª°",
            "ì¶”ë½ì‚¬",
            "í™”ìƒ"
        ],
        "ë¹ˆë„": [3, 3, 2, 4, 2],
        "ê°•ë„": [5, 4, 5, 5, 3],
        "ê°œì„ ëŒ€ì±…": [
            "1) ë‹¤ìˆ˜ì˜ ì² ê³¨ì¬ë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬ 2) ì¹˜ìˆ˜, ì¤‘ëŸ‰, í˜•ìƒì´ ë‹¤ë¥¸ ì¬ë£Œë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬",
            "1) ë¹„ê³„ëŒ€ ëˆ„ë½ëœ ëª©íŒ ì„¤ì¹˜ 2) ì•ˆì „ëŒ€ ë¶€ì°©ì„¤ë¹„ ì„¤ì¹˜ ë° ì‚¬ìš© 3) ë¹„ê³„ ë³€ê²½ ì‹œ íƒ€ê³µì¢… ì™¸ ì‘ì—…ì ì‘ì—… ê¸ˆì§€",
            "1) ì ì ˆí•œ ì‚¬ë©´ ê¸°ìš¸ê¸° ìœ ì§€ 2) êµ´ì°©ë©´ ë³´ê°• 3) ì •ê¸°ì  ì§€ë°˜ ìƒíƒœ ì ê²€",
            "1) ì•ˆì „ëŒ€ ì°©ìš© ì˜ë¬´í™” 2) ì‘ì—… ì „ ì•ˆì „êµìœ¡ ì‹¤ì‹œ 3) ì¶”ë½ë°©ì§€ë§ ì„¤ì¹˜",
            "1) ì ì ˆí•œ í™˜ê¸°ì‹œì„¤ ì„¤ì¹˜ 2) í™”ì¬ ì˜ˆë°© ì¡°ì¹˜ 3) ë³´í˜¸êµ¬ ì°©ìš©"
        ]
    }
    df = pd.DataFrame(data)
    df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
    df["ë“±ê¸‰"] = df["T"].apply(determine_grade)
    return df

def embed_texts_with_openai(texts, model="text-embedding-3-large", api_key=None):
    """OpenAIë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    if api_key: 
        openai.api_key = api_key
    
    embeddings = []
    batch_size = 10  # ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_texts = [str(txt).replace("\n", " ").strip() for txt in batch_texts]
            
            resp = openai.Embedding.create(
                model=model, 
                input=processed_texts
            )
            
            for j, item in enumerate(resp["data"]):
                embeddings.append(item["embedding"])
                
        except Exception as e:
            st.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ë”ë¯¸ ì„ë² ë”© ì¶”ê°€
            for _ in batch_texts:
                embeddings.append([0] * 1536)
    
    return embeddings

def generate_with_gpt(prompt, api_key, language, max_retries=3):
    """í–¥ìƒëœ GPT ìƒì„± í•¨ìˆ˜"""
    if api_key: 
        openai.api_key = api_key
    
    sys_prompts = {
        "Korean": "ë‹¹ì‹ ì€ ê±´ì„¤ í˜„ì¥ ìœ„í—˜ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        "English": "You are a construction site risk assessment expert. Provide accurate and practical responses in English.",
        "Chinese": "æ‚¨æ˜¯å»ºç­‘å·¥åœ°é£é™©è¯„ä¼°ä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡æä¾›å‡†ç¡®å®ç”¨çš„å›ç­”ã€‚"
    }
    
    for attempt in range(max_retries):
        try:
            resp = openai.ChatCompletion.create(
                model="gpt-4", 
                messages=[
                    {"role": "system", "content": sys_prompts.get(language, sys_prompts['Korean'])},
                    {"role": "user", "content": prompt}
                ], 
                temperature=0.1,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature
                max_tokens=500,   # í† í° ì œí•œ ì¦ê°€
                top_p=0.9
            )
            return resp['choices'][0]['message']['content'].strip()
            
        except Exception as e:
            if attempt == max_retries - 1:
                st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                return ""
            else:
                st.warning(f"GPT í˜¸ì¶œ ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                continue

# ----------------- í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ë“¤ (ê°œì„ ë¨) -----------------

def construct_prompt_phase1_hazard(retrieved_docs, activity_text, language="Korean"):
    """í–¥ìƒëœ ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ í”„ë¡¬í”„íŠ¸"""
    prompt_templates = {
        "Korean": {
            "intro": "ê±´ì„¤ í˜„ì¥ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ë“¤ì´ ìˆìŠµë‹ˆë‹¤:\n\n",
            "example_format": "ì‚¬ë¡€ {i}:\n- ì‘ì—…í™œë™: {activity}\n- ìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\n\n",
            "query_format": "ìœ„ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì‘ì—…í™œë™ì˜ ì£¼ìš” ìœ í•´ìœ„í—˜ìš”ì¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”:\n\nì‘ì—…í™œë™: {activity}\n\nì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸: "
        },
        "English": {
            "intro": "Here are examples of work activities and associated hazards at construction sites:\n\n",
            "example_format": "Case {i}:\n- Work Activity: {activity}\n- Hazard: {hazard}\n\n", 
            "query_format": "Based on the above cases, please predict the main hazards for the following work activity:\n\nWork Activity: {activity}\n\nPredicted Hazard: "
        },
        "Chinese": {
            "intro": "ä»¥ä¸‹æ˜¯å»ºç­‘å·¥åœ°å·¥ä½œæ´»åŠ¨å’Œç›¸å…³å±å®³çš„ä¾‹å­:\n\n",
            "example_format": "æ¡ˆä¾‹ {i}:\n- å·¥ä½œæ´»åŠ¨: {activity}\n- å±å®³: {hazard}\n\n",
            "query_format": "æ ¹æ®ä¸Šè¿°æ¡ˆä¾‹ï¼Œè¯·é¢„æµ‹ä»¥ä¸‹å·¥ä½œæ´»åŠ¨çš„ä¸»è¦å±å®³:\n\nå·¥ä½œæ´»åŠ¨: {activity}\n\né¢„æµ‹çš„å±å®³: "
        }
    }
    
    template = prompt_templates.get(language, prompt_templates["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            activity = doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']
            hazard = doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']
            if pd.notna(activity) and pd.notna(hazard):
                retrieved_examples.append((activity, hazard))
        except:
            continue
    
    prompt = template["intro"]
    for i, (activity, hazard) in enumerate(retrieved_examples[:5], 1):  # ìµœëŒ€ 5ê°œ ì˜ˆì‹œ
        prompt += template["example_format"].format(i=i, activity=activity, hazard=hazard)
    
    prompt += template["query_format"].format(activity=activity_text)
    return prompt

def construct_prompt_phase1_risk(retrieved_docs, activity_text, hazard_text, language="Korean"):
    """í–¥ìƒëœ ìœ„í—˜ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸"""
    prompt_templates = {
        "Korean": {
            "intro": "ê±´ì„¤ í˜„ì¥ ìœ„í—˜ì„± í‰ê°€ ê¸°ì¤€:\n- ë¹ˆë„(1-5): 1=ë§¤ìš°ë“œë¬¼ê²Œ, 2=ë“œë¬¼ê²Œ, 3=ê°€ë”, 4=ìì£¼, 5=ë§¤ìš°ìì£¼\n- ê°•ë„(1-5): 1=ê²½ë¯¸í•œë¶€ìƒ, 2=ê°€ë²¼ìš´ë¶€ìƒ, 3=ì¤‘ê°„ë¶€ìƒ, 4=ì‹¬ê°í•œë¶€ìƒ, 5=ì‚¬ë§\n- Tê°’ = ë¹ˆë„ Ã— ê°•ë„\n\nì°¸ê³  ì‚¬ë¡€ë“¤:\n\n",
            "example_format": "ì‚¬ë¡€ {i}:\nì…ë ¥: {input}\ní‰ê°€: ë¹ˆë„={freq}, ê°•ë„={intensity}, Tê°’={t_value}\n\n",
            "query_format": "ìœ„ ê¸°ì¤€ê³¼ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒì„ í‰ê°€í•´ì£¼ì„¸ìš”:\n\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”:\n{json_format}"
        },
        "English": {
            "intro": "Construction site risk assessment criteria:\n- Frequency(1-5): 1=Very Rare, 2=Rare, 3=Occasional, 4=Frequent, 5=Very Frequent\n- Intensity(1-5): 1=Minor Injury, 2=Light Injury, 3=Moderate Injury, 4=Serious Injury, 5=Fatality\n- T-value = Frequency Ã— Intensity\n\nReference cases:\n\n",
            "example_format": "Case {i}:\nInput: {input}\nAssessment: Frequency={freq}, Intensity={intensity}, T-value={t_value}\n\n",
            "query_format": "Based on the above criteria and cases, please assess the following:\n\nWork Activity: {activity}\nHazard: {hazard}\n\nRespond in the following JSON format:\n{json_format}"
        },
        "Chinese": {
            "intro": "å»ºç­‘å·¥åœ°é£é™©è¯„ä¼°æ ‡å‡†:\n- é¢‘ç‡(1-5): 1=éå¸¸ç½•è§, 2=ç½•è§, 3=å¶å°”, 4=é¢‘ç¹, 5=éå¸¸é¢‘ç¹\n- å¼ºåº¦(1-5): 1=è½»å¾®ä¼¤å®³, 2=è½»ä¼¤, 3=ä¸­åº¦ä¼¤å®³, 4=ä¸¥é‡ä¼¤å®³, 5=æ­»äº¡\n- Tå€¼ = é¢‘ç‡ Ã— å¼ºåº¦\n\nå‚è€ƒæ¡ˆä¾‹:\n\n",
            "example_format": "æ¡ˆä¾‹ {i}:\nè¾“å…¥: {input}\nè¯„ä¼°: é¢‘ç‡={freq}, å¼ºåº¦={intensity}, Tå€¼={t_value}\n\n",
            "query_format": "æ ¹æ®ä¸Šè¿°æ ‡å‡†å’Œæ¡ˆä¾‹ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹å†…å®¹:\n\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: {hazard}\n\nè¯·ä»¥ä»¥ä¸‹JSONæ ¼å¼å›ç­”:\n{json_format}"
        }
    }
    
    json_formats = {
        "Korean": '{"ë¹ˆë„": ìˆ«ì, "ê°•ë„": ìˆ«ì, "T": ìˆ«ì}',
        "English": '{"frequency": number, "intensity": number, "T": number}',
        "Chinese": '{"é¢‘ç‡": æ•°å­—, "å¼ºåº¦": æ•°å­—, "T": æ•°å­—}'
    }
    
    template = prompt_templates.get(language, prompt_templates["Korean"])
    json_format = json_formats.get(language, json_formats["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            example_input = f"{doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']} - {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}"
            frequency = int(doc['ë¹ˆë„'])
            intensity = int(doc['ê°•ë„'])
            T_value = frequency * intensity
            retrieved_examples.append((example_input, frequency, intensity, T_value))
        except:
            continue
    
    prompt = template["intro"]
    for i, (example_input, freq, intensity, t_val) in enumerate(retrieved_examples[:3], 1):
        prompt += template["example_format"].format(
            i=i, input=example_input, freq=freq, intensity=intensity, t_value=t_val
        )
    
    prompt += template["query_format"].format(
        activity=activity_text, hazard=hazard_text, json_format=json_format
    )
    
    return prompt

def parse_gpt_output_phase1(gpt_output, language="Korean"):
    """í–¥ìƒëœ GPT ì¶œë ¥ íŒŒì‹±"""
    json_patterns = {
        "Korean": r'\{"ë¹ˆë„":\s*([1-5]),\s*"ê°•ë„":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "English": r'\{"frequency":\s*([1-5]),\s*"intensity":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "Chinese": r'\{"é¢‘ç‡":\s*([1-5]),\s*"å¼ºë„":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    }
    
    # í˜„ì¬ ì–¸ì–´ íŒ¨í„´ë¶€í„° ì‹œë„
    pattern = json_patterns.get(language, json_patterns["Korean"])
    match = re.search(pattern, gpt_output)
    
    if match:
        pred_frequency = int(match.group(1))
        pred_intensity = int(match.group(2))
        pred_T = int(match.group(3))
        return pred_frequency, pred_intensity, pred_T
    
    # ë‹¤ë¥¸ ì–¸ì–´ íŒ¨í„´ë“¤ë„ ì‹œë„
    for lang, pattern in json_patterns.items():
        if lang != language:
            match = re.search(pattern, gpt_output)
            if match:
                pred_frequency = int(match.group(1))
                pred_intensity = int(match.group(2))
                pred_T = int(match.group(3))
                return pred_frequency, pred_intensity, pred_T
    
    # ëŒ€ì²´ íŒŒì‹± ì‹œë„ (ìˆ«ìë§Œ ì¶”ì¶œ)
    numbers = re.findall(r'\b([1-5])\b', gpt_output)
    if len(numbers) >= 2:
        freq, intensity = int(numbers[0]), int(numbers[1])
        return freq, intensity, freq * intensity
    
    return None

def construct_prompt_phase2(retrieved_docs, activity_text, hazard_text, freq, intensity, T, target_language="Korean"):
    """í–¥ìƒëœ ê°œì„ ëŒ€ì±… ìƒì„± í”„ë¡¬í”„íŠ¸"""
    # ê¸°ì¡´ construct_prompt_phase2 í•¨ìˆ˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, 
    # ë” êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ë„ë¡ ê°œì„ 
    
    example_section = ""
    examples_added = 0
    
    # ì–¸ì–´ë³„ í•„ë“œëª… (ê¸°ì¡´ê³¼ ë™ì¼)
    field_names = {
        "Korean": {
            "improvement_fields": ['ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ', 'ê°œì„ ëŒ€ì±…', 'ê°œì„ ë°©ì•ˆ'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
        },
        "English": {
            "improvement_fields": ['Improvement Measures', 'Improvement Plan', 'Countermeasures'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
        },
        "Chinese": {
            "improvement_fields": ['æ”¹è¿›æªæ–½', 'æ”¹è¿›è®¡åˆ’', 'å¯¹ç­–'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©", 
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
        }
    }
    
    fields = field_names.get(target_language, field_names["Korean"])
    
    # ì˜ˆì‹œ ë°ì´í„° ìˆ˜ì§‘
    for _, row in retrieved_docs.iterrows():
        try:
            improvement_plan = ""
            for field in fields["improvement_fields"]:
                if field in row and pd.notna(row[field]):
                    improvement_plan = row[field]
                    break
            
            if not improvement_plan:
                continue
                
            original_freq = int(row[fields["freq"]]) if fields["freq"] in row else 3
            original_intensity = int(row[fields["intensity"]]) if fields["intensity"] in row else 3
            original_T = original_freq * original_intensity
                
            # ê°œì„  í›„ ë°ì´í„° (ì¶”ì •)
            improved_freq = max(1, original_freq - 1)
            improved_intensity = max(1, original_intensity - 1) 
            improved_T = improved_freq * improved_intensity
            
            if target_language == "Korean":
                example_section += f"""
ì˜ˆì‹œ {examples_added + 1}:
ì…ë ¥ ì‘ì—…í™œë™: {row[fields['activity']]}
ì…ë ¥ ìœ í•´ìœ„í—˜ìš”ì¸: {row[fields['hazard']]}
ì…ë ¥ ì›ë˜ ë¹ˆë„: {original_freq}
ì…ë ¥ ì›ë˜ ê°•ë„: {original_intensity}
ì…ë ¥ ì›ë˜ Tê°’: {original_T}
ì¶œë ¥ (ê°œì„ ê³„íš ë° ìœ„í—˜ê°ì†Œ) JSON í˜•ì‹:
{{
  "ê°œì„ ëŒ€ì±…": "{improvement_plan}",
  "ê°œì„  í›„ ë¹ˆë„": {improved_freq},
  "ê°œì„  í›„ ê°•ë„": {improved_intensity}, 
  "ê°œì„  í›„ T": {improved_T},
  "T ê°ì†Œìœ¨": {compute_rrr(original_T, improved_T):.2f}
}}

"""
            elif target_language == "English":
                example_section += f"""
Example {examples_added + 1}:
Input Work Activity: {row[fields['activity']]}
Input Hazard: {row[fields['hazard']]}
Input Original Frequency: {original_freq}
Input Original Intensity: {original_intensity}
Input Original T-value: {original_T}
Output (Improvement Plan and Risk Reduction) JSON format:
{{
  "improvement_plan": "{improvement_plan}",
  "improved_frequency": {improved_freq},
  "improved_intensity": {improved_intensity},
  "improved_T": {improved_T},
  "reduction_rate": {compute_rrr(original_T, improved_T):.2f}
}}

"""
            else:  # Chinese
                example_section += f"""
ç¤ºä¾‹ {examples_added + 1}:
è¾“å…¥å·¥ä½œæ´»åŠ¨: {row[fields['activity']]}
è¾“å…¥å±å®³: {row[fields['hazard']]}
è¾“å…¥åŸé¢‘ç‡: {original_freq}
è¾“å…¥åŸå¼ºåº¦: {original_intensity}
è¾“å…¥åŸTå€¼: {original_T}
è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) JSONæ ¼å¼:
{{
  "æ”¹è¿›æªæ–½": "{improvement_plan}",
  "æ”¹è¿›åé¢‘ç‡": {improved_freq},
  "æ”¹è¿›åå¼ºåº¦": {improved_intensity},
  "æ”¹è¿›åTå€¼": {improved_T},
  "Tå€¼é™ä½ç‡": {compute_rrr(original_T, improved_T):.2f}
}}

"""
            
            examples_added += 1
            if examples_added >= 3:
                break
                
        except Exception as e:
            continue
    
    # ê¸°ë³¸ ì˜ˆì‹œ (ì˜ˆì‹œê°€ ì—†ëŠ” ê²½ìš°)
    if examples_added == 0:
        if target_language == "Korean":
            example_section = """
ì˜ˆì‹œ 1:
ì…ë ¥ ì‘ì—…í™œë™: êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…
ì…ë ¥ ìœ í•´ìœ„í—˜ìš”ì¸: êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°
ì…ë ¥ ì›ë˜ ë¹ˆë„: 3
ì…ë ¥ ì›ë˜ ê°•ë„: 4
ì…ë ¥ ì›ë˜ Tê°’: 12
ì¶œë ¥ (ê°œì„ ê³„íš ë° ìœ„í—˜ê°ì†Œ) JSON í˜•ì‹:
{
  "ê°œì„ ëŒ€ì±…": "1) í† ì–‘ ë¶„ë¥˜ì— ë”°ë¥¸ ì ì ˆí•œ ê²½ì‚¬ ìœ ì§€ 2) êµ´ì°© ë²½ë©´ ë³´ê°• 3) ì •ê¸°ì ì¸ ì§€ë°˜ ìƒíƒœ ê²€ì‚¬ ì‹¤ì‹œ",
  "ê°œì„  í›„ ë¹ˆë„": 1,
  "ê°œì„  í›„ ê°•ë„": 2,
  "ê°œì„  í›„ T": 2,
  "T ê°ì†Œìœ¨": 83.33
}

"""
        elif target_language == "English":
            example_section = """
Example 1:
Input Work Activity: Excavation and backfilling
Input Hazard: Collapse of excavation wall
Input Original Frequency: 3
Input Original Intensity: 4
Input Original T-value: 12
Output (Improvement Plan and Risk Reduction) JSON format:
{
  "improvement_plan": "1) Maintain proper slope according to soil classification 2) Reinforce excavation walls 3) Conduct regular ground condition inspections",
  "improved_frequency": 1,
  "improved_intensity": 2,
  "improved_T": 2,
  "reduction_rate": 83.33
}

"""
        else:  # Chinese
            example_section = """
ç¤ºä¾‹ 1:
è¾“å…¥å·¥ä½œæ´»åŠ¨: æŒ–æ˜å’Œå›å¡«ä½œä¸š
è¾“å…¥å±å®³: æŒ–æ˜å¢™å£å€’å¡Œ
è¾“å…¥åŸé¢‘ç‡: 3
è¾“å…¥åŸå¼ºåº¦: 4
è¾“å…¥åŸTå€¼: 12
è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) JSONæ ¼å¼:
{
  "æ”¹è¿›æªæ–½": "1) æ ¹æ®åœŸå£¤åˆ†ç±»ç»´æŒé€‚å½“çš„æ–œå¡ 2) åŠ å›ºæŒ–æ˜å¢™å£ 3) å®šæœŸè¿›è¡Œåœ°é¢çŠ¶å†µæ£€æŸ¥",
  "æ”¹è¿›åé¢‘ç‡": 1,
  "æ”¹è¿›åå¼ºåº¦": 2,
  "æ”¹è¿›åTå€¼": 2,
  "Tå€¼é™ä½ç‡": 83.33
}

"""
    
    # ì–¸ì–´ë³„ JSON í‚¤ ë° ì•ˆë‚´ë¬¸
    json_keys = {
        "Korean": {
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„  í›„ ë¹ˆë„",
            "improved_intensity": "ê°œì„  í›„ ê°•ë„",
            "improved_t": "ê°œì„  í›„ T",
            "reduction_rate": "T ê°ì†Œìœ¨"
        },
        "English": {
            "improvement": "improvement_plan",
            "improved_freq": "improved_frequency", 
            "improved_intensity": "improved_intensity",
            "improved_t": "improved_T",
            "reduction_rate": "reduction_rate"
        },
        "Chinese": {
            "improvement": "æ”¹è¿›æªæ–½",
            "improved_freq": "æ”¹è¿›åé¢‘ç‡",
            "improved_intensity": "æ”¹è¿›åå¼ºåº¦",
            "improved_t": "æ”¹è¿›åTå€¼", 
            "reduction_rate": "Tå€¼é™ä½ç‡"
        }
    }
    
    instructions = {
        "Korean": {
            "new_input": "ì´ì œ ìƒˆë¡œìš´ ì…ë ¥ì…ë‹ˆë‹¤:",
            "output_format": "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ê°œì„ ëŒ€ì±…ì„ ì œê³µí•˜ì„¸ìš”:",
            "requirements": "ê°œì„ ëŒ€ì±…ì€ ì‹¤ì œ í˜„ì¥ì—ì„œ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ë°©ë²•ì„ 3ê°œ ì´ìƒ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ì œì‹œí•˜ì„¸ìš”. ê°œì„  í›„ ë¹ˆë„ì™€ ê°•ë„ëŠ” í•©ë¦¬ì ìœ¼ë¡œ ê°ì†Œëœ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.",
            "output": "ì¶œë ¥:"
        },
        "English": {
            "new_input": "Now here is a new input:",
            "output_format": "Please provide practical and specific improvement measures in the following JSON format:",
            "requirements": "Improvement measures should include at least 3 specific, field-applicable methods in a numbered list. Set improved frequency and intensity to reasonably reduced values.",
            "output": "Output:"
        },
        "Chinese": {
            "new_input": "ç°åœ¨æ˜¯æ–°çš„è¾“å…¥:",
            "output_format": "è¯·ä»¥ä»¥ä¸‹JSONæ ¼å¼æä¾›å®ç”¨ä¸”å…·ä½“çš„æ”¹è¿›æªæ–½:",
            "requirements": "æ”¹è¿›æªæ–½åº”åŒ…æ‹¬è‡³å°‘3ä¸ªå…·ä½“çš„ã€ç°åœºå¯åº”ç”¨çš„æ–¹æ³•ï¼Œä»¥ç¼–å·åˆ—è¡¨å½¢å¼ã€‚å°†æ”¹è¿›åçš„é¢‘ç‡å’Œå¼ºåº¦è®¾ç½®ä¸ºåˆç†é™ä½çš„å€¼ã€‚",
            "output": "è¾“å‡º:"
        }
    }
    
    keys = json_keys.get(target_language, json_keys["Korean"])
    instr = instructions.get(target_language, instructions["Korean"])
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""{example_section}
{instr['new_input']}
ì…ë ¥ ì‘ì—…í™œë™: {activity_text}
ì…ë ¥ ìœ í•´ìœ„í—˜ìš”ì¸: {hazard_text}
ì…ë ¥ ì›ë˜ ë¹ˆë„: {freq}
ì…ë ¥ ì›ë˜ ê°•ë„: {intensity}
ì…ë ¥ ì›ë˜ Tê°’: {T}

{instr['output_format']}
{{
  "{keys["improvement"]}": "ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ êµ¬ì²´ì  ê°œì„ ëŒ€ì±… ë¦¬ìŠ¤íŠ¸",
  "{keys["improved_freq"]}": (1-5 ì‚¬ì´ì˜ ì •ìˆ˜),
  "{keys["improved_intensity"]}": (1-5 ì‚¬ì´ì˜ ì •ìˆ˜),
  "{keys["improved_t"]}": (ê°œì„  í›„ ë¹ˆë„ Ã— ê°œì„  í›„ ê°•ë„),
  "{keys["reduction_rate"]}": (ìœ„í—˜ ê°ì†Œ ë°±ë¶„ìœ¨)
}}

{instr['requirements']}
{instr['output']}
"""
    
    return prompt

def parse_gpt_output_phase2(gpt_output, language="Korean"):
    """í–¥ìƒëœ Phase 2 ì¶œë ¥ íŒŒì‹±"""
    try:
        # JSON ë¸”ë¡ ì¶”ì¶œ
        json_match = re.search(r'```json\s*(.*?)\s*```', gpt_output, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # JSON ê°ì²´ ì§ì ‘ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', gpt_output, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                json_str = gpt_output

        import json
        result = json.loads(json_str)
        
        # ì–¸ì–´ë³„ í‚¤ ë§¤í•‘
        key_mappings = {
            "Korean": {
                "improvement": ["ê°œì„ ëŒ€ì±…"],
                "improved_freq": ["ê°œì„  í›„ ë¹ˆë„", "ê°œì„ ë¹ˆë„"],
                "improved_intensity": ["ê°œì„  í›„ ê°•ë„", "ê°œì„ ê°•ë„"],
                "improved_t": ["ê°œì„  í›„ T", "ê°œì„ T"],
                "reduction_rate": ["T ê°ì†Œìœ¨", "ê°ì†Œìœ¨", "ìœ„í—˜ ê°ì†Œìœ¨"]
            },
            "English": {
                "improvement": ["improvement_plan", "improvement_measures"],
                "improved_freq": ["improved_frequency", "new_frequency"],
                "improved_intensity": ["improved_intensity", "new_intensity"],
                "improved_t": ["improved_T", "new_T"],
                "reduction_rate": ["reduction_rate", "risk_reduction_rate"]
            },
            "Chinese": {
                "improvement": ["æ”¹è¿›æªæ–½", "æ”¹è¿›è®¡åˆ’"],
                "improved_freq": ["æ”¹è¿›åé¢‘ç‡", "æ–°é¢‘ç‡"],
                "improved_intensity": ["æ”¹è¿›åå¼ºåº¦", "æ–°å¼ºåº¦"],
                "improved_t": ["æ”¹è¿›åTå€¼", "æ–°Tå€¼"],
                "reduction_rate": ["Tå€¼é™ä½ç‡", "é™ä½ç‡"]
            }
        }
        
        mappings = key_mappings.get(language, key_mappings["Korean"])
        mapped_result = {}
        
        # í‚¤ ë§¤í•‘ ìˆ˜í–‰
        for result_key, possible_keys in mappings.items():
            for key in possible_keys:
                if key in result:
                    mapped_result[result_key] = result[key]
                    break
        
        # í•„ìˆ˜ ê°’ ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
        if "improved_freq" not in mapped_result:
            mapped_result["improved_freq"] = 1
        if "improved_intensity" not in mapped_result:
            mapped_result["improved_intensity"] = 1
        if "improved_t" not in mapped_result:
            mapped_result["improved_t"] = mapped_result["improved_freq"] * mapped_result["improved_intensity"]
        if "reduction_rate" not in mapped_result:
            mapped_result["reduction_rate"] = 50.0  # ê¸°ë³¸ê°’
            
        return mapped_result
        
    except Exception as e:
        st.error(f"ê°œì„ ëŒ€ì±… íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.write("ì›ë³¸ GPT ì‘ë‹µ:", gpt_output)
        return {
            "improvement": "ì•ˆì „ êµìœ¡ ì‹¤ì‹œ ë° ë³´í˜¸êµ¬ ì°©ìš© ì˜ë¬´í™”",
            "improved_freq": 1,
            "improved_intensity": 1, 
            "improved_t": 1,
            "reduction_rate": 50.0
        }

# -----------------------------------------------------------------------------
# ---------------------------  Overview íƒ­ ------------------------------------
# -----------------------------------------------------------------------------
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)
    
    col_overview, col_features = st.columns([3, 2])
    
    with col_overview:
        st.markdown(f"<div class='info-text'>{texts['overview_text']}</div>", unsafe_allow_html=True)
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ í‘œì‹œ
        col_metric1, col_metric2, col_metric3 = st.columns(3)
        
        with col_metric1:
            st.metric(texts["supported_languages"], texts["languages_count"], texts["languages_detail"])
        with col_metric2:
            st.metric(texts["assessment_phases"], texts["phases_count"], texts["phases_detail"])
        with col_metric3:
            st.metric(texts["risk_grades"], texts["grades_count"], texts["grades_detail"])
    
    with col_features:
        st.markdown(f"**{texts['features_title']}**")
        st.markdown(texts['phase1_features'])
        st.markdown(texts['phase2_features'])

# -----------------------------------------------------------------------------
# --------------  Risk Assessment í†µí•© íƒ­ --------------------------------------
# -----------------------------------------------------------------------------
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["tab_phase1"]} & {texts["tab_phase2"]}</div>', unsafe_allow_html=True)

    # â‘  API Key & Dataset Setup
    col_api, col_dataset = st.columns([2, 1])
    
    with col_api:
        api_key = st.text_input(texts['api_key_label'], type='password', key='api_key_all')
    
    with col_dataset:
        dataset_name = st.selectbox(texts['dataset_label'], [
            "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)",
            "Civil (í† ëª©)", 
            "Marine (í† ëª©)",
            "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)",
            "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)"
        ], key='dataset_all')

    # â‘¡ Data Loading Section
    if ss.retriever_pool_df is None or st.button(texts['load_data_btn'], type="primary"):
        if not api_key:
            st.warning(texts['api_key_warning'])
        else:
            with st.spinner(texts['data_loading']):
                try:
                    df = load_data(dataset_name)
                    
                    # ë°ì´í„° ë¶„í•  ë° ì²˜ë¦¬
                    if len(df) > 10:
                        train_df, _ = train_test_split(df, test_size=0.1, random_state=42)
                    else:
                        train_df = df.copy()
                    
                    pool_df = train_df.copy()
                    pool_df['content'] = pool_df.apply(lambda r: ' '.join(r.values.astype(str)), axis=1)
                    
                    # ì„ë² ë”© ì²˜ë¦¬
                    to_embed = pool_df['content'].tolist()
                    max_texts = min(len(to_embed), 30)  # ì²˜ë¦¬ ê°œìˆ˜ ì¦ê°€
                    
                    st.info(texts['demo_limit_info'].format(max_texts=max_texts))
                    
                    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„±
                    embeds = embed_texts_with_openai(to_embed[:max_texts], api_key=api_key)
                    
                    # FAISS ì¸ë±ìŠ¤ êµ¬ì„±
                    vecs = np.array(embeds, dtype='float32')
                    dim = vecs.shape[1]
                    index = faiss.IndexFlatL2(dim)
                    index.add(vecs)
                    
                    # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                    ss.index = index
                    ss.embeddings = vecs
                    ss.retriever_pool_df = pool_df.iloc[:max_texts]
                    
                    st.success(texts['data_load_success'].format(max_texts=max_texts))
                    
                    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                    with st.expander("ğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                        st.dataframe(df.head(), use_container_width=True)
                        
                except Exception as e:
                    st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    st.divider()

    # â‘¢ Risk Assessment Interface
    st.markdown("### ğŸ” ìœ„í—˜ì„± í‰ê°€ ìˆ˜í–‰")
    
    # ì‘ì—…í™œë™ ì…ë ¥
    activity = st.text_area(
        texts['activity_label'], 
        placeholder="ì˜ˆ: ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
        height=100,
        key='user_activity'
    )
    
    # ì¶”ê°€ ì˜µì…˜
    col_options1, col_options2 = st.columns(2)
    
    with col_options1:
        include_similar_cases = st.checkbox("ìœ ì‚¬ ì‚¬ë¡€ í¬í•¨", value=True)
        
    with col_options2:
        result_language = st.selectbox("ê²°ê³¼ ì–¸ì–´", ["Korean", "English", "Chinese"], 
                                     index=["Korean", "English", "Chinese"].index(ss.language))

    # ì‹¤í–‰ ë²„íŠ¼
    run_button = st.button("ğŸš€ ìœ„í—˜ì„± í‰ê°€ ì‹¤í–‰", type="primary", use_container_width=True)

    if run_button and activity:
        if not api_key:
            st.warning(texts['api_key_warning'])
        elif ss.index is None:
            st.warning(texts['load_first_warning'])
        else:
            with st.spinner("ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¤‘..."):
                try:
                    # === Phase 1: Risk Assessment ===
                    
                    # 1) ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
                    q_emb = embed_texts_with_openai([activity], api_key=api_key)[0]
                    D, I = ss.index.search(np.array([q_emb], dtype='float32'), 
                                         k=min(10, len(ss.retriever_pool_df)))
                    sim_docs = ss.retriever_pool_df.iloc[I[0]]

                    # 2) ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡
                    hazard_prompt = construct_prompt_phase1_hazard(sim_docs, activity, result_language)
                    hazard = generate_with_gpt(hazard_prompt, api_key, result_language)

                    # 3) ë¹ˆë„Â·ê°•ë„ ì˜ˆì¸¡
                    risk_prompt = construct_prompt_phase1_risk(sim_docs, activity, hazard, result_language)
                    risk_json = generate_with_gpt(risk_prompt, api_key, result_language)
                    
                    parse_result = parse_gpt_output_phase1(risk_json, result_language)
                    if not parse_result:
                        st.error(texts['parsing_error'])
                        st.expander("GPT ì›ë¬¸ ì‘ë‹µ").write(risk_json)
                        st.stop()
                    
                    freq, intensity, T = parse_result
                    grade = determine_grade(T)

                    # === Phase 2: Improvement Measures ===
                    
                    # 4) ê°œì„ ëŒ€ì±… ìƒì„±
                    improvement_prompt = construct_prompt_phase2(
                        sim_docs, activity, hazard, freq, intensity, T, result_language
                    )
                    improvement_response = generate_with_gpt(improvement_prompt, api_key, result_language)
                    
                    parsed_improvement = parse_gpt_output_phase2(improvement_response, result_language)
                    if not parsed_improvement:
                        st.error(texts['parsing_error_improvement'])
                        st.expander("GPT ì›ë¬¸ ì‘ë‹µ").write(improvement_response)
                        st.stop()

                    # ê°œì„ ëŒ€ì±… ê²°ê³¼ ì¶”ì¶œ
                    improvement_plan = parsed_improvement.get('improvement', '')
                    improved_freq = parsed_improvement.get('improved_freq', 1)
                    improved_intensity = parsed_improvement.get('improved_intensity', 1)
                    improved_T = parsed_improvement.get('improved_t', improved_freq * improved_intensity)
                    rrr = compute_rrr(T, improved_T)

                    # === Results Display ===
                    
                    # Phase 1 ê²°ê³¼ í‘œì‹œ
                    st.markdown("## ğŸ“‹ Phase 1: ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼")
                    
                    col_result1, col_result2 = st.columns([2, 1])
                    
                    with col_result1:
                        st.markdown(f"**ì‘ì—…í™œë™:** {activity}")
                        st.markdown(f"**ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸:** {hazard}")
                        
                        # ìœ„í—˜ë„ í…Œì´ë¸”
                        result_df = pd.DataFrame({
                            texts['result_table_columns'][0]: texts['result_table_rows'],
                            texts['result_table_columns'][1]: [freq, intensity, T, grade]
                        })
                        st.dataframe(result_df, use_container_width=True, hide_index=True)
                    
                    with col_result2:
                        # ìœ„í—˜ë“±ê¸‰ ì‹œê°í™”
                        grade_color = get_grade_color(grade)
                        st.markdown(f"""
                        <div style="text-align:center; padding:20px; background-color:{grade_color}; 
                                    color:white; border-radius:10px; margin:10px 0;">
                            <h2 style="margin:0;">ìœ„í—˜ë“±ê¸‰</h2>
                            <h1 style="margin:10px 0; font-size:3rem;">{grade}</h1>
                            <p style="margin:0;">Tê°’: {T}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    # ìœ ì‚¬ ì‚¬ë¡€ í‘œì‹œ (ì˜µì…˜)
                    if include_similar_cases:
                        st.markdown("### ğŸ” ìœ ì‚¬í•œ ì‚¬ë¡€")
                        
                        similar_records = []
                        for i, (_, doc) in enumerate(sim_docs.iterrows(), 1):
                            plan, imp_f, imp_i, imp_t = _extract_improvement_info(doc)
                            
                            # ì‚¬ë¡€ í‘œì‹œ
                            with st.expander(f"ì‚¬ë¡€ {i}: {doc['ì‘ì—…í™œë™ ë° ë‚´ìš©'][:50]}..."):
                                col_case1, col_case2 = st.columns(2)
                                
                                with col_case1:
                                    st.write(f"**ì‘ì—…í™œë™:** {doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']}")
                                    st.write(f"**ìœ í•´ìœ„í—˜ìš”ì¸:** {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}")
                                    st.write(f"**ìœ„í—˜ë„:** ë¹ˆë„ {doc['ë¹ˆë„']}, ê°•ë„ {doc['ê°•ë„']}, Tê°’ {doc['T']} (ë“±ê¸‰ {doc['ë“±ê¸‰']})")
                                
                                with col_case2:
                                    st.write(f"**ê°œì„ ëŒ€ì±…:** {plan if plan else 'ë¯¸ì œê³µ'}")
                                    if plan:
                                        st.write(f"**ê°œì„  í›„:** F={imp_f}, I={imp_i}, T={imp_t}")
                            
                            # ì—‘ì…€ìš© ë ˆì½”ë“œ
                            similar_records.append({
                                "No": i,
                                "ì‘ì—…í™œë™": doc['ì‘ì—…í™œë™ ë° ë‚´ìš©'],
                                "ìœ í•´ìœ„í—˜ìš”ì¸": doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥'],
                                "ë¹ˆë„": doc['ë¹ˆë„'],
                                "ê°•ë„": doc['ê°•ë„'],
                                "T": doc['T'],
                                "ìœ„í—˜ë“±ê¸‰": doc['ë“±ê¸‰'],
                                "ê°œì„ ëŒ€ì±…": plan,
                            })

                    # Phase 2 ê²°ê³¼ í‘œì‹œ
                    st.markdown("## ğŸ› ï¸ Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼")
                    
                    col_improvement1, col_improvement2 = st.columns([3, 2])
                    
                    with col_improvement1:
                        st.markdown(f"### {texts['improvement_plan_header']}")
                    
                        # 1) 2) ë“± ë²ˆí˜¸ ë’¤ì— ì¤„ë°”ê¿ˆ ë„£ê¸°
                        import re
                        # ì˜ˆ: "1) ì²«ë²ˆì§¸ 2) ë‘ë²ˆì§¸ 3) ì„¸ë²ˆì§¸" -> 
                        #    "1) ì²«ë²ˆì§¸\n2) ë‘ë²ˆì§¸\n3) ì„¸ë²ˆì§¸"
                        plan_md = re.sub(r'(\d\))\s*', r'\1  \n', improvement_plan.strip())
                    
                        st.markdown(plan_md)
                    
                    with col_improvement2:
                        st.markdown(f"### {texts['risk_improvement_header']}")
                        
                        # ê°œì„  ì „í›„ ë¹„êµ í…Œì´ë¸”
                        comparison_df = pd.DataFrame({
                            texts['comparison_columns'][0]: texts['result_table_rows'],
                            texts['comparison_columns'][1]: [freq, intensity, T, grade],
                            texts['comparison_columns'][2]: [improved_freq, improved_intensity, improved_T, determine_grade(improved_T)]
                        })
                        st.dataframe(comparison_df, use_container_width=True, hide_index=True)
                        
                        # ìœ„í—˜ ê°ì†Œìœ¨ í‘œì‹œ
                        st.metric(
                            label=texts['risk_reduction_label'],
                            value=f"{rrr:.1f}%",
                            delta=f"-{T-improved_T} Tê°’"
                        )

                    # ìœ„í—˜ë„ ë³€í™” ì‹œê°í™”
                    st.markdown("### ğŸ“Š ìœ„í—˜ë„ ë³€í™” ì‹œê°í™”")
                    
                    col_vis1, col_vis2 = st.columns(2)
                    
                    with col_vis1:
                        st.markdown("**ê°œì„  ì „ ìœ„í—˜ë„**")
                        progress_before = min(T / 25, 1.0)  # ìµœëŒ€ 25ë¡œ ì •ê·œí™”
                        st.progress(progress_before)
                        st.caption(f"Tê°’: {T} (ë“±ê¸‰: {grade})")
                    
                    with col_vis2:
                        st.markdown("**ê°œì„  í›„ ìœ„í—˜ë„**")
                        progress_after = min(improved_T / 25, 1.0)
                        st.progress(progress_after)
                        st.caption(f"Tê°’: {improved_T} (ë“±ê¸‰: {determine_grade(improved_T)})")

                    # ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
                    ss.last_assessment = {
                        'activity': activity,
                        'hazard': hazard,
                        'freq': freq,
                        'intensity': intensity,
                        'T': T,
                        'grade': grade,
                        'improvement_plan': improvement_plan,
                        'improved_freq': improved_freq,
                        'improved_intensity': improved_intensity,
                        'improved_T': improved_T,
                        'rrr': rrr,
                        'similar_cases': similar_records if include_similar_cases else []
                    }

                    # Excel ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
                    st.markdown("### ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
                    
                    def create_excel_download():
                        output = io.BytesIO()
                        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                            # â”€â”€â”€ Phase 1 ê²°ê³¼ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            phase1_df = pd.DataFrame({
                                "í•­ëª©": ["ì‘ì—…í™œë™", "ìœ í•´ìœ„í—˜ìš”ì¸", "ë¹ˆë„", "ê°•ë„", "Tê°’", "ìœ„í—˜ë“±ê¸‰"],
                                "ê°’": [activity, hazard, freq, intensity, T, grade]
                            })
                            phase1_df.to_excel(writer, sheet_name="Phase1_ê²°ê³¼", index=False)
                    
                            # â”€â”€â”€ Phase 2 ê²°ê³¼ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            phase2_df = pd.DataFrame({
                                "í•­ëª©": ["ê°œì„ ëŒ€ì±…", "ê°œì„  í›„ ë¹ˆë„", "ê°œì„  í›„ ê°•ë„", "ê°œì„  í›„ Tê°’", "ê°œì„  í›„ ë“±ê¸‰", "ìœ„í—˜ ê°ì†Œìœ¨"],
                                "ê°’": [improvement_plan,
                                        improved_freq,
                                        improved_intensity,
                                        improved_T,
                                        determine_grade(improved_T),
                                        f"{rrr:.2f}%"]
                            })
                            phase2_df.to_excel(writer, sheet_name="Phase2_ê²°ê³¼", index=False)
                    
                            # â”€â”€â”€ ë¹„êµ ë¶„ì„ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            comparison_detail_df = pd.DataFrame({
                                "í•­ëª©": ["ë¹ˆë„", "ê°•ë„", "Tê°’", "ìœ„í—˜ë“±ê¸‰"],
                                "ê°œì„  ì „": [freq, intensity, T, grade],
                                "ê°œì„  í›„": [improved_freq,
                                          improved_intensity,
                                          improved_T,
                                          determine_grade(improved_T)],
                                "ê°œì„ ìœ¨": [
                                    f"{(freq-improved_freq)/freq*100:.1f}%" if freq > 0 else "0%",
                                    f"{(intensity-improved_intensity)/intensity*100:.1f}%" if intensity > 0 else "0%",
                                    f"{rrr:.1f}%",
                                    f"{grade} â†’ {determine_grade(improved_T)}"
                                ]
                            })
                            comparison_detail_df.to_excel(writer, sheet_name="ë¹„êµë¶„ì„", index=False)
                    
                            # â”€â”€â”€ ìœ ì‚¬ì‚¬ë¡€ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                            # ì´ë¯¸ similar_records ì—ëŠ” 10ê±´ì´ ë“¤ì–´ì™€ ìˆë‹¤ê³  ê°€ì •
                            if similar_records:
                                sim_df = pd.DataFrame(similar_records)
                    
                                # ê°œì„  í›„ ë¹ˆë„Â·ê°•ë„ ê³„ì‚°
                                sim_df["ê°œì„  í›„ ë¹ˆë„"] = sim_df["ë¹ˆë„"].astype(int).apply(lambda x: max(1, x - 1))
                                sim_df["ê°œì„  í›„ ê°•ë„"] = sim_df["ê°•ë„"].astype(int).apply(lambda x: max(1, x - 1))
                    
                                # ë‚´ë³´ë‚¼ ì»¬ëŸ¼ êµ¬ì„±
                                export_df = pd.DataFrame({
                                    "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence":      sim_df["ì‘ì—…í™œë™"],
                                    "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazardous Factors": sim_df["ìœ í•´ìœ„í—˜ìš”ì¸"],
                                    "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood":     sim_df["ë¹ˆë„"],
                                    "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity":      sim_df["ê°•ë„"],
                                    "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures":    sim_df["ê°œì„ ëŒ€ì±…"],
                                    "ìœ„í—˜ì„± Risk (ê°œì„  í›„) â€“ ë¹ˆë„ likelihood": sim_df["ê°œì„  í›„ ë¹ˆë„"],
                                    "ìœ„í—˜ì„± Risk (ê°œì„  í›„) â€“ ê°•ë„ severity":  sim_df["ê°œì„  í›„ ê°•ë„"],
                                })
                    
                                export_df.to_excel(writer, sheet_name="ìœ ì‚¬ì‚¬ë¡€", index=False)
                    
                                # ì „ì²´ ì»¬ëŸ¼ ë¹¨ê°„ìƒ‰, ì¤„ë°”ê¿ˆ ì ìš©
                                workbook = writer.book
                                worksheet = writer.sheets["ìœ ì‚¬ì‚¬ë¡€"]
                                red_fmt = workbook.add_format({
                                    "font_color": "#FF0000",
                                    "text_wrap": True
                                })
                                for col_idx in range(len(export_df.columns)):
                                    # í­ 20, ì„œì‹ ì ìš©
                                    worksheet.set_column(col_idx, col_idx, 20, red_fmt)
                    
                        return output.getvalue()


# ------------------- í‘¸í„° ------------------------
st.markdown('<hr style="margin-top: 3rem;">', unsafe_allow_html=True)

footer_col1, footer_col2, footer_col3 = st.columns([1, 1, 1])

with footer_col1:
    if os.path.exists('cau.png'):
        st.image('cau.png', width=140)

with footer_col2:
    st.markdown("""
    <div style="text-align: center; padding: 20px;">
        <h4>ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°</h4>
        <p>AI ê¸°ë°˜ ìœ„í—˜ì„± í‰ê°€ ì‹œìŠ¤í…œ</p>
        <p style="font-size: 0.8rem; color: #666;">
            Â© 2025 Doosan Enerbility. All rights reserved.
        </p>
    </div>
    """, unsafe_allow_html=True)

with footer_col3:
    if os.path.exists('doosan.png'):
        st.image('doosan.png', width=160)
