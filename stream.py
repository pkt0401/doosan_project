import streamlit as st
import pandas as pd
import numpy as np
import faiss
import re
import os
import io
from sklearn.model_selection import train_test_split
from openai import OpenAI

# ----------------- ì‹œìŠ¤í…œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ -----------------
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_phase": "ìœ„í—˜ì„± í‰ê°€ & ê°œì„ ëŒ€ì±…",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": (
            "Doosan Enerbility AI Risk AssessmentëŠ” êµ­ë‚´ ë° í•´ì™¸ ê±´ì„¤í˜„ì¥ì˜ 'ìˆ˜ì‹œ ìœ„í—˜ì„± í‰ê°€' ë° "
            "'ë…¸ë™ë¶€ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€'ë¥¼ í•™ìŠµí•˜ì—¬ ê°œë°œëœ ìë™ ìœ„í—˜ì„±í‰ê°€ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. "
            "ìƒì„±ëœ ìœ„í—˜ì„± í‰ê°€ëŠ” ë°˜ë“œì‹œ ìˆ˜ì‹œ ìœ„í—˜ì„±í‰ê°€ ì‹¬ì˜íšŒë¥¼ í†µí•´ ê²€ì¦ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        ),
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase_features": (
            "#### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”\n"
            "- ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ\n"
            "- ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡ (ì˜ì–´ ë‚´ë¶€ ì‹¤í–‰)\n"
            "- ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ (ì˜ì–´ ë‚´ë¶€ ì²˜ë¦¬ â†’ ìµœì¢… ì¶œë ¥ ë²ˆì—­)\n"
            "- LLM ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì • (ì˜ì–´ ë‚´ë¶€ ì‹¤í–‰)\n"
            "- ìœ„í—˜ë“±ê¸‰(Aâ€“E) ìë™ ì‚°ì •\n\n"
            "#### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±\n"
            "- ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (ì˜ì–´ ë‚´ë¶€ ì‹¤í–‰)\n"
            "- ë‹¤êµ­ì–´(í•œêµ­ì–´/ì˜ì–´/ì¤‘êµ­ì–´) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›\n"
            "- ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„\n"
            "- ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"
        ),
        "supported_languages_label": "ì§€ì› ì–¸ì–´",
        "supported_languages_value": "3ê°œ",
        "supported_languages_detail": "í•œ/ì˜/ì¤‘",
        "assessment_phases_label": "í‰ê°€ ë‹¨ê³„",
        "assessment_phases_value": "2ë‹¨ê³„",
        "assessment_phases_detail": "Phase1+Phase2",
        "risk_grades_label": "ìœ„í—˜ë“±ê¸‰",
        "risk_grades_value": "5ë“±ê¸‰",
        "risk_grades_detail": "A~E",
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "include_similar_cases": "ìœ ì‚¬ ì‚¬ë¡€ í¬í•¨",
        "run_assessment": "ğŸš€ ìœ„í—˜ì„± í‰ê°€ ì‹¤í–‰",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "performing_assessment": "ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¤‘...",
        "phase1_results": "ğŸ“‹ Phase 1: ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼",
        "work_activity": "ì‘ì—…í™œë™",
        "predicted_hazard": "ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸",
        "risk_grade_display": "ìœ„í—˜ë“±ê¸‰",
        "t_value_display": "Tê°’",
        "risk_level_text": "ìœ„í—˜ë„ : ë¹ˆë„ {freq}, ê°•ë„ {intensity}, T {T} ({grade})",
        "similar_cases_section": "ğŸ” ìœ ì‚¬í•œ ì‚¬ë¡€",
        "case_number": "ì‚¬ë¡€",
        "phase2_results": "ğŸ› ï¸ Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ìœ„í—˜ë„ ê°œì„  ê²°ê³¼",
        "comparison_columns": ["í•­ëª©", "ê°œì„  ì „", "ê°œì„  í›„"],
        "risk_reduction_label": "ìœ„í—˜ ê°ì†Œìœ¨ (RRR)",
        "risk_visualization": "ğŸ“Š ìœ„í—˜ë„ ë³€í™” ì‹œê°í™”",
        "before_improvement": "ê°œì„  ì „",
        "after_improvement": "ê°œì„  í›„",
        "grade_label": "ë“±ê¸‰",
        "download_results": "ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
        "excel_export": "ğŸ“¥ ê²°ê³¼ Excel ë‹¤ìš´ë¡œë“œ",
        "col_activity_header": "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence",
        "col_hazard_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazarous Factors",
        "col_ehs_header": "EHS",
        "col_risk_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_risk_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity",
        "col_control_header": "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures",
        "col_incharge_header": "ê°œì„ ë‹´ë‹¹ì In Charge",
        "col_duedate_header": "ê°œì„ ì¼ì Correction Due Date",
        "col_after_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_after_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity"
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_phase": "Risk Assessment & Improvement",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": (
            "Doosan Enerbility AI Risk Assessment is an automated program trained on on-demand risk-assessment reports "
            "from domestic and overseas construction sites and major-accident cases compiled by Korea's Ministry of Employment "
            "and Labor. Please ensure that every generated assessment is reviewed and approved by the On-Demand Risk Assessment "
            "Committee before it is used."
        ),
        "features_title": "System Features and Components",
        "phase_features": (
            "#### Phase 1: Risk Assessment Automation\n"
            "- Learning risk assessment data per work activity\n"
            "- Automatic hazard prediction when work activities are entered (internal: English)\n"
            "- Similar case search & display (internal: English â†’ final: translation)\n"
            "- LLM-based risk level (frequency, intensity, T) measurement (internal: English)\n"
            "- Automatic risk grade (Aâ€“E) calculation\n\n"
            "#### Phase 2: Automatic Generation of Improvement Measures\n"
            "- Customized improvement measures generation (internal: English)\n"
            "- Multilingual (Korean/English/Chinese) improvement measure support\n"
            "- Automatic comparative analysis before/after improvement\n"
            "- Database of optimal improvement measures per process"
        ),
        "supported_languages_label": "Supported Languages",
        "supported_languages_value": "3",
        "supported_languages_detail": "KOR/ENG/CHN",
        "assessment_phases_label": "Assessment Phases",
        "assessment_phases_value": "2",
        "assessment_phases_detail": "Phase1+Phase2",
        "risk_grades_label": "Risk Grades",
        "risk_grades_value": "5",
        "risk_grades_detail": "Aâ€“E",
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset",
        "load_data_btn": "Load Data and Configure Index",
        "api_key_warning": "Please enter an OpenAI API key to continue.",
        "data_loading": "Loading data and configuring index...",
        "demo_limit_info": "For demo purposes, only embedding {max_texts} texts. In a real environment, process all data.",
        "data_load_success": "Data load and index configuration complete! (Total {max_texts} items processed)",
        "load_first_warning": "Please click [Load Data and Configure Index] first.",
        "activity_label": "Work Activity:",
        "include_similar_cases": "Include Similar Cases",
        "run_assessment": "ğŸš€ Run Risk Assessment",
        "activity_warning": "Please enter a work activity.",
        "performing_assessment": "Performing risk assessment...",
        "phase1_results": "ğŸ“‹ Phase 1: Risk Assessment Results",
        "work_activity": "Work Activity",
        "predicted_hazard": "Predicted Hazard",
        "risk_grade_display": "Risk Grade",
        "t_value_display": "T Value",
        "risk_level_text": "Risk Level : Frequency {freq}, Intensity {intensity}, T {T} (Grade {grade})",
        "similar_cases_section": "ğŸ” Similar Cases",
        "case_number": "Case",
        "phase2_results": "ğŸ› ï¸ Phase 2: Improvement Measures Results",
        "improvement_plan_header": "Improvement Plan",
        "risk_improvement_header": "Risk Improvement Results",
        "comparison_columns": ["Item", "Before Improvement", "After Improvement"],
        "risk_reduction_label": "Risk Reduction Rate (RRR)",
        "risk_visualization": "ğŸ“Š Risk Level Change Visualization",
        "before_improvement": "Before Improvement",
        "after_improvement": "After Improvement",
        "grade_label": "Grade",
        "download_results": "ğŸ’¾ Download Results",
        "excel_export": "ğŸ“¥ Download Excel Report",
        "col_activity_header": "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence",
        "col_hazard_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazarous Factors",
        "col_ehs_header": "EHS",
        "col_risk_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_risk_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity",
        "col_control_header": "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures",
        "col_incharge_header": "ê°œì„ ë‹´ë‹¹ì In Charge",
        "col_duedate_header": "ê°œì„ ì¼ì Correction Due Date",
        "col_after_likelihood_header": "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood",
        "col_after_severity_header": "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity"
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_phase": "é£é™©è¯„ä¼° & æ”¹è¿›æªæ–½",
        "overview_header": "åŸºäºLLMçš„é£é™©è¯„ä¼°ç³»ç»Ÿ",
        "overview_text": (
            "Doosan Enerbility AI é£é™©è¯„ä¼°ç³»ç»Ÿæ˜¯ä¸€æ¬¾è‡ªåŠ¨åŒ–é£é™©è¯„ä¼°ç¨‹åºï¼ŒåŸºäºå›½å†…å¤–æ–½å·¥ç°åœºçš„'ä¸´æ—¶é£é™©è¯„ä¼°'æ•°æ®åŠéŸ©å›½åŠ³å·¥éƒ¨ "
            "é‡å¤§äº‹æ•…æ¡ˆä¾‹è®­ç»ƒå¼€å‘è€Œæˆã€‚ç”Ÿæˆçš„é£é™©è¯„ä¼°ç»“æœå¿…é¡»ç»è¿‡ä¸´æ—¶é£é™©è¯„ä¼°å®¡è®®å§”å‘˜ä¼šçš„å®¡æ ¸åæ–¹å¯ä½¿ç”¨ã€‚"
        ),
        "features_title": "ç³»ç»Ÿç‰¹ç‚¹å’Œç»„ä»¶",
        "phase_features": (
            "#### ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°è‡ªåŠ¨åŒ–\n"
            "- æŒ‰å·¥ä½œæ´»åŠ¨å­¦ä¹ é£é™©è¯„ä¼°æ•°æ®\n"
            "- è¾“å…¥å·¥ä½œæ´»åŠ¨æ—¶è‡ªåŠ¨é¢„æµ‹å±å®³ (å†…éƒ¨ï¼šè‹±è¯­)\n"
            "- ç›¸ä¼¼æ¡ˆä¾‹æœç´¢ä¸æ˜¾ç¤º (å†…éƒ¨ï¼šè‹±è¯­ â†’ æœ€ç»ˆï¼šç¿»è¯‘)\n"
            "- åŸºäºLLMçš„é£é™©ç­‰çº§ï¼ˆé¢‘ç‡ã€å¼ºåº¦ã€Tï¼‰æµ‹é‡ (å†…éƒ¨ï¼šè‹±è¯­)\n"
            "- è‡ªåŠ¨è®¡ç®—é£é™©ç­‰çº§(Aâ€“E)\n\n"
            "#### ç¬¬2é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½\n"
            "- å®šåˆ¶åŒ–æ”¹è¿›æªæ–½è‡ªåŠ¨ç”Ÿæˆ (å†…éƒ¨ï¼šè‹±è¯­)\n"
            "- å¤šè¯­è¨€ (éŸ©/è‹±/ä¸­) æ”¹è¿›æªæ–½æ”¯æŒ\n"
            "- è‡ªåŠ¨æ¯”è¾ƒæ”¹è¿›å‰åé£é™©ç­‰çº§\n"
            "- æŒ‰å·¥åºç®¡ç†æœ€ä¼˜æ”¹è¿›æªæ–½æ•°æ®åº“"
        ),
        "supported_languages_label": "æ”¯æŒè¯­è¨€",
        "supported_languages_value": "3 ç§",
        "supported_languages_detail": "éŸ©/è‹±/ä¸­",
        "assessment_phases_label": "è¯„ä¼°é˜¶æ®µ",
        "assessment_phases_value": "2 é˜¶æ®µ",
        "assessment_phases_detail": "Phase1+Phase2",
        "risk_grades_label": "é£é™©ç­‰çº§",
        "risk_grades_value": "5 ç­‰çº§",
        "risk_grades_detail": "Aâ€“E",
        "api_key_label": "è¾“å…¥ OpenAI API å¯†é’¥ï¼š",
        "dataset_label": "é€‰æ‹©æ•°æ®é›†",
        "load_data_btn": "åŠ è½½æ•°æ®å¹¶é…ç½®ç´¢å¼•",
        "api_key_warning": "è¯·è¾“å…¥ OpenAI API å¯†é’¥ä»¥ç»§ç»­ã€‚",
        "data_loading": "æ­£åœ¨åŠ è½½æ•°æ®å¹¶é…ç½®ç´¢å¼•...",
        "demo_limit_info": "æ¼”ç¤ºç”¨é€”ä»…åµŒå…¥ {max_texts} ä¸ªæ–‡æœ¬ã€‚å®é™…ç¯å¢ƒåº”å¤„ç†æ‰€æœ‰æ•°æ®ã€‚",
        "data_load_success": "æ•°æ®åŠ è½½ä¸ç´¢å¼•é…ç½®å®Œæˆï¼(å…±å¤„ç† {max_texts} é¡¹ç›®)",
        "load_first_warning": "è¯·å…ˆç‚¹å‡» [åŠ è½½æ•°æ®å¹¶é…ç½®ç´¢å¼•]ã€‚",
        "activity_label": "å·¥ä½œæ´»åŠ¨ï¼š",
        "include_similar_cases": "åŒ…æ‹¬ç›¸ä¼¼æ¡ˆä¾‹",
        "run_assessment": "ğŸš€ è¿è¡Œé£é™©è¯„ä¼°",
        "activity_warning": "è¯·è¾“å…¥å·¥ä½œæ´»åŠ¨ã€‚",
        "performing_assessment": "æ­£åœ¨è¿›è¡Œé£é™©è¯„ä¼°...",
        "phase1_results": "ğŸ“‹ ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°ç»“æœ",
        "work_activity": "å·¥ä½œæ´»åŠ¨",
        "predicted_hazard": "é¢„æµ‹å±å®³",
        "risk_grade_display": "é£é™©ç­‰çº§",
        "t_value_display": "T å€¼",
        "risk_level_text": "é£é™©ç­‰çº§ : é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, T {T} (ç­‰çº§ {grade})",
        "similar_cases_section": "ğŸ” ç›¸ä¼¼æ¡ˆä¾‹",
        "case_number": "æ¡ˆä¾‹",
        "phase2_results": "ğŸ› ï¸ ç¬¬2é˜¶æ®µï¼šæ”¹è¿›æªæ–½ç»“æœ",
        "improvement_plan_header": "æ”¹è¿›æªæ–½",
        "risk_improvement_header": "é£é™©æ”¹è¿›ç»“æœ",
        "comparison_columns": ["é¡¹ç›®", "æ”¹è¿›å‰", "æ”¹è¿›å"],
        "risk_reduction_label": "é£é™©é™ä½ç‡ (RRR)",
        "risk_visualization": "ğŸ“Š é£é™©ç­‰çº§å˜åŒ–å¯è§†åŒ–",
        "before_improvement": "æ”¹è¿›å‰",
        "after_improvement": "æ”¹è¿›å",
        "grade_label": "ç­‰çº§",
        "download_results": "ğŸ’¾ ä¸‹è½½ç»“æœ",
        "excel_export": "ğŸ“¥ ä¸‹è½½ Excel æŠ¥è¡¨",
        "col_activity_header": "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence",
        "col_hazard_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazarous Factors",
        "col_ehs_header": "EHS",
        "col_risk_likelihood_header": "ìœ„í—˜æ€§ Risk â€“ ë¹ˆë„ likelihood",
        "col_risk_severity_header": "ìœ„í—˜æ€§ Risk â€“ ê°•ë„ severity",
        "col_control_header": "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures",
        "col_incharge_header": "ê°œì„ ë‹´ë‹¹ì In Charge",
        "col_duedate_header": "ê°œì„ ì¼ì Correction Due Date",
        "col_after_likelihood_header": "ìœ„í—˜æ€§ Risk â€“ ë¹ˆë„ likelihood",
        "col_after_severity_header": "ìœ„í—˜æ€§ Risk â€“ ê°•ë„ severity"
    }
}

# ----------------- í˜ì´ì§€ ìŠ¤íƒ€ì¼ -----------------
st.set_page_config(page_title="AI Risk Assessment", page_icon="ğŸ› ï¸", layout="wide")
st.markdown(
    """
    <style>
    .main-header{font-size:2.5rem;color:#1E88E5;text-align:center;margin-bottom:1rem}
    .sub-header{font-size:1.8rem;color:#0D47A1;margin-top:2rem;margin-bottom:1rem}
    .similar-case{background-color:#f1f8e9;border-radius:8px;padding:12px;margin-bottom:8px;border-left:4px solid #689f38}
    </style>
    """,
    unsafe_allow_html=True
)

# ----------------- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -----------------
ss = st.session_state
for key, default in {
    "language": "Korean",            # í™”ë©´ í‘œì‹œ ë° ê²°ê³¼ ì–¸ì–´
    "index": None,                   # FAISS ì¸ë±ìŠ¤
    "embeddings": None,              # ì„ë² ë”© í–‰ë ¬
    "retriever_pool_df": None,       # ìœ ì‚¬ ì‚¬ë¡€ í›„ë³´ ë°ì´í„°í”„ë ˆì„ (í•œêµ­ì–´ ì›ë³¸)
    "last_assessment": None          # ë§ˆì§€ë§‰ í‰ê°€ ê²°ê³¼ ì €ì¥ìš©
}.items():
    if key not in ss:
        ss[key] = default

# ----------------- ì–¸ì–´ ì„ íƒ -----------------
col0, colLang = st.columns([6, 1])
with colLang:
    lang = st.selectbox(
        "ì–¸ì–´ ì„ íƒ",
        list(system_texts.keys()),
        index=list(system_texts.keys()).index(ss.language),
        label_visibility="hidden"
    )
    ss.language = lang
texts = system_texts[ss.language]
result_language = ss.language   # UI ì–¸ì–´ì™€ ë™ì¼í•˜ê²Œ ê²°ê³¼ ì–¸ì–´ë¡œ ì‚¬ìš©

# ----------------- í—¤ë” -----------------
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# ----------------- íƒ­ êµ¬ì„± -----------------
tabs = st.tabs([texts["tab_overview"], texts["tab_phase"]])

# -----------------------------------------------------------------------------  
# ---------------- Utility Functions ------------------------------------------
# -----------------------------------------------------------------------------  

def determine_grade(value: int) -> str:
    if 16 <= value <= 25:
        return 'A'
    if 10 <= value <= 15:
        return 'B'
    if 5 <= value <= 9:
        return 'C'
    if 3 <= value <= 4:
        return 'D'
    if 1 <= value <= 2:
        return 'E'
    return 'Unknown' if ss.language != 'Korean' else 'ì•Œ ìˆ˜ ì—†ìŒ'

def get_grade_color(grade: str) -> str:
    colors = {
        'A': '#ff1744',
        'B': '#ff9800',
        'C': '#ffc107',
        'D': '#4caf50',
        'E': '#2196f3',
    }
    return colors.get(grade, '#808080')

def compute_rrr(original_t: int, improved_t: int) -> float:
    if original_t == 0:
        return 0.0
    return ((original_t - improved_t) / original_t) * 100

# â”€â”€â”€ ì‹ ê·œ ì¶”ê°€: ê°œì„ ëŒ€ì±… ë²ˆí˜¸ ê¸°ì¤€ ì¤„ë°”ê¿ˆ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def format_improvement_plan_for_display(plan_text: str) -> str:
    """
    ê°œì„ ëŒ€ì±…(plan_text)ì´ '1) ... 2) ... 3) ...' ì‹ ë²ˆí˜¸ ë§¤ê²¨ì ¸ ìˆì„ ë•Œ,
    ê° ë²ˆí˜¸ ì•ì— ì¤„ë°”ê¿ˆì„ ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not plan_text:
        return ""

    # (1) ê¸°ì¡´ ê°œí–‰ë¬¸ì ëª¨ë‘ ì œê±°í•˜ê³  í•œ ì¤„ë¡œ ë§Œë“  ë’¤
    single_line = plan_text.replace("\r\n", " ").replace("\r", " ").replace("\n", " ")

    # (2) ìˆ«ì) íŒ¨í„´(ì˜ˆ: '2)') ì•ì— '\n' ì‚½ì…
    formatted = re.sub(r"(?<!\n)(\d\))", r"\n\1", single_line)

    # (3) ì²« ê¸€ìê°€ ë¶ˆí•„ìš”í•œ \n ì¸ ê²½ìš° ì œê±°
    if formatted.startswith("\n"):
        formatted = formatted[1:]

    return formatted
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_data(show_spinner=False)
def load_data(selected_dataset_name: str) -> pd.DataFrame:
    try:
        dataset_mapping = {
            "ê±´ì¶•": "ê±´ì¶•", "Architecture": "ê±´ì¶•",
            "í† ëª©": "í† ëª©", "Civil": "í† ëª©",
            "í”ŒëœíŠ¸": "í”ŒëœíŠ¸", "Plant": "í”ŒëœíŠ¸"
        }
        actual_filename = dataset_mapping.get(selected_dataset_name, selected_dataset_name)

        if os.path.exists(f"{actual_filename}.xlsx"):
            try:
                df = pd.read_excel(f"{actual_filename}.xlsx", engine='openpyxl')
            except Exception:
                df = pd.read_excel(f"{actual_filename}.xlsx", engine='xlrd')
        elif os.path.exists(f"{actual_filename}.xls"):
            df = pd.read_excel(f"{actual_filename}.xls", engine='xlrd')
        else:
            st.info(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {actual_filename}.xlsx ë˜ëŠ” {actual_filename}.xls")
            st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return create_sample_data()

        if "ì‚­ì œ Del" in df.columns:
            df.drop(["ì‚­ì œ Del"], axis=1, inplace=True)
        df = df.dropna(how='all')

        column_mapping = {
            "ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect": "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ\nCorrective Action": "ê°œì„ ëŒ€ì±…"
        }
        df.rename(columns=column_mapping, inplace=True)

        for col in ["ë¹ˆë„", "ê°•ë„"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        if 'ë¹ˆë„' not in df.columns:
            df['ë¹ˆë„'] = 3
        if 'ê°•ë„' not in df.columns:
            df['ê°•ë„'] = 3

        df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
        df["ë“±ê¸‰"] = df["T"].apply(determine_grade)

        if "ê°œì„ ëŒ€ì±…" not in df.columns:
            alt_cols = [c for c in df.columns if "ê°œì„ " in c or "Corrective" in c]
            if alt_cols:
                df.rename(columns={alt_cols[0]: "ê°œì„ ëŒ€ì±…"}, inplace=True)
            else:
                df["ê°œì„ ëŒ€ì±…"] = "ì•ˆì „ êµìœ¡ ì‹¤ì‹œ ë° ë³´í˜¸êµ¬ ì°©ìš©"

        required_cols = [
            "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ë¹ˆë„",
            "ê°•ë„",
            "T",
            "ë“±ê¸‰",
            "ê°œì„ ëŒ€ì±…"
        ]
        final_cols = [col for col in required_cols if col in df.columns]
        df = df[final_cols]

        df = df.fillna({
            "ì‘ì—…í™œë™ ë° ë‚´ìš©": "ì¼ë°˜ ì‘ì—…",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": "ì¼ë°˜ì  ìœ„í—˜",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": "ë¶€ìƒ",
            "ê°œì„ ëŒ€ì±…": "ì•ˆì „ ì¡°ì¹˜ ìˆ˜í–‰"
        })
        return df

    except Exception as e:
        st.warning(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return create_sample_data()

def create_sample_data() -> pd.DataFrame:
    data = {
        "ì‘ì—…í™œë™ ë° ë‚´ìš©": [
            "ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "ì½˜í¬ë¦¬íŠ¸/CMU ë¸”ë¡ ì„¤ì¹˜ ì‘ì—…",
            "êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…",
            "ê³ ì†Œ ì‘ì—…ëŒ€ë¥¼ ì´ìš©í•œ ì™¸ë²½ ì‘ì—…",
            "ìš©ì ‘ ì‘ì—…"
        ],
        "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": [
            "ë‹¤ì¤‘ ì¸ì–‘ìœ¼ë¡œ ì¸í•œ ì ì¬ë¬¼ ë‚™í•˜",
            "ë¶ˆì¶©ë¶„í•œ ì‘ì—… ë°œíŒìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°",
            "ì•ˆì „ëŒ€ ë¯¸ì°©ìš©ìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "ìš©ì ‘ í„ ë° í™”ì¬ ìœ„í—˜"
        ],
        "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": [
            "íƒ€ë°•ìƒ", "ê³¨ì ˆ", "ë§¤ëª°", "ì¶”ë½ì‚¬", "í™”ìƒ"
        ],
        "ë¹ˆë„": [3, 3, 2, 4, 2],
        "ê°•ë„": [5, 4, 5, 5, 3],
        "ê°œì„ ëŒ€ì±…": [
            "1) ë‹¤ìˆ˜ì˜ ì² ê³¨ì¬ë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬\n"
            "2) ì¹˜ìˆ˜, ì¤‘ëŸ‰, í˜•ìƒì´ ë‹¤ë¥¸ ì¬ë£Œë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬",
            "1) ë¹„ê³„ëŒ€ ëˆ„ë½ëœ ëª©íŒ ì„¤ì¹˜\n"
            "2) ì•ˆì „ëŒ€ ë¶€ì°©ì„¤ë¹„ ì„¤ì¹˜ ë° ì‚¬ìš©\n"
            "3) ë¹„ê³„ ë³€ê²½ ì‹œ íƒ€ê³µì¢… ì™¸ ì‘ì—…ì ì‘ì—… ê¸ˆì§€",
            "1) ì ì ˆí•œ ì‚¬ë©´ ê¸°ìš¸ê¸° ìœ ì§€\n"
            "2) êµ´ì°©ë©´ ë³´ê°•\n"
            "3) ì •ê¸°ì  ì§€ë°˜ ìƒíƒœ ì ê²€",
            "1) ì•ˆì „ëŒ€ ì°©ìš© ì˜ë¬´í™”\n"
            "2) ì‘ì—… ì „ ì•ˆì „êµìœ¡ ì‹¤ì‹œ\n"
            "3) ì¶”ë½ë°©ì§€ë§ ì„¤ì¹˜",
            "1) ì ì ˆí•œ í™˜ê¸°ì‹œì„¤ ì„¤ì¹˜\n"
            "2) í™”ì¬ ì˜ˆë°© ì¡°ì¹˜\n"
            "3) ë³´í˜¸êµ¬ ì°©ìš©"
        ]
    }
    df = pd.DataFrame(data)
    df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
    df["ë“±ê¸‰"] = df["T"].apply(determine_grade)
    return df

def embed_texts_with_openai(texts: list[str], api_key: str, model: str="text-embedding-3-large") -> list[list[float]]:
    if not api_key:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []
    client = OpenAI(api_key=api_key)
    embeddings = []
    batch_size = 10
    for i in range(0, len(texts), batch_size):
        batch = texts[i : i + batch_size]
        processed = [str(t).replace("\n", " ").strip() for t in batch]
        try:
            resp = client.embeddings.create(
                model=model,
                input=processed
            )
            for item in resp.data:
                embeddings.append(item.embedding)
        except Exception as e:
            st.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ë°°ì¹˜ {i}): {e}")
            for _ in processed:
                embeddings.append([0.0] * 1536)
    return embeddings

def generate_with_gpt(prompt: str, api_key: str, model: str="gpt-4o", max_retries: int=3) -> str:
    if not api_key:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return ""
    client = OpenAI(api_key=api_key)
    sys_prompt = "You are a construction site risk assessment expert. Provide accurate and practical responses in English."
    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system",  "content": sys_prompt},
                    {"role": "user",    "content": prompt}
                ],
                temperature=0.1,
                max_tokens=700,
                top_p=0.9
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            if attempt == max_retries - 1:
                st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜ ({attempt+1}/{max_retries}): {e}")
                return ""
            else:
                st.warning(f"GPT ì¬ì‹œë„ ì¤‘... ({attempt+1}/{max_retries})")
                continue

def translate_similar_cases(sim_docs: pd.DataFrame, api_key: str) -> pd.DataFrame:
    sim_docs_en = sim_docs.copy().reset_index(drop=True)
    sim_docs_en["activity_en"] = sim_docs_en["ì‘ì—…í™œë™ ë° ë‚´ìš©"]
    sim_docs_en["hazard_en"] = sim_docs_en["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"]
    sim_docs_en["plan_en"] = sim_docs_en["ê°œì„ ëŒ€ì±…"]
    for idx, row in sim_docs_en.iterrows():
        act_ko = row["ì‘ì—…í™œë™ ë° ë‚´ìš©"]
        prompt_act = (
            "Translate the following construction work activity into English. "
            "Only provide the translation:\n\n" + act_ko
        )
        act_en = generate_with_gpt(prompt_act, api_key)
        if act_en:
            sim_docs_en.at[idx, "activity_en"] = act_en

        haz_ko = row["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"]
        prompt_haz = (
            "Translate the following construction hazard into English. "
            "Only provide the translation:\n\n" + haz_ko
        )
        haz_en = generate_with_gpt(prompt_haz, api_key)
        if haz_en:
            sim_docs_en.at[idx, "hazard_en"] = haz_en

        plan_ko = row["ê°œì„ ëŒ€ì±…"]
        prompt_plan = (
            "Translate the following safety improvement measures into English. "
            "Keep the numbered format. Only provide the translation:\n\n" + plan_ko
        )
        plan_en = generate_with_gpt(prompt_plan, api_key)
        if plan_en:
            sim_docs_en.at[idx, "plan_en"] = plan_en

    return sim_docs_en

def translate_output(content: str, target_language: str, api_key: str, max_retries: int=2) -> str:
    if target_language == "English" or not api_key:
        return content
    for attempt in range(max_retries):
        try:
            prompt = f"Translate the following into {target_language}. Only provide the translation:\n\n{content}"
            translated = generate_with_gpt(prompt, api_key)
            if translated:
                return translated
        except Exception:
            if attempt == max_retries - 1:
                return content
            else:
                continue
    return content

def construct_prompt_phase1_hazard(sim_docs_en: pd.DataFrame, activity_en: str) -> str:
    intro = "Below are examples of work activities and associated hazards at construction sites:\n\n"
    example_fmt = "Example {i}:\n- Work Activity: {act}\n- Hazard: {haz}\n\n"
    query_fmt = (
        "Based on the above examples, predict the main hazards for the following work activity:\n\n"
        f"Work Activity: {activity_en}\n\nPredicted Hazard: "
    )
    prompt = intro
    for i, (_, row) in enumerate(sim_docs_en.head(10).iterrows(), start=1):
        act = row["activity_en"]
        haz = row["hazard_en"]
        if pd.notna(act) and pd.notna(haz):
            prompt += example_fmt.format(i=i, act=act, haz=haz)
    prompt += query_fmt
    return prompt

def construct_prompt_phase1_risk(sim_docs_en: pd.DataFrame, activity_en: str, hazard_en: str) -> str:
    intro = (
        "Construction site risk assessment criteria:\n"
        "- Frequency(1-5): 1=Very Rare, 2=Rare, 3=Occasional, 4=Frequent, 5=Very Frequent\n"
        "- Intensity(1-5): 1=Minor Injury, 2=Light Injury, 3=Moderate Injury, 4=Serious Injury, 5=Fatality\n"
        "- T-value = Frequency Ã— Intensity\n\n"
        "Reference Cases:\n\n"
    )
    example_fmt = "Case {i}:\nInput: {inp}\nAssessment: Frequency={freq}, Intensity={intensity}, T-value={t}\n\n"
    json_format = '{"frequency": number, "intensity": number, "T": number}'
    query_fmt = (
        "Based on the above criteria and cases, assess the following:\n\n"
        f"Work Activity: {activity_en}\n"
        f"Hazard: {hazard_en}\n\n"
        f"Respond exactly in this JSON format:\n{json_format}"
    )
    prompt = intro
    count = 0
    for _, row in sim_docs_en.head(10).iterrows():
        try:
            inp = f"{row['activity_en']} - {row['hazard_en']}"
            freq = int(row["ë¹ˆë„"])
            intensity = int(row["ê°•ë„"])
            t_val = freq * intensity
            count += 1
            prompt += example_fmt.format(i=count, inp=inp, freq=freq, intensity=intensity, t=t_val)
            if count >= 3:
                break
        except Exception:
            continue
    prompt += query_fmt
    return prompt

def parse_gpt_output_phase1(gpt_output: str) -> tuple[int, int, int]:
    pattern = r'\{"frequency":\s*([1-5]),\s*"intensity":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    match = re.search(pattern, gpt_output)
    if match:
        freq = int(match.group(1))
        intensity = int(match.group(2))
        t_val = int(match.group(3))
        return freq, intensity, t_val
    nums = re.findall(r'\b([1-5])\b', gpt_output)
    if len(nums) >= 2:
        freq = int(nums[0])
        intensity = int(nums[1])
        return freq, intensity, freq * intensity
    return None

def construct_prompt_phase2(sim_docs_en: pd.DataFrame, activity_en: str, hazard_en: str,
                             freq: int, intensity: int, t_val: int, api_key: str) -> str:
    example_section = ""
    count = 0
    for _, row in sim_docs_en.head(10).iterrows():
        try:
            plan_en = row["plan_en"]
            orig_freq = int(row["ë¹ˆë„"])
            orig_intensity = int(row["ê°•ë„"])
            orig_t = orig_freq * orig_intensity
            new_freq = max(1, orig_freq - 1)
            new_intensity = max(1, orig_intensity - 1)
            new_t = new_freq * new_intensity

            count += 1
            example_section += (
                f"Example {count}:\n"
                f"Input Work Activity: {row['activity_en']}\n"
                f"Input Hazard: {row['hazard_en']}\n"
                f"Original Frequency: {orig_freq}\n"
                f"Original Intensity: {orig_intensity}\n"
                f"Original T-value: {orig_t}\n"
                "Output (Improvement Plan and Risk Reduction) in JSON:\n"
                "{\n"
                f'  "improvement_plan": "{plan_en}",\n'
                f'  "improved_frequency": {new_freq},\n'
                f'  "improved_intensity": {new_intensity},\n'
                f'  "improved_T": {new_t},\n'
                f'  "reduction_rate": {compute_rrr(orig_t, new_t):.2f}\n'
                "}\n\n"
            )
            if count >= 2:
                break
        except Exception:
            continue

    if count == 0:
        example_section = (
            "Example 1:\n"
            "Input Work Activity: Excavation and backfilling\n"
            "Input Hazard: Collapse of excavation wall\n"
            "Original Frequency: 3\n"
            "Original Intensity: 4\n"
            "Original T-value: 12\n"
            "Output (Improvement Plan and Risk Reduction) in JSON:\n"
            "{\n"
            '  "improvement_plan": "1) Maintain proper slope according to soil classification\\n'
            '2) Reinforce excavation walls\\n3) Conduct regular ground condition inspections",\n'
            '  "improved_frequency": 1,\n'
            '  "improved_intensity": 2,\n'
            '  "improved_T": 2,\n'
            '  "reduction_rate": 83.33\n'
            "}\n\n"
        )

    prompt = (
        example_section +
        "Now here is a new input:\n"
        f"Work Activity: {activity_en}\n"
        f"Hazard: {hazard_en}\n"
        f"Original Frequency: {freq}\n"
        f"Original Intensity: {intensity}\n"
        f"Original T-value: {t_val}\n\n"
        "Please provide practical and specific improvement measures in the following JSON format:\n"
        "{\n"
        '  "improvement_plan": "numbered list of specific measures",\n'
        '  "improved_frequency": (integer 1-5),\n'
        '  "improved_intensity": (integer 1-5),\n'
        '  "improved_T": (improved_frequency Ã— improved_intensity),\n'
        '  "reduction_rate": (percentage)\n'
        "}\n\n"
        "Improvement measures should include at least 3 field-applicable methods."
    )
    return prompt

def parse_gpt_output_phase2(gpt_output: str) -> dict:
    try:
        json_match = re.search(r'\{.*\}', gpt_output, re.DOTALL)
        if not json_match:
            raise ValueError("JSON match not found")
        import json
        json_str = json_match.group(0)
        parsed = json.loads(json_str)
        if "improvement_plan" in parsed:
            return {
                "improvement_plan": parsed.get("improvement_plan", ""),
                "improved_freq": parsed.get("improved_frequency", 1),
                "improved_intensity": parsed.get("improved_intensity", 1),
                "improved_T": parsed.get("improved_T", parsed.get("improved_frequency", 1) * parsed.get("improved_intensity", 1)),
                "reduction_rate": parsed.get("reduction_rate", 0.0)
            }
        chinese_keys = {
            "improvement": ["æ”¹è¿›æªæ–½", "æ”¹è¿›è®¡åˆ’"],
            "improved_freq": ["æ”¹è¿›åé¢‘ç‡", "æ–°é¢‘ç‡"],
            "improved_intensity": ["æ”¹è¿›åå¼ºåº¦", "æ–°å¼ºåº¦"],
            "improved_T": ["æ”¹è¿›åTå€¼", "æ–°Tå€¼"],
            "reduction_rate": ["Tå€¼é™ä½ç‡", "é™ä½ç‡"]
        }
        if any(key in parsed for key in chinese_keys["improvement"]):
            im_plan = ""
            for k in chinese_keys["improvement"]:
                if k in parsed:
                    im_plan = parsed[k]
                    break
            imp_freq = 1
            for k in chinese_keys["improved_freq"]:
                if k in parsed:
                    imp_freq = int(parsed[k])
                    break
            imp_int = 1
            for k in chinese_keys["improved_intensity"]:
                if k in parsed:
                    imp_int = int(parsed[k])
                    break
            imp_t = imp_freq * imp_int
            for k in chinese_keys["improved_T"]:
                if k in parsed:
                    imp_t = int(parsed[k])
                    break
            r_rate = 0.0
            for k in chinese_keys["reduction_rate"]:
                if k in parsed:
                    try:
                        r_rate = float(parsed[k])
                    except:
                        r_rate = 0.0
                    break
            return {
                "improvement_plan": im_plan,
                "improved_freq": imp_freq,
                "improved_intensity": imp_int,
                "improved_T": imp_t,
                "reduction_rate": r_rate
            }
        raise ValueError("No recognized keys found in JSON")
    except Exception:
        plan = ""
        m_plan_en = re.search(r'"improvement_plan"\s*:\s*"(?P<plan>.*?)"', gpt_output, re.DOTALL)
        if m_plan_en:
            raw = m_plan_en.group("plan")
            plan = raw.replace('\n', '\\n').strip()
        else:
            m_plan_cn = re.search(r'"(æ”¹è¿›æªæ–½|æ”¹è¿›è®¡åˆ’)"\s*:\s*"(?P<plan>.*?)"', gpt_output, re.DOTALL)
            if m_plan_cn:
                raw = m_plan_cn.group("plan")
                plan = raw.replace('\n', '\\n').strip()
            else:
                plan = (
                    "1) Educate workers and mandate PPE usage\n"
                    "2) Install pedestrian walkways\n"
                    "3) Provide high-visibility vests"
                )
        def extract_int(keys: list[str]) -> int:
            for key in keys:
                m = re.search(rf'"{key}"\s*:\s*(\d+)', gpt_output)
                if m:
                    return int(m.group(1))
            return 1
        def extract_float(keys: list[str]) -> float:
            for key in keys:
                m = re.search(rf'"{key}"\s*:\s*([\d\.]+)', gpt_output)
                if m:
                    try:
                        return float(m.group(1))
                    except:
                        return 0.0
            return 0.0
        english_keys = {
            "improved_freq": ["improved_frequency"],
            "improved_intensity": ["improved_intensity"],
            "improved_T": ["improved_T"],
            "reduction_rate": ["reduction_rate"]
        }
        chinese_keys = {
            "improved_freq": ["æ”¹è¿›åé¢‘ç‡", "æ–°é¢‘ç‡"],
            "improved_intensity": ["æ”¹è¿›åå¼ºåº¦", "æ–°å¼ºåº¦"],
            "improved_T": ["æ”¹è¿›åTå€¼", "æ–°Tå€¼"],
            "reduction_rate": ["Tå€¼é™ä½ç‡", "é™ä½ç‡"]
        }
        imp_freq = extract_int(english_keys["improved_freq"] + chinese_keys["improved_freq"])
        imp_int = extract_int(english_keys["improved_intensity"] + chinese_keys["improved_intensity"])
        imp_t   = extract_int(english_keys["improved_T"] + chinese_keys["improved_T"])
        r_rate  = extract_float(english_keys["reduction_rate"] + chinese_keys["reduction_rate"])
        return {
            "improvement_plan": plan,
            "improved_freq": imp_freq,
            "improved_intensity": imp_int,
            "improved_T": imp_t,
            "reduction_rate": r_rate
        }

def create_excel_download(result_dict: dict, similar_records: list[dict]) -> bytes:
    output = io.BytesIO()
    try:
        # í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        from datetime import datetime
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            workbook = writer.book

def create_excel_download(result_dict: dict, similar_records: list[dict]) -> bytes:
    output = io.BytesIO()
    try:
        # í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
        from datetime import datetime
        current_date = datetime.now().strftime("%Y-%m-%d")
        
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            workbook = writer.book

            # â”€â”€â”€ ë©”ì¸ ê²°ê³¼ë¥¼ ë°ì´í„°ì…‹ í˜•ì‹ì— ë§ì¶° ë‹¨ì¼ ì‹œíŠ¸ë¡œ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ì›ë³¸ ë°ì´í„°ì…‹ì˜ ì •í™•í•œ ì»¬ëŸ¼ êµ¬ì¡°ì™€ ë™ì¼í•˜ê²Œ ìƒì„±
            main_result_df = pd.DataFrame({
                "ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents": [result_dict["activity"]],
                "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk": [result_dict["hazard"]],
                "EHS": ["S"],  # EHSëŠ” "S"ë¡œ ì„¤ì •
                "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect": [""],  # ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •
                "ë¹ˆë„\nRisk Rate": [result_dict["freq"]],  # ì²« ë²ˆì§¸ ìœ„í—˜ì„± Risk Rate í•˜ìœ„
                "ê°•ë„\nSeverity": [result_dict["intensity"]],
                "T": [result_dict["T"]],
                "ë“±ê¸‰\nRate": [result_dict["grade"]],
                "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ\nCorrective Action": [result_dict["improvement_plan"]],
                "ê°œì„ ë‹´ë‹¹ì\nResponsibility": [""],  # ê°œì„ ë‹´ë‹¹ìëŠ” ë¹ˆì¹¸
                "ê°œì„ ì¼ì\nAction Date": [current_date],  # ê°œì„ ì¼ìëŠ” í˜„ì¬ ë‚ ì§œ
                "ë¹ˆë„\nLikelihood": [result_dict["improved_freq"]],  # ë‘ ë²ˆì§¸ ìœ„í—˜ì„± Risk Rate í•˜ìœ„
                "ê°•ë„\nSeverity ": [result_dict["improved_intensity"]],  # ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
                "T ": [result_dict["improved_T"]],  # ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
                "ë“±ê¸‰\nRate ": [determine_grade(result_dict["improved_T"])]  # ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
            })
            
            main_result_df.to_excel(writer, sheet_name="ìœ„í—˜ì„±í‰ê°€ê²°ê³¼", index=False)
            ws_main = writer.sheets["ìœ„í—˜ì„±í‰ê°€ê²°ê³¼"]
            
            # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì •
            for col_idx, column in enumerate(main_result_df.columns):
                max_length = max(
                    len(str(column)),
                    main_result_df[column].astype(str).str.len().max() if not main_result_df[column].empty else 0
                )
                ws_main.set_column(col_idx, col_idx, min(max_length + 2, 50))

            # â”€â”€â”€ ìœ ì‚¬ì‚¬ë¡€ ì‹œíŠ¸ (ê¸°ì¡´ ë°ì´í„°ì…‹ í˜•ì‹) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if similar_records:
                sim_df = pd.DataFrame(similar_records)
                sim_df["ê°œì„  í›„ ë¹ˆë„"] = sim_df["ë¹ˆë„"].astype(int).apply(lambda x: max(1, x - 1))
                sim_df["ê°œì„  í›„ ê°•ë„"] = sim_df["ê°•ë„"].astype(int).apply(lambda x: max(1, x - 1))
                sim_df["ê°œì„  í›„ T"] = sim_df["ê°œì„  í›„ ë¹ˆë„"] * sim_df["ê°œì„  í›„ ê°•ë„"]
                sim_df["ê°œì„  í›„ ë“±ê¸‰"] = sim_df["ê°œì„  í›„ T"].apply(determine_grade)

                export_df = pd.DataFrame({
                    "ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents": sim_df["ì‘ì—…í™œë™"],
                    "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk": sim_df["ìœ í•´ìœ„í—˜ìš”ì¸"],
                    "EHS": ["S" for _ in range(len(sim_df))],  # EHSëŠ” "S"ë¡œ ì„¤ì •
                    "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect": ["" for _ in range(len(sim_df))],
                    "ë¹ˆë„\nRisk Rate": sim_df["ë¹ˆë„"],
                    "ê°•ë„\nSeverity": sim_df["ê°•ë„"],
                    "T": sim_df["T"],
                    "ë“±ê¸‰\nRate": sim_df["ë“±ê¸‰"],
                    "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ\nCorrective Action": sim_df["ê°œì„ ëŒ€ì±…"],
                    "ê°œì„ ë‹´ë‹¹ì\nResponsibility": ["" for _ in range(len(sim_df))],  # ê°œì„ ë‹´ë‹¹ìëŠ” ë¹ˆì¹¸
                    "ê°œì„ ì¼ì\nAction Date": [current_date for _ in range(len(sim_df))],  # ê°œì„ ì¼ìëŠ” í˜„ì¬ ë‚ ì§œ
                    "ë¹ˆë„\nLikelihood": sim_df["ê°œì„  í›„ ë¹ˆë„"],
                    "ê°•ë„\nSeverity ": sim_df["ê°œì„  í›„ ê°•ë„"],  # ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
                    "T ": sim_df["ê°œì„  í›„ T"],
                    "ë“±ê¸‰\nRate ": sim_df["ê°œì„  í›„ ë“±ê¸‰"]
                })
                export_df.to_excel(writer, sheet_name="ìœ ì‚¬ì‚¬ë¡€", index=False)
                ws_sim = writer.sheets["ìœ ì‚¬ì‚¬ë¡€"]
                
                # ìœ ì‚¬ì‚¬ë¡€ ì‹œíŠ¸ ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •
                for col_idx, column in enumerate(export_df.columns):
                    max_length = max(
                        len(str(column)),
                        export_df[column].astype(str).str.len().max() if not export_df[column].empty else 0
                    )
                    ws_sim.set_column(col_idx, col_idx, min(max_length + 2, 50))

        return output.getvalue()
        
    except ImportError:
        st.warning("Excel ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. CSVë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
        # CSV ë°±ì—…ë„ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ìƒì„±
        from datetime import datetime
        csv_buffer = io.StringIO()
        
        # ë©”ì¸ ê²°ê³¼ë¥¼ CSVë¡œ ìƒì„±
        main_result_df = pd.DataFrame({
            "ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents": [result_dict["activity"]],
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk": [result_dict["hazard"]],
            "EHS": ["S"],  # EHSëŠ” "S"ë¡œ ì„¤ì •
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect": [""],
            "ë¹ˆë„\nRisk Rate": [result_dict["freq"]],
            "ê°•ë„\nSeverity": [result_dict["intensity"]],
            "T": [result_dict["T"]],
            "ë“±ê¸‰\nRate": [result_dict["grade"]],
            "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ\nCorrective Action": [result_dict["improvement_plan"]],
            "ê°œì„ ë‹´ë‹¹ì\nResponsibility": [""],  # ê°œì„ ë‹´ë‹¹ìëŠ” ë¹ˆì¹¸
            "ê°œì„ ì¼ì\nAction Date": [datetime.now().strftime("%Y-%m-%d")],  # ê°œì„ ì¼ìëŠ” í˜„ì¬ ë‚ ì§œ
            "ë¹ˆë„\nLikelihood": [result_dict["improved_freq"]],
            "ê°•ë„\nSeverity ": [result_dict["improved_intensity"]],  # ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
            "T ": [result_dict["improved_T"]],
            "ë“±ê¸‰\nRate ": [determine_grade(result_dict["improved_T"])]
        })
        
        main_result_df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
        return csv_buffer.getvalue().encode("utf-8-sig")
# -----------------------------------------------------------------------------  
# ---------------------- Overview íƒ­ ------------------------------------------  
# -----------------------------------------------------------------------------  
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)
    col_overview, col_features = st.columns([3, 2])
    with col_overview:
        st.markdown(texts["overview_text"])
        st.markdown(f"**{texts['features_title']}**")
        st.markdown(texts["phase_features"])
    with col_features:
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                label=texts["supported_languages_label"],
                value=texts["supported_languages_value"],
                delta=texts["supported_languages_detail"]
            )
        with col2:
            st.metric(
                label=texts["assessment_phases_label"],
                value=texts["assessment_phases_value"],
                delta=texts["assessment_phases_detail"]
            )
        with col3:
            st.metric(
                label=texts["risk_grades_label"],
                value=texts["risk_grades_value"],
                delta=texts["risk_grades_detail"]
            )

# -----------------------------------------------------------------------------  
# -------------------- Risk Assessment & Improvement íƒ­ ------------------------
# -----------------------------------------------------------------------------  
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["tab_phase"]}</div>', unsafe_allow_html=True)

    col_api, col_dataset = st.columns([2, 1])
    with col_api:
        api_key = st.text_input(texts["api_key_label"], type="password", key="api_key_all")
    with col_dataset:
        dataset_options = ["ê±´ì¶•", "í† ëª©", "í”ŒëœíŠ¸"]
        dataset_name = st.selectbox(
            texts["dataset_label"],
            dataset_options,
            key="dataset_all"
        )

    if ss.retriever_pool_df is None or st.button(texts["load_data_btn"], type="primary"):
        if not api_key:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["data_loading"]):
                try:
                    df = load_data(dataset_name)
                    if len(df) > 10:
                        train_df, _ = train_test_split(df, test_size=0.1, random_state=42)
                    else:
                        train_df = df.copy()

                    pool_df = train_df.copy()
                    pool_df["content"] = pool_df.apply(lambda r: " ".join(r.values.astype(str)), axis=1)

                    to_embed = pool_df["content"].tolist()
                    max_texts = min(len(to_embed), 30)
                    st.info(texts["demo_limit_info"].format(max_texts=max_texts))

                    embeds = embed_texts_with_openai(to_embed[:max_texts], api_key=api_key)
                    vecs = np.array(embeds, dtype="float32")
                    dim = vecs.shape[1]
                    index = faiss.IndexFlatL2(dim)
                    index.add(vecs)

                    ss.index = index
                    ss.embeddings = vecs
                    ss.retriever_pool_df = pool_df.iloc[:max_texts]
                    st.success(texts["data_load_success"].format(max_texts=max_texts))
                    with st.expander("ğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                        st.dataframe(df.head(), use_container_width=True)
                except Exception as e:
                    st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")

    st.divider()
    st.markdown(f"### {texts['performing_assessment'].split('.')[0]}")

    activity = st.text_area(
        texts["activity_label"],
        placeholder={
            "Korean": "ì˜ˆ: ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "English": "e.g.: Unloading steel structural materials using forklift at temporary site storage",
            "Chinese": "ä¾‹: åœ¨ä¸´æ—¶ç°åœºä»“åº“ä½¿ç”¨å‰è½¦å¸è½½é’¢ç»“æ„ææ–™"
        }.get(ss.language),
        height=100,
        key="user_activity"
    )
    include_similar_cases = st.checkbox(texts["include_similar_cases"], value=True)
    run_button = st.button(texts["run_assessment"], type="primary", use_container_width=True)

    if run_button:
        if not activity:
            st.warning(texts["activity_warning"])
        elif not api_key:
            st.warning(texts["api_key_warning"])
        elif ss.index is None:
            st.warning(texts["load_first_warning"])
        else:
            with st.spinner(texts["performing_assessment"]):
                try:
                    # ===== Phase 1 =====
                    prompt_to_english = (
                        "Translate the following construction work activity into English. "
                        "Only provide the translation:\n\n" + activity
                    )
                    activity_en = generate_with_gpt(prompt_to_english, api_key)
                    if not activity_en:
                        activity_en = activity

                    sim_docs = ss.retriever_pool_df.copy().reset_index(drop=True)
                    sim_docs_en = translate_similar_cases(sim_docs, api_key)

                    q_emb_list = embed_texts_with_openai([activity_en], api_key=api_key)
                    if not q_emb_list:
                        st.error("ìœ„í—˜ì„± í‰ê°€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()
                    q_emb = q_emb_list[0]

                    D, I = ss.index.search(np.array([q_emb], dtype="float32"), k=min(10, len(sim_docs_en)))
                    if I is None or len(I[0]) == 0:
                        st.error("ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    sim_docs_subset = sim_docs_en.iloc[I[0]].reset_index(drop=True)

                    hazard_prompt_en = construct_prompt_phase1_hazard(sim_docs_subset, activity_en)
                    hazard_en = generate_with_gpt(hazard_prompt_en, api_key)
                    if not hazard_en:
                        st.error("ìœ„í—˜ì„± í‰ê°€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    risk_prompt_en = construct_prompt_phase1_risk(sim_docs_subset, activity_en, hazard_en)
                    risk_json_en = generate_with_gpt(risk_prompt_en, api_key)
                    parse_result = parse_gpt_output_phase1(risk_json_en)
                    if not parse_result:
                        st.error("ìœ„í—˜ì„± í‰ê°€ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.expander("GPT ì›ë³¸ ì‘ë‹µ").write(risk_json_en)
                        st.stop()

                    freq, intensity, T_val = parse_result
                    grade = determine_grade(T_val)

                    # ===== Phase 2 =====
                    prompt_phase2_en = construct_prompt_phase2(
                        sim_docs_subset, activity_en, hazard_en, freq, intensity, T_val, api_key
                    )
                    improvement_json_en = generate_with_gpt(prompt_phase2_en, api_key)
                    parsed_improvement = parse_gpt_output_phase2(improvement_json_en)
                    improvement_plan_en = parsed_improvement.get("improvement_plan", "")
                    improved_freq = parsed_improvement.get("improved_freq", 1)
                    improved_intensity = parsed_improvement.get("improved_intensity", 1)
                    improved_T = parsed_improvement.get("improved_T", improved_freq * improved_intensity)
                    rrr_value = compute_rrr(T_val, improved_T)

                    # ===== ìµœì¢… ì¶œë ¥ìš© ë²ˆì—­ =====
                    hazard_user = translate_output(hazard_en, result_language, api_key)
                    improvement_user = translate_output(improvement_plan_en, result_language, api_key)

                    # ===== ìœ ì‚¬ ì‚¬ë¡€ ì¶œë ¥ìš© ë°ì´í„° ìƒì„± =====
                    display_sim_records = []
                    for idx, row in sim_docs_subset.iterrows():
                        eng_act = row["activity_en"]
                        eng_haz = row["hazard_en"]
                        eng_plan = row["plan_en"]
                        orig_freq = row["ë¹ˆë„"]
                        orig_intensity = row["ê°•ë„"]
                        orig_T = row["T"]
                        orig_grade = row["ë“±ê¸‰"]

                        if result_language == "English":
                            act_disp = eng_act
                            haz_disp = eng_haz
                            plan_disp = eng_plan
                        elif result_language == "Chinese":
                            act_disp = translate_output(eng_act, "Chinese", api_key)
                            haz_disp = translate_output(eng_haz, "Chinese", api_key)
                            plan_disp = translate_output(eng_plan, "Chinese", api_key)
                        else:  # Korean
                            act_disp = translate_output(eng_act, "Korean", api_key)
                            haz_disp = translate_output(eng_haz, "Korean", api_key)
                            plan_disp = translate_output(eng_plan, "Korean", api_key)

                        display_sim_records.append({
                            "ì‘ì—…í™œë™": act_disp,
                            "ìœ í•´ìœ„í—˜ìš”ì¸": haz_disp,
                            "ë¹ˆë„": orig_freq,
                            "ê°•ë„": orig_intensity,
                            "T": orig_T,
                            "ë“±ê¸‰": orig_grade,
                            "ê°œì„ ëŒ€ì±…": plan_disp
                        })

                    # ===== í™”ë©´ ì¶œë ¥ =====
                    st.markdown(f"## {texts['phase1_results']}")
                    col_r1, col_r2 = st.columns([2, 1])
                    with col_r1:
                        activity_user = (
                            translate_output(activity_en, result_language, api_key)
                            if result_language != "English"
                            else activity_en
                        )
                        st.markdown(f"**{texts['work_activity']}:** {activity_user}")
                        st.markdown(f"**{texts['predicted_hazard']}:** {hazard_user}")

                        df_display = pd.DataFrame({
                            texts["comparison_columns"][0]: ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
                            "ê°’": [str(freq), str(intensity), str(T_val), grade]
                        })
                        st.dataframe(df_display.astype(str), use_container_width=True, hide_index=True)
                    with col_r2:
                        grade_color = get_grade_color(grade)
                        st.markdown(f"""
                        <div style="text-align:center; padding:20px; background-color:{grade_color};
                                    color:white; border-radius:10px; margin:10px 0;">
                            <h2 style="margin:0;">{texts['risk_grade_display']}</h2>
                            <h1 style="margin:10px 0; font-size:3rem;">{grade}</h1>
                            <p style="margin:0;">{texts['t_value_display']}: {T_val}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    # ===== ìœ ì‚¬ì‚¬ë¡€ í‘œì‹œ =====
                    if include_similar_cases and display_sim_records:
                        st.markdown(f"### {texts['similar_cases_section']}")
                        for idx, rec in enumerate(display_sim_records):
                            with st.expander(f"{texts['case_number']} {idx+1}: {rec['ì‘ì—…í™œë™'][:30]}â€¦"):
                                c1, c2 = st.columns(2)
                                with c1:
                                    st.write(f"**{texts['work_activity']} :** {rec['ì‘ì—…í™œë™']}")
                                    st.write(f"**{texts['predicted_hazard']} :** {rec['ìœ í•´ìœ„í—˜ìš”ì¸']}")
                                    st.write(
                                        f"**{texts['risk_level_text'].format(freq=rec['ë¹ˆë„'], intensity=rec['ê°•ë„'], T=rec['T'], grade=rec['ë“±ê¸‰'])}**"
                                    )
                                with c2:
                                    st.write(f"**{texts['improvement_plan_header']} :**")
                                    raw_plan = rec["ê°œì„ ëŒ€ì±…"]
                                    formatted_plan = format_improvement_plan_for_display(raw_plan)
                                    st.markdown(formatted_plan)

                    # ===== Phase 2 ì¶œë ¥ =====
                    st.markdown(f"## {texts['phase2_results']}")
                    c_imp, c_riskimp = st.columns([3, 2])
                    with c_imp:
                        st.markdown(f"### {texts['improvement_plan_header']}")
                        if improvement_user:
                            formatted_plan2 = format_improvement_plan_for_display(improvement_user)
                            st.markdown(formatted_plan2)
                        else:
                            st.write("ê°œì„ ëŒ€ì±… ìƒì„± ì‹¤íŒ¨")

                    with c_riskimp:
                        st.markdown(f"### {texts['risk_improvement_header']}")
                        comp_df_user = pd.DataFrame({
                            texts["comparison_columns"][0]: ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
                            texts["comparison_columns"][1]: [str(freq), str(intensity), str(T_val), grade],
                            texts["comparison_columns"][2]: [str(improved_freq), str(improved_intensity), str(improved_T), determine_grade(improved_T)]
                        })
                        st.dataframe(comp_df_user.astype(str), use_container_width=True, hide_index=True)
                        st.metric(
                            label=texts["risk_reduction_label"],
                            value=f"{rrr_value:.1f}%",
                            delta=f"-{T_val - improved_T} T"
                        )

                    # ===== ìœ„í—˜ë„ ì‹œê°í™” =====
                    st.markdown(f"### {texts['risk_visualization']}")
                    vis1, vis2 = st.columns(2)
                    with vis1:
                        st.markdown(f"**{texts['before_improvement']}**")
                        col_before = get_grade_color(grade)
                        st.markdown(f"""
                        <div style="background-color:{col_before}; color:white; padding:15px; 
                                    border-radius:10px; text-align:center; margin:10px 0;">
                            <h3 style="margin:0;">{texts['grade_label']} {grade}</h3>
                            <p style="margin:5px 0; font-size:1.2em;">{texts['t_value_display']}: {T_val}</p>
                        </div>
                        """, unsafe_allow_html=True)
                    with vis2:
                        st.markdown(f"**{texts['after_improvement']}**")
                        grade_after = determine_grade(improved_T)
                        col_after = get_grade_color(grade_after)
                        st.markdown(f"""
                        <div style="background-color:{col_after}; color:white; padding:15px; 
                                    border-radius:10px; text-align:center; margin:10px 0;">
                            <h3 style="margin:0;">{texts['grade_label']} {grade_after}</h3>
                            <p style="margin:5px 0; font-size:1.2em;">{texts['t_value_display']}: {improved_T}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    # ===== ì„¸ì…˜ ì €ì¥ =====
                    ss.last_assessment = {
                        "activity": activity_user,
                        "hazard": hazard_user,
                        "freq": freq,
                        "intensity": intensity,
                        "T": T_val,
                        "grade": grade,
                        "improvement_plan": improvement_user,
                        "improved_freq": improved_freq,
                        "improved_intensity": improved_intensity,
                        "improved_T": improved_T,
                        "rrr": rrr_value,
                        "similar_cases": display_sim_records
                    }

                    # ===== ì—‘ì…€ ë‹¤ìš´ë¡œë“œ =====
                    st.markdown(f"### {texts['download_results']}")
                    excel_bytes = create_excel_download(ss.last_assessment, display_sim_records)
                    st.download_button(
                        label=texts["excel_export"],
                        data=excel_bytes,
                        file_name="risk_assessment_report.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                except Exception as e:
                    st.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
                    st.stop()

    # í‘¸í„°
    st.markdown('<hr style="margin-top: 3rem;">', unsafe_allow_html=True)
    footer_c1, footer_c2, footer_c3 = st.columns([1, 1, 1])
    with footer_c1:
        if os.path.exists("cau.png"):
            st.image("cau.png", width=140)
    with footer_c2:
        st.markdown(
            """
            <div style="text-align: center; padding: 20px;">
                <h4>ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°</h4>
                <p>AI ê¸°ë°˜ ìœ„í—˜ì„± í‰ê°€ ì‹œìŠ¤í…œ</p>
                <p style="font-size: 0.8rem; color: #666;">
                    Â© 2025 Doosan Enerbility. All rights reserved.
                </p>
            </div>
            """,
            unsafe_allow_html=True
        )
    with footer_c3:
        if os.path.exists("doosan.png"):
            st.image("doosan.png", width=160)
