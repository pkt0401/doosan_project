import streamlit as st
import pandas as pd
import numpy as np
import faiss
import re
import os
import io
from sklearn.model_selection import train_test_split
from openai import OpenAI

# ----------------- ì‹œìŠ¤í…œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ -----------------
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_assessment": "ìœ„í—˜ì„± í‰ê°€ & ê°œì„ ëŒ€ì±…",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹° AI Risk AssessmentëŠ” êµ­ë‚´ ë° í•´ì™¸ ê±´ì„¤í˜„ì¥ 'ìˆ˜ì‹œìœ„í—˜ì„±í‰ê°€' ë° 'ë…¸ë™ë¶€ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€'ë¥¼ í•™ìŠµí•˜ì—¬ ê°œë°œëœ ìë™ ìœ„í—˜ì„±í‰ê°€ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. ìƒì„±ëœ ìœ„í—˜ì„±í‰ê°€ëŠ” ë°˜ë“œì‹œ ìˆ˜ì‹œ ìœ„í—˜ì„±í‰ê°€ ì‹¬ì˜íšŒë¥¼ í†µí•´ ê²€ì¦ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase1_features": """
        #### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”
        - ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ
        - ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡
        - ìœ ì‚¬ ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ
        - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì •
        - Excel ê¸°ë°˜ ê³µì •ë³„ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° ìë™ ë¶„ì„
        - ìœ„í—˜ë“±ê¸‰(A-E) ìë™ ì‚°ì •
        """,
        "phase2_features": """
        #### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ìœ„í—˜ìš”ì†Œë³„ ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ë‹¤êµ­ì–´(í•œ/ì˜/ì¤‘) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›
        - ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„
        - ìœ„í—˜ ê°ì†Œìœ¨(RRR) ì •ëŸ‰ì  ì‚°ì¶œ
        - ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        """,
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "include_similar": "ìœ ì‚¬ ì‚¬ë¡€ í¬í•¨",
        "result_language_label": "ê²°ê³¼ ì–¸ì–´ ì„ íƒ:",
        "run_button": "ğŸš€ ìœ„í—˜ì„± í‰ê°€ ì‹¤í–‰",
        "phase1_header": "## ğŸ“‹ Phase 1: ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼",
        "phase2_header": "## ğŸ› ï¸ Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "### ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "### ê°œì„  ì „í›„ ìœ„í—˜ì„± ë¹„êµ",
        "risk_table_pre": "Pre-Improvement",
        "risk_table_post": "Post-Improvement",
        "excel_export": "ğŸ“¥ ê²°ê³¼ Excel ë‹¤ìš´ë¡œë“œ",
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "footer_text": "Â© 2025 Doosan Enerbility. All rights reserved."
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_assessment": "Risk Assessment & Improvement",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": "Doosan Enerbility AI Risk Assessment is an automated program trained on both on-demand risk-assessment reports from domestic and overseas construction sites and major-accident cases compiled by Korea's Ministry of Employment and Labor. Please ensure that every generated assessment is reviewed and approved by the On-Demand Risk Assessment Committee before it is used.",
        "features_title": "System Features and Components",
        "phase1_features": """
        #### Phase 1: Risk Assessment Automation
        - Learning risk assessment data according to work activities by process
        - Automatic hazard prediction when work activities are entered
        - Similar case search and display
        - Risk level (frequency, severity, T) measurement based on large language models (LLM)
        - Automatic analysis of Excel-based process-specific risk assessment data
        - Automatic risk grade (A-E) calculation
        """,
        "phase2_features": """
        #### Phase 2: Automatic Generation of Improvement Measures
        - Automatic generation of customized improvement measures for risk factors
        - Multilingual (Korean/English/Chinese) improvement measure generation support
        - Automatic comparative analysis of risk level (T) before and after improvement
        - Quantitative calculation of Risk Reduction Rate (RRR)
        - Building a database of optimal improvement measures by work type/process
        """,
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset",
        "load_data_btn": "Load Data and Configure Index",
        "api_key_warning": "Please enter an OpenAI API key to continue.",
        "data_loading": "Loading data and configuring index...",
        "demo_limit_info": "For demo purposes, only embedding {max_texts} texts. In a real environment, all data should be processed.",
        "data_load_success": "Data load and index configuration complete! (Total {max_texts} items processed)",
        "load_first_warning": "Please click the [Load Data and Configure Index] button first.",
        "activity_label": "Work Activity:",
        "activity_warning": "Please enter a work activity.",
        "include_similar": "Include Similar Cases",
        "result_language_label": "Select Result Language:",
        "run_button": "ğŸš€ Run Risk Assessment",
        "phase1_header": "## ğŸ“‹ Phase 1: Risk Assessment Results",
        "phase2_header": "## ğŸ› ï¸ Phase 2: Improvement Measures Results",
        "improvement_plan_header": "### Control Measures",
        "risk_improvement_header": "### Pre/Post-Improvement Risk Comparison",
        "risk_table_pre": "Pre-Improvement",
        "risk_table_post": "Post-Improvement",
        "excel_export": "ğŸ“¥ Download Results as Excel",
        "parsing_error": "Unable to parse risk assessment results.",
        "footer_text": "Â© 2025 Doosan Enerbility. All rights reserved."
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_assessment": "é£é™©è¯„ä¼° & æ”¹è¿›",
        "overview_header": "åŸºäºLLMçš„é£é™©è¯„ä¼°ç³»ç»Ÿ",
        "overview_text": "Doosan Enerbility AI é£é™©è¯„ä¼°ç³»ç»Ÿæ˜¯ä¸€æ¬¾è‡ªåŠ¨åŒ–é£é™©è¯„ä¼°ç¨‹åºï¼ŒåŸºäºå›½å†…å¤–æ–½å·¥ç°åœºçš„'ä¸´æ—¶é£é™©è¯„ä¼°'æ•°æ®ä»¥åŠéŸ©å›½é›‡ä½£åŠ³åŠ¨éƒ¨çš„é‡å¤§äº‹æ•…æ¡ˆä¾‹è¿›è¡Œè®­ç»ƒå¼€å‘è€Œæˆã€‚ç”Ÿæˆçš„é£é™©è¯„ä¼°ç»“æœå¿…é¡»ç»è¿‡ä¸´æ—¶é£é™©è¯„ä¼°å®¡è®®å§”å‘˜ä¼šçš„å®¡æ ¸åæ–¹å¯ä½¿ç”¨ã€‚",
        "features_title": "ç³»ç»Ÿç‰¹ç‚¹å’Œç»„ä»¶",
        "phase1_features": """
        #### ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°è‡ªåŠ¨åŒ–
        - æŒ‰å·¥åºå­¦ä¹ ä¸å·¥ä½œæ´»åŠ¨ç›¸å…³çš„é£é™©è¯„ä¼°æ•°æ®
        - è¾“å…¥å·¥ä½œæ´»åŠ¨æ—¶è‡ªåŠ¨é¢„æµ‹å±å®³å› ç´ 
        - ç›¸ä¼¼æ¡ˆä¾‹æœç´¢å’Œæ˜¾ç¤º
        - åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„é£é™©ç­‰çº§ï¼ˆé¢‘ç‡ã€ä¸¥é‡åº¦ã€Tå€¼ï¼‰æµ‹é‡
        - è‡ªåŠ¨åˆ†æåŸºäºExcelçš„ç‰¹å®šå·¥åºé£é™©è¯„ä¼°æ•°æ®
        - è‡ªåŠ¨è®¡ç®—é£é™©ç­‰çº§(A-E)
        """,
        "phase2_features": """
        #### ç¬¬2é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½
        - ä¸ºé£é™©å› ç´ è‡ªåŠ¨ç”Ÿæˆå®šåˆ¶çš„æ”¹è¿›æªæ–½
        - å¤šè¯­è¨€ï¼ˆéŸ©è¯­/è‹±è¯­/ä¸­æ–‡ï¼‰æ”¹è¿›æªæ–½ç”Ÿæˆæ”¯æŒ
        - æ”¹è¿›å‰åé£é™©ç­‰çº§ï¼ˆTå€¼ï¼‰çš„è‡ªåŠ¨æ¯”è¾ƒåˆ†æ
        - é£é™©é™ä½ç‡(RRR)çš„å®šé‡è®¡ç®—
        - å»ºç«‹æŒ‰å·¥ä½œç±»å‹/å·¥åºçš„æœ€ä½³æ”¹è¿›æªæ–½æ•°æ®åº“
        """,
        "api_key_label": "è¾“å…¥OpenAI APIå¯†é’¥ï¼š",
        "dataset_label": "é€‰æ‹©æ•°æ®é›†",
        "load_data_btn": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "api_key_warning": "è¯·è¾“å…¥OpenAI APIå¯†é’¥ä»¥ç»§ç»­ã€‚",
        "data_loading": "æ­£åœ¨åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•...",
        "demo_limit_info": "å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œä»…åµŒå…¥{max_texts}ä¸ªæ–‡æœ¬ã€‚åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œåº”å¤„ç†æ‰€æœ‰æ•°æ®ã€‚",
        "data_load_success": "æ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®å®Œæˆï¼ï¼ˆå…±å¤„ç†{max_texts}ä¸ªé¡¹ç›®ï¼‰",
        "load_first_warning": "è¯·å…ˆç‚¹å‡»[åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•]æŒ‰é’®ã€‚",
        "activity_label": "å·¥ä½œæ´»åŠ¨ï¼š",
        "activity_warning": "è¯·è¾“å…¥å·¥ä½œæ´»åŠ¨ã€‚",
        "include_similar": "åŒ…å«ç›¸ä¼¼æ¡ˆä¾‹",
        "result_language_label": "é€‰æ‹©ç»“æœè¯­è¨€ï¼š",
        "run_button": "ğŸš€ è¿è¡Œé£é™©è¯„ä¼°",
        "phase1_header": "## ğŸ“‹ ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°ç»“æœ",
        "phase2_header": "## ğŸ› ï¸ ç¬¬2é˜¶æ®µï¼šæ”¹è¿›æªæ–½ç»“æœ",
        "improvement_plan_header": "### æ§åˆ¶æªæ–½",
        "risk_improvement_header": "### æ”¹è¿›å‰åé£é™©æ¯”è¾ƒ",
        "risk_table_pre": "æ”¹è¿›å‰",
        "risk_table_post": "æ”¹è¿›å",
        "excel_export": "ğŸ“¥ ä¸‹è½½ç»“æœä¸ºExcel",
        "parsing_error": "æ— æ³•è§£æé£é™©è¯„ä¼°ç»“æœã€‚",
        "footer_text": "Â© 2025 Doosan Enerbility. ç‰ˆæƒæ‰€æœ‰ã€‚"
    }
}

# ----------------- í˜ì´ì§€ ìŠ¤íƒ€ì¼ -----------------
st.set_page_config(page_title="AI Risk Assessment", page_icon="ğŸ› ï¸", layout="wide")
st.markdown(
    """
    <style>
    .main-header{font-size:2.5rem;color:#1E88E5;text-align:center;margin-bottom:1rem}
    .sub-header{font-size:1.8rem;color:#0D47A1;margin-top:2rem;margin-bottom:1rem}
    .metric-container{background-color:#f0f2f6;border-radius:10px;padding:20px;box-shadow:2px 2px 5px rgba(0,0,0,0.1)}
    .result-box{background-color:#f8f9fa;border-radius:10px;padding:15px;margin-top:10px;margin-bottom:10px;border-left:5px solid #4CAF50}
    .phase-badge{background-color:#4CAF50;color:white;padding:5px 10px;border-radius:15px;font-size:0.8rem;margin-right:10px}
    .similar-case{background-color:#f1f8e9;border-radius:8px;padding:12px;margin-bottom:8px;border-left:4px solid #689f38}
    .language-selector{position:absolute;top:10px;right:10px;z-index:1000}
    .calculation-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:15px;margin:20px 0}
    .risk-grade-a{background-color:#ff1744;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-b{background-color:#ff9800;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-c{background-color:#ffc107;color:black;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-d{background-color:#4caf50;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-e{background-color:#2196f3;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    </style>
    """,
    unsafe_allow_html=True
)

# ----------------- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -----------------
ss = st.session_state
for key, default in {
    "index": None,
    "embeddings": None,
    "retriever_pool_df": None,
    "last_assessment": None
}.items():
    if key not in ss:
        ss[key] = default

# ----------------- ì–¸ì–´ ì„ íƒ UI -----------------
# ê²°ê³¼ ì–¸ì–´ë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ë„ë¡ í•¨
result_language = st.selectbox(
    "ê²°ê³¼ ì–¸ì–´ ì„ íƒ:",
    ["Korean", "English", "Chinese"],
    index=0
)
texts = system_texts[result_language]

# ----------------- í—¤ë” -----------------
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# ----------------- íƒ­ êµ¬ì„± -----------------
tabs = st.tabs([texts["tab_overview"], texts["tab_assessment"]])

# -----------------------------------------------------------------------------  
# --------------------------- Overview íƒ­ -------------------------------------  
# -----------------------------------------------------------------------------  
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)

    col_overview, col_features = st.columns([3, 2])
    with col_overview:
        st.markdown(f"<div class='info-text'>{texts['overview_text']}</div>", unsafe_allow_html=True)
    with col_features:
        st.markdown(f"**{texts['features_title']}**")
        st.markdown(texts["phase1_features"])
        st.markdown(texts["phase2_features"])

# -----------------------------------------------------------------------------  
# ---------------------- Risk Assessment & Improvement íƒ­ ----------------------  
# -----------------------------------------------------------------------------  
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["tab_assessment"]}</div>', unsafe_allow_html=True)

    col_api, col_dataset = st.columns([2, 1])
    with col_api:
        api_key = st.text_input(texts["api_key_label"], type="password")
    with col_dataset:
        dataset_name = st.selectbox(
            texts["dataset_label"],
            ["ê±´ì¶•", "í† ëª©", "í”ŒëœíŠ¸"]
        )

    # --- ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ë²„íŠ¼ ---
    if ss.retriever_pool_df is None or st.button(texts["load_data_btn"], type="primary"):
        if not api_key:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["data_loading"]):
                try:
                    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
                    def load_data(name: str):
                        if os.path.exists(f"{name}.xlsx"):
                            df = pd.read_excel(f"{name}.xlsx")
                        else:
                            # ìƒ˜í”Œ ë°ì´í„°
                            data = {
                                "ì‘ì—…í™œë™ ë° ë‚´ìš©": [
                                    "ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
                                    "ì½˜í¬ë¦¬íŠ¸/CMU ë¸”ë¡ ì„¤ì¹˜ ì‘ì—…",
                                    "êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…",
                                    "ê³ ì†Œ ì‘ì—…ëŒ€ë¥¼ ì´ìš©í•œ ì™¸ë²½ ì‘ì—…",
                                    "ìš©ì ‘ ì‘ì—…"
                                ],
                                "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": [
                                    "ë‹¤ì¤‘ ì¸ì–‘ìœ¼ë¡œ ì¸í•œ ì ì¬ë¬¼ ë‚™í•˜",
                                    "ë¶ˆì¶©ë¶„í•œ ì‘ì—… ë°œíŒìœ¼ë¡œ ì¸í•œ ì¶”ë½",
                                    "êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°",
                                    "ì•ˆì „ëŒ€ ë¯¸ì°©ìš©ìœ¼ë¡œ ì¸í•œ ì¶”ë½",
                                    "ìš©ì ‘ í„ ë° í™”ì¬ ìœ„í—˜"
                                ],
                                "ë¹ˆë„": [3, 3, 2, 4, 2],
                                "ê°•ë„": [5, 4, 5, 5, 3],
                                "ê°œì„ ëŒ€ì±…": [
                                    "1) ì•ˆì „ë²¨íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ì—­ë¬¼ ê³ ì • 2) ì ì¬ë¬¼ ê· í˜• ë§ì¶”ê¸° 3) ì‘ì—…ì ì•ˆì „ êµìœ¡ ì‹¤ì‹œ",
                                    "1) ë¹„ê³„ ì„¤ì¹˜ ë° ì•ˆì „ëŒ€ ì‚¬ìš© 2) ì¶”ë½ ë°©ì§€ë§ ì„¤ì¹˜ 3) ì‘ì—… ì „ ì ê²€",
                                    "1) ì‚¬ë©´ ê²½ì‚¬ ìœ ì§€ 2) êµ´ì°© ë²½ ë³´ê°• 3) ì§€ë°˜ ìƒíƒœ ì ê²€",
                                    "1) ì•ˆì „ëŒ€ ì°©ìš© ì˜ë¬´í™” 2) ì¶”ë½ ë°©ì§€ë§ ì„¤ì¹˜ 3) ì‘ì—… ì „ ì•ˆì „ êµìœ¡",
                                    "1) ìš©ì ‘ ë¶€ìœ„ ì°¨ë‹¨ 2) í™˜ê¸° ì‹œìŠ¤í…œ ì‚¬ìš© 3) ë³´í˜¸êµ¬ ì°©ìš©"
                                ]
                            }
                            df = pd.DataFrame(data)
                        df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
                        def determine_grade(val):
                            if 16 <= val <= 25: return "A"
                            if 10 <= val <= 15: return "B"
                            if 5 <= val <= 9: return "C"
                            if 3 <= val <= 4: return "D"
                            if 1 <= val <= 2: return "E"
                            return "Unknown"
                        df["ë“±ê¸‰"] = df["T"].apply(determine_grade)
                        return df

                    df = load_data(dataset_name)

                    if len(df) > 10:
                        train_df, _ = train_test_split(df, test_size=0.1, random_state=42)
                    else:
                        train_df = df.copy()

                    pool_df = train_df.copy()
                    pool_df["content"] = pool_df.apply(lambda r: " ".join(r.values.astype(str)), axis=1)

                    to_embed = pool_df["content"].tolist()
                    max_texts = min(len(to_embed), 30)
                    st.info(texts["demo_limit_info"].format(max_texts=max_texts))

                    # ì„ë² ë”© ìƒì„±
                    def embed_texts(texts_list, api_key, model="text-embedding-3-large"):
                        client = OpenAI(api_key=api_key)
                        embeds = []
                        batch_size = 10
                        for i in range(0, len(texts_list), batch_size):
                            batch = texts_list[i : i + batch_size]
                            proc = [str(x).replace("\n", " ").strip() for x in batch]
                            try:
                                resp = client.embeddings.create(model=model, input=proc)
                                for item in resp.data:
                                    embeds.append(item.embedding)
                            except Exception as e:
                                st.error(f"ì„ë² ë”© í˜¸ì¶œ ì‹¤íŒ¨ (ë°°ì¹˜ {i}): {e}")
                                for _ in batch:
                                    embeds.append([0.0] * 1536)
                        return embeds

                    embeds = embed_texts(to_embed[:max_texts], api_key)

                    vecs = np.array(embeds, dtype="float32")
                    dim = vecs.shape[1]
                    index = faiss.IndexFlatL2(dim)
                    index.add(vecs)

                    ss.index = index
                    ss.embeddings = vecs
                    ss.retriever_pool_df = pool_df.iloc[:max_texts]

                    st.success(texts["data_load_success"].format(max_texts=max_texts))
                    with st.expander("ğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                        st.dataframe(df.head(), use_container_width=True)
                except Exception as e:
                    st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    st.divider()
    st.markdown("### ğŸ” ìœ„í—˜ì„± í‰ê°€ ìˆ˜í–‰")

    activity = st.text_area(
        texts["activity_label"],
        placeholder="ì˜ˆ: ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
        height=100
    )
    include_similar_cases = st.checkbox(texts["include_similar"], value=True)
    run_button = st.button(texts["run_button"], type="primary", use_container_width=True)

    if run_button:
        if not activity:
            st.warning(texts["activity_warning"])
        elif not api_key:
            st.warning(texts["api_key_warning"])
        elif ss.index is None:
            st.warning(texts["load_first_warning"])
        else:
            with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                try:
                    # === Phase 1: Risk Assessment ===
                    client = OpenAI(api_key=api_key)

                    # Query embedding
                    def embed_single(text):
                        resp = client.embeddings.create(model="text-embedding-3-large", input=[text])
                        return resp.data[0].embedding

                    q_emb = embed_single(activity)
                    D, I = ss.index.search(np.array([q_emb], dtype="float32"), k=min(10, len(ss.retriever_pool_df)))
                    sim_docs = ss.retriever_pool_df.iloc[I[0]]

                    # 1) Hazard prediction prompt (English internal)
                    def construct_hazard_prompt(docs, activity):
                        prompt = "Here are examples of work activities and associated hazardous factors:\n\n"
                        for i, row in docs.head(5).iterrows():
                            prompt += f"Case {i+1}:\n- Work Activity: {row['ì‘ì—…í™œë™ ë° ë‚´ìš©']}\n- Hazardous Factors: {row['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}\n\n"
                        prompt += f"Based on the above examples, predict the main hazardous factors for the following work activity:\n\nWork Activity: {activity}\n\nPredicted Hazardous Factors: "
                        return prompt

                    hazard_prompt = construct_hazard_prompt(sim_docs, activity)
                    hazard_en = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[{"role": "system", "content": "You are a construction site risk assessment expert. Provide practical responses in English."},
                                  {"role": "user", "content": hazard_prompt}],
                        temperature=0.1,
                        max_tokens=200
                    ).choices[0].message.content.strip()

                    # Translate hazard to selected language if needed
                    if result_language == "Korean":
                        hazard = client.chat.completions.create(
                            model="gpt-4o",
                            messages=[{"role": "system", "content": "Translate English to Korean. Keep technical terms."},
                                      {"role": "user", "content": hazard_en}],
                            temperature=0.1,
                            max_tokens=200
                        ).choices[0].message.content.strip()
                    elif result_language == "Chinese":
                        hazard = client.chat.completions.create(
                            model="gpt-4o",
                            messages=[{"role": "system", "content": "Translate English to Chinese. Keep technical terms."},
                                      {"role": "user", "content": hazard_en}],
                            temperature=0.1,
                            max_tokens=200
                        ).choices[0].message.content.strip()
                    else:
                        hazard = hazard_en

                    # 2) Risk assessment prompt (English)
                    def construct_risk_prompt(docs, activity, hazard_en):
                        prompt = (
                            "Construction site risk assessment criteria:\n"
                            "- Frequency (1-5): 1=Very Rare, 2=Rare, 3=Occasional, 4=Frequent, 5=Very Frequent\n"
                            "- Severity (1-5): 1=Minor Injury, 2=Light Injury, 3=Moderate Injury, 4=Serious Injury, 5=Fatality\n"
                            "- T-value = Frequency Ã— Severity\n\n"
                            "Reference cases:\n\n"
                        )
                        for i, row in docs.head(3).iterrows():
                            inp = f"{row['ì‘ì—…í™œë™ ë° ë‚´ìš©']} - {row['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}"
                            freq = int(row["ë¹ˆë„"])
                            sev = int(row["ê°•ë„"])
                            t_val = freq * sev
                            prompt += f"Case {i+1}:\nInput: {inp}\nAssessment: Frequency={freq}, Severity={sev}, T-value={t_val}\n\n"
                        prompt += (
                            f"Based on the above criteria and cases, assess the following:\n\n"
                            f"Work Activity: {activity}\n"
                            f"Hazardous Factors: {hazard_en}\n\n"
                            f"Respond in JSON format: " 
                            f'{{"frequency": number, "severity": number, "T": number}}'
                        )
                        return prompt

                    risk_prompt = construct_risk_prompt(sim_docs, activity, hazard_en)
                    risk_resp = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[{"role": "system", "content": "You are a construction site risk assessment expert. Provide practical responses in English."},
                                  {"role": "user", "content": risk_prompt}],
                        temperature=0.1,
                        max_tokens=200
                    ).choices[0].message.content.strip()

                    # Parse JSON
                    match = re.search(r'\{"frequency":\s*([1-5]),\s*"severity":\s*([1-5]),\s*"T":\s*([0-9]+)\}', risk_resp)
                    if match:
                        freq = int(match.group(1))
                        sev = int(match.group(2))
                        T = int(match.group(3))
                    else:
                        nums = re.findall(r'\b([1-5])\b', risk_resp)
                        if len(nums) >= 2:
                            freq = int(nums[0])
                            sev = int(nums[1])
                            T = freq * sev
                        else:
                            st.error(texts["parsing_error"])
                            st.stop()

                    def determine_grade(val):
                        if 16 <= val <= 25: return "A"
                        if 10 <= val <= 15: return "B"
                        if 5 <= val <= 9: return "C"
                        if 3 <= val <= 4: return "D"
                        if 1 <= val <= 2: return "E"
                        return "Unknown"

                    grade = determine_grade(T)

                    # === Phase 2: Improvement Measures ===
                    def construct_improvement_prompt(docs, activity, hazard_en, freq, sev, T):
                        prompt = ""
                        for i, row in docs.head(3).iterrows():
                            plan = row.get("ê°œì„ ëŒ€ì±…", "")
                            orig_f = int(row["ë¹ˆë„"])
                            orig_s = int(row["ê°•ë„"])
                            orig_t = orig_f * orig_s
                            new_f = max(1, orig_f - 1)
                            new_s = max(1, orig_s - 1)
                            new_t = new_f * new_s
                            prompt += (
                                f"Example {i+1}:\n"
                                f"Input Work Activity: {row['ì‘ì—…í™œë™ ë° ë‚´ìš©']}\n"
                                f"Input Hazardous Factors: {row['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}\n"
                                f"Input Original Frequency: {orig_f}\n"
                                f"Input Original Severity: {orig_s}\n"
                                f"Input Original T-value: {orig_t}\n"
                                f"Output (JSON):\n"
                                "{\n"
                                f'  "control_measures": "{plan}",\n'
                                f'  "post_frequency": {new_f},\n'
                                f'  "post_severity": {new_s},\n'
                                f'  "post_T": {new_t},\n'
                                f'  "reduction_rate": {((orig_t - new_t)/orig_t)*100:.2f}\n'
                                "}\n\n"
                            )
                        prompt += (
                            f"Now provide improvement measures in JSON for the following:\n\n"
                            f"Work Activity: {activity}\n"
                            f"Hazardous Factors: {hazard_en}\n"
                            f"Original Frequency: {freq}\n"
                            f"Original Severity: {sev}\n"
                            f"Original T-value: {T}\n\n"
                            "JSON schema:\n"
                            "{\n"
                            '  "control_measures": "string",\n'
                            '  "post_frequency": number,\n'
                            '  "post_severity": number,\n'
                            '  "post_T": number,\n'
                            '  "reduction_rate": number\n'
                            "}"
                        )
                        return prompt

                    imp_prompt = construct_improvement_prompt(sim_docs, activity, hazard_en, freq, sev, T)
                    imp_resp = client.chat.completions.create(
                        model="gpt-4o",
                        messages=[{"role": "system", "content": "You are a construction site risk assessment expert. Provide practical responses in English."},
                                  {"role": "user", "content": imp_prompt}],
                        temperature=0.1,
                        max_tokens=300
                    ).choices[0].message.content.strip()

                    # Parse improvement JSON
                    imp_match = re.search(r'\{.*\}', imp_resp, re.DOTALL)
                    if not imp_match:
                        st.error(texts["parsing_error"])
                        st.stop()
                    import json
                    try:
                        imp_data = json.loads(imp_match.group())
                        ctrl_en = imp_data.get("control_measures", "")
                        post_freq = imp_data.get("post_frequency", 1)
                        post_sev = imp_data.get("post_severity", 1)
                        post_T = imp_data.get("post_T", post_freq * post_sev)
                        rrr = imp_data.get("reduction_rate", 0.0)
                    except:
                        st.error(texts["parsing_error"])
                        st.stop()

                    # Translate control measures if needed
                    if result_language == "Korean":
                        ctrl = client.chat.completions.create(
                            model="gpt-4o",
                            messages=[{"role": "system", "content": "Translate English to Korean. Keep technical terms."},
                                      {"role": "user", "content": ctrl_en}],
                            temperature=0.1,
                            max_tokens=200
                        ).choices[0].message.content.strip()
                    elif result_language == "Chinese":
                        ctrl = client.chat.completions.create(
                            model="gpt-4o",
                            messages=[{"role": "system", "content": "Translate English to Chinese. Keep technical terms."},
                                      {"role": "user", "content": ctrl_en}],
                            temperature=0.1,
                            max_tokens=200
                        ).choices[0].message.content.strip()
                    else:
                        ctrl = ctrl_en

                    # === Display Results ===
                    st.markdown(texts["phase1_header"])
                    col1, col2 = st.columns([2, 1])
                    with col1:
                        st.markdown(f"**ì‘ì—…í™œë™ / Work Activity:** {activity}")
                        st.markdown(f"**ìœ í•´ìœ„í—˜ìš”ì¸ / Hazardous Factors:** {hazard}")
                        st.markdown(f"**ë¹ˆë„ / Frequency:** {freq}")
                        st.markdown(f"**ê°•ë„ / Severity:** {sev}")
                        st.markdown(f"**Tê°’ / T-value:** {T} (Grade: {grade})")
                    with col2:
                        color_map = {"A": "#ff1744","B": "#ff9800","C": "#ffc107","D": "#4caf50","E": "#2196f3"}
                        grade_color = color_map.get(grade, "#808080")
                        st.markdown(f"""
                        <div style="text-align:center; padding:20px; background-color:{grade_color};
                                    color:white; border-radius:10px; margin:10px 0;">
                            <h2 style="margin:0;">Grade</h2>
                            <h1 style="margin:10px 0; font-size:3rem;">{grade}</h1>
                            <p style="margin:0;">T-value: {T}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    if include_similar_cases:
                        st.markdown("### ğŸ” ìœ ì‚¬í•œ ì‚¬ë¡€ / Similar Cases")
                        for i in range(len(sim_docs)):
                            doc = sim_docs.iloc[i]
                            plan_candidate, imp_f, imp_i, imp_t = "", max(1, int(doc["ë¹ˆë„"]) - 1), max(1, int(doc["ê°•ë„"]) - 1), None
                            if "ê°œì„ ëŒ€ì±…" in doc and pd.notna(doc["ê°œì„ ëŒ€ì±…"]):
                                plan_candidate = doc["ê°œì„ ëŒ€ì±…"]
                            imp_t = imp_f * imp_i
                            with st.expander(f"ì‚¬ë¡€ {i+1}: {doc['ì‘ì—…í™œë™ ë° ë‚´ìš©'][:30]}â€¦"):
                                c1, c2 = st.columns(2)
                                with c1:
                                    st.write(f"**ì‘ì—…í™œë™ / Work Activity:** {doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']}")
                                    st.write(f"**ìœ í•´ìœ„í—˜ìš”ì¸ / Hazardous Factors:** {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}")
                                    st.write(f"**ë¹ˆë„ / Frequency:** {doc['ë¹ˆë„']}")
                                    st.write(f"**ê°•ë„ / Severity:** {doc['ê°•ë„']}")
                                    st.write(f"**Tê°’ / T-value:** {doc['T']} (Grade: {doc['ë“±ê¸‰']})")
                                with c2:
                                    st.write(f"**ê°œì„ ëŒ€ì±… / Control Measures:**")
                                    formatted = re.sub(r"\s*\n\s*", "<br>", plan_candidate.strip())
                                    st.markdown(formatted, unsafe_allow_html=True)

                    st.markdown(texts["phase2_header"])
                    col3, col4 = st.columns([3, 2])
                    with col3:
                        st.markdown(f"### {texts['improvement_plan_header']}")
                        # ê³ ì •ëœ ê°œì„ ëŒ€ì±… í…ìŠ¤íŠ¸ ì˜ˆì‹œ (ì¤„ë°”ê¿ˆ í¬í•¨)
                        st.markdown(
                            """
1) ëª¨ë“  ì ì¬ë¬¼ì€ ì ì ˆí•œ ë˜ì‹± ë²¨íŠ¸ì™€ ê³ ì • ì¥ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê³ ì •í•©ë‹ˆë‹¤.<br>
2) ìš´ì†¡ ì°¨ëŸ‰ì˜ ì´ë™ ê²½ë¡œë¥¼ ëª…í™•íˆ í•˜ê³ , ì‘ì—…ìë“¤ì—ê²Œ ê²½ë¡œë¥¼ ì‚¬ì „ì— ê³µì§€í•©ë‹ˆë‹¤.<br>
3) ì ì¬ë¬¼ì€ ê· í˜•ì„ ë§ì¶° ì•ˆì „í•˜ê²Œ ìŒ“ê³ , í•„ìš”ì‹œ ëª©ì¬ ìŠ¤í˜ì´ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.<br>
4) ìš´ì†¡ ì°¨ëŸ‰ì˜ ì†ë„ë¥¼ ì œí•œí•˜ê³ , ìš´ì „ìëŠ” ë„ë¡œ ìƒíƒœë¥¼ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•˜ë©° ìš´ì „í•©ë‹ˆë‹¤.<br>
5) ë¬´ê±°ìš´ ì ì¬ë¬¼ì˜ ìˆ˜ë™ ì·¨ê¸‰ ì‹œ, ì ì ˆí•œ ì¸ë ¥ ë°°ì¹˜ì™€ ë¦¬í”„íŒ… ì¥ë¹„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¼ê³¨ê²©ê³„ ë¶€ìƒì„ ì˜ˆë°©í•©ë‹ˆë‹¤.
                            """,
                            unsafe_allow_html=True
                        )
                    with col4:
                        st.markdown(f"### {texts['risk_improvement_header']}")
                        risk_df = pd.DataFrame(
                            [
                                {
                                    "Work Sequence": activity,
                                    "Hazardous Factors": hazard,
                                    "EHS": "",
                                    "Frequency": freq,
                                    "Severity": sev,
                                    "Control Measures": ctrl,
                                    "In Charge": "",
                                    "Correction Due Date": ""
                                },
                                {
                                    "Work Sequence": activity,
                                    "Hazardous Factors": hazard,
                                    "EHS": "",
                                    "Frequency": post_freq,
                                    "Severity": post_sev,
                                    "Control Measures": ctrl,
                                    "In Charge": "",
                                    "Correction Due Date": ""
                                }
                            ],
                            index=[texts["risk_table_pre"], texts["risk_table_post"]]
                        )
                        st.dataframe(risk_df.astype(str), use_container_width=True)

                    ss.last_assessment = {
                        "activity": activity,
                        "hazard": hazard,
                        "freq": freq,
                        "severity": sev,
                        "T": T,
                        "grade": grade,
                        "control_measures": ctrl,
                        "post_freq": post_freq,
                        "post_severity": post_sev,
                        "post_T": post_T,
                        "rrr": rrr
                    }

                    st.markdown("### ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
                    def create_excel_download():
                        output = io.BytesIO()
                        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                            workbook = writer.book
                            fmt = workbook.add_format({"font_color": "#FF0000", "text_wrap": True})

                            excel_df = pd.DataFrame(
                                [
                                    {
                                        "Work Sequence": activity,
                                        "Hazardous Factors": hazard,
                                        "EHS": "",
                                        "Frequency": freq,
                                        "Severity": sev,
                                        "Control Measures": ctrl,
                                        "In Charge": "",
                                        "Correction Due Date": ""
                                    },
                                    {
                                        "Work Sequence": activity,
                                        "Hazardous Factors": hazard,
                                        "EHS": "",
                                        "Frequency": post_freq,
                                        "Severity": post_sev,
                                        "Control Measures": ctrl,
                                        "In Charge": "",
                                        "Correction Due Date": ""
                                    }
                                ],
                                index=[texts["risk_table_pre"], texts["risk_table_post"]]
                            )
                            excel_df.reset_index(drop=True, inplace=True)
                            excel_df.to_excel(writer, sheet_name="Risk_and_Improvement", index=False)
                            ws = writer.sheets["Risk_and_Improvement"]
                            for col_idx in range(len(excel_df.columns)):
                                ws.set_column(col_idx, col_idx, 20, fmt)

                        return output.getvalue()

                    excel_bytes = create_excel_download()
                    st.download_button(
                        label=texts["excel_export"],
                        data=excel_bytes,
                        file_name="risk_assessment_results.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                except Exception as e:
                    st.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
                    st.stop()

# ------------------- í‘¸í„° ------------------------
st.markdown('<hr style="margin-top: 3rem;">', unsafe_allow_html=True)
footer_col1, footer_col2, footer_col3 = st.columns([1, 1, 1])
with footer_col2:
    st.markdown(
        f"<div style='text-align: center; padding: 20px;'>{texts['footer_text']}</div>",
        unsafe_allow_html=True
    )
