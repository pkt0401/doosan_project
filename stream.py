import streamlit as st
import pandas as pd
import numpy as np
import faiss
import re
import os
import io
from PIL import Image
from sklearn.model_selection import train_test_split
from openai import OpenAI

# ----------------- ì‹œìŠ¤í…œ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ -----------------
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_phase1": "ìœ„í—˜ì„± í‰ê°€ (Phase 1)",
        "tab_phase2": "ê°œì„ ëŒ€ì±… ìƒì„± (Phase 2)",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹° AI Risk AssessmentëŠ” êµ­ë‚´ ë° í•´ì™¸ ê±´ì„¤í˜„ì¥ 'ìˆ˜ì‹œìœ„í—˜ì„±í‰ê°€' ë° 'ë…¸ë™ë¶€ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€'ë¥¼ í•™ìŠµí•˜ì—¬ ê°œë°œëœ ìë™ ìœ„í—˜ì„±í‰ê°€ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. ìƒì„±ëœ ìœ„í—˜ì„±í‰ê°€ëŠ” ë°˜ë“œì‹œ ìˆ˜ì‹œ ìœ„í—˜ì„±í‰ê°€ ì‹¬ì˜íšŒë¥¼ í†µí•´ ê²€ì¦ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase1_features": """
        #### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”
        - ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ
        - ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡
        - ìœ ì‚¬ ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ
        - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì •
        - Excel ê¸°ë°˜ ê³µì •ë³„ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° ìë™ ë¶„ì„
        - ìœ„í—˜ë“±ê¸‰(A-E) ìë™ ì‚°ì •
        """,
        "phase2_features": """
        #### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ìœ„í—˜ìš”ì†Œë³„ ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ë‹¤êµ­ì–´(í•œ/ì˜/ì¤‘) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›
        - ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„
        - ìœ„í—˜ ê°ì†Œìœ¨(RRR) ì •ëŸ‰ì  ì‚°ì¶œ
        - ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        """,
        "phase1_header": "ìœ„í—˜ì„± í‰ê°€ ìë™í™” (Phase 1)",
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ ëª©ì ìœ¼ë¡œ {max_texts}ê°œì˜ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤. ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {max_texts}ê°œ í•­ëª© ì²˜ë¦¬)",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "predict_hazard_btn": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡í•˜ê¸°",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "similar_cases_header": "ìœ ì‚¬í•œ ì‚¬ë¡€",
        "similar_case_text": """
        <div class="similar-case">
            <strong>ì‚¬ë¡€ {i}</strong><br>
            <strong>ì‘ì—…í™œë™:</strong> {activity}<br>
            <strong>ìœ í•´ìœ„í—˜ìš”ì¸:</strong> {hazard}<br>
            <strong>ìœ„í—˜ë„:</strong> ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})
        </div>
        """,
        "result_table_columns": ["í•­ëª©", "ê°’"],
        "result_table_rows": ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "phase2_header": "ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (Phase 2)",
        "language_select_label": "ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ:",
        "input_method_label": "ì…ë ¥ ë°©ì‹ ì„ íƒ:",
        "input_methods": ["Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©", "ì§ì ‘ ì…ë ¥"],
        "phase1_results_header": "Phase 1 í‰ê°€ ê²°ê³¼",
        "risk_level_text": "ìœ„í—˜ë„: ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})",
        "phase1_first_warning": "ë¨¼ì € Phase 1ì—ì„œ ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.",
        "hazard_label": "ìœ í•´ìœ„í—˜ìš”ì¸:",
        "frequency_label": "ë¹ˆë„ (1-5):",
        "intensity_label": "ê°•ë„ (1-5):",
        "t_value_text": "Tê°’: {t_value} (ë“±ê¸‰: {grade})",
        "generate_improvement_btn": "ê°œì„ ëŒ€ì±… ìƒì„±",
        "generating_improvement": "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ëŠ” ì¤‘...",
        "no_data_warning": "Phase 1ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±ì„ ì™„ë£Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "improvement_result_header": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ìœ„í—˜ë„ ê°œì„  ê²°ê³¼",
        "comparison_columns": ["í•­ëª©", "ê°œì„  ì „", "ê°œì„  í›„"],
        "risk_reduction_label": "ìœ„í—˜ ê°ì†Œìœ¨ (RRR)",
        "t_value_change_header": "ìœ„í—˜ë„(Tê°’) ë³€í™”",
        "excel_export": "ğŸ“¥ ê²°ê³¼ Excel ë‹¤ìš´ë¡œë“œ",
        "risk_classification": "ìœ„í—˜ë„ ë¶„ë¥˜",
        "supported_languages": "ì§€ì› ì–¸ì–´",
        "languages_count": "3ê°œ",
        "languages_detail": "í•œ/ì˜/ì¤‘",
        "assessment_phases": "í‰ê°€ ë‹¨ê³„",
        "phases_count": "2ë‹¨ê³„",
        "phases_detail": "í‰ê°€+ê°œì„ ",
        "risk_grades": "ìœ„í—˜ë“±ê¸‰",
        "grades_count": "5ë“±ê¸‰",
        "grades_detail": "A~E",
        "dataset_options": ["ê±´ì¶•", "í† ëª©", "í”ŒëœíŠ¸"],
        "include_similar_cases": "ìœ ì‚¬ ì‚¬ë¡€ í¬í•¨",
        "result_language": "ê²°ê³¼ ì–¸ì–´",
        "run_assessment": "ğŸš€ ìœ„í—˜ì„± í‰ê°€ ì‹¤í–‰",
        "performing_assessment": "ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¤‘...",
        "phase1_results": "ğŸ“‹ Phase 1: ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼",
        "work_activity": "ì‘ì—…í™œë™",
        "predicted_hazard": "ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸",
        "risk_grade_display": "ìœ„í—˜ë“±ê¸‰",
        "t_value_display": "Tê°’",
        "similar_cases_section": "ğŸ” ìœ ì‚¬í•œ ì‚¬ë¡€",
        "case_number": "ì‚¬ë¡€",
        "phase2_results": "ğŸ› ï¸ Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_failed": "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
        "risk_visualization": "ğŸ“Š ìœ„í—˜ë„ ë³€í™” ì‹œê°í™”",
        "before_improvement": "ê°œì„  ì „ ìœ„í—˜ë„",
        "after_improvement": "ê°œì„  í›„ ìœ„í—˜ë„",
        "grade_label": "ë“±ê¸‰",
        "download_results": "ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ"
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_phase1": "Risk Assessment (Phase 1)",
        "tab_phase2": "Improvement Measures (Phase 2)",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": "Doosan Enerbility AI Risk Assessment is an automated program trained on both on-demand risk-assessment reports from domestic and overseas construction sites and major-accident cases compiled by Korea's Ministry of Employment and Labor. Please ensure that every generated assessment is reviewed and approved by the On-Demand Risk Assessment Committee before it is used.",
        "features_title": "System Features and Components",
        "phase1_features": """
        #### Phase 1: Risk Assessment Automation
        - Learning risk assessment data according to work activities by process
        - Automatic hazard prediction when work activities are entered
        - Similar case search and display
        - Risk level (frequency, intensity, T) measurement based on large language models (LLM)
        - Automatic analysis of Excel-based process-specific risk assessment data
        - Automatic risk grade (A-E) calculation
        """,
        "phase2_features": """
        #### Phase 2: Automatic Generation of Improvement Measures
        - Automatic generation of customized improvement measures for risk factors
        - Multilingual (Korean/English/Chinese) improvement measure generation support
        - Automatic comparative analysis of risk level (T) before and after improvement
        - Quantitative calculation of Risk Reduction Rate (RRR)
        - Building a database of optimal improvement measures by work type/process
        """,
        "phase1_header": "Risk Assessment Automation (Phase 1)",
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset",
        "load_data_btn": "Load Data and Configure Index",
        "api_key_warning": "Please enter an OpenAI API key to continue.",
        "data_loading": "Loading data and configuring index...",
        "demo_limit_info": "For demo purposes, only embedding {max_texts} texts. In a real environment, all data should be processed.",
        "data_load_success": "Data load and index configuration complete! (Total {max_texts} items processed)",
        "load_first_warning": "Please click the [Load Data and Configure Index] button first.",
        "activity_label": "Work Activity:",
        "predict_hazard_btn": "Predict Hazards",
        "activity_warning": "Please enter a work activity.",
        "similar_cases_header": "Similar Cases",
        "similar_case_text": """
        <div class="similar-case">
            <strong>Case {i}</strong><br>
            <strong>Work Activity:</strong> {activity}<br>
            <strong>Hazard:</strong> {hazard}<br>
            <strong>Risk Level:</strong> Frequency {freq}, Intensity {intensity}, T-value {t_value} (Grade {grade})
        </div>
        """,
        "result_table_columns": ["Item", "Value"],
        "result_table_rows": ["Frequency", "Intensity", "T Value", "Risk Grade"],
        "parsing_error": "Unable to parse risk assessment results.",
        "phase2_header": "Automatic Generation of Improvement Measures (Phase 2)",
        "language_select_label": "Select Language for Improvement Measures:",
        "input_method_label": "Select Input Method:",
        "input_methods": ["Use Phase 1 Assessment Results", "Direct Input"],
        "phase1_results_header": "Phase 1 Assessment Results",
        "risk_level_text": "Risk Level: Frequency {freq}, Intensity {intensity}, T-value {t_value} (Grade {grade})",
        "phase1_first_warning": "Please perform a risk assessment in Phase 1 first.",
        "hazard_label": "Hazard:",
        "frequency_label": "Frequency (1-5):",
        "intensity_label": "Intensity (1-5):",
        "t_value_text": "T-value: {t_value} (Grade: {grade})",
        "generate_improvement_btn": "Generate Improvement Measures",
        "generating_improvement": "Generating improvement measures...",
        "no_data_warning": "Data loading and index configuration was not completed in Phase 1. Using basic examples.",
        "improvement_result_header": "Improvement Measure Generation Results",
        "improvement_plan_header": "Improvement Measures",
        "risk_improvement_header": "Risk Level Improvement Results",
        "comparison_columns": ["Item", "Before Improvement", "After Improvement"],
        "risk_reduction_label": "Risk Reduction Rate (RRR)",
        "t_value_change_header": "Risk Level (T-value) Change",
        "excel_export": "ğŸ“¥ Download Excel Results",
        "risk_classification": "Risk Classification",
        "supported_languages": "Supported Languages",
        "languages_count": "3 Languages",
        "languages_detail": "KOR/ENG/CHN",
        "assessment_phases": "Assessment Phases",
        "phases_count": "2 Phases",
        "phases_detail": "Assessment+Improvement",
        "risk_grades": "Risk Grades",
        "grades_count": "5 Grades",
        "grades_detail": "A~E",
        "dataset_options": ["Architecture", "Civil", "Plant"],
        "include_similar_cases": "Include Similar Cases",
        "result_language": "Result Language",
        "run_assessment": "ğŸš€ Run Risk Assessment",
        "performing_assessment": "Performing risk assessment...",
        "phase1_results": "ğŸ“‹ Phase 1: Risk Assessment Results",
        "work_activity": "Work Activity",
        "predicted_hazard": "Predicted Hazard",
        "risk_grade_display": "Risk Grade",
        "t_value_display": "T-value",
        "similar_cases_section": "ğŸ” Similar Cases",
        "case_number": "Case",
        "phase2_results": "ğŸ› ï¸ Phase 2: Improvement Measures Results",
        "improvement_failed": "Failed to generate improvement measures.",
        "risk_visualization": "ğŸ“Š Risk Level Change Visualization",
        "before_improvement": "Before Improvement",
        "after_improvement": "After Improvement",
        "grade_label": "Grade",
        "download_results": "ğŸ’¾ Download Results"
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_phase1": "é£é™©è¯„ä¼° (ç¬¬1é˜¶æ®µ)",
        "tab_phase2": "æ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "overview_header": "åŸºäºLLMçš„é£é™©è¯„ä¼°ç³»ç»Ÿ",
        "overview_text": "Doosan Enerbility AI é£é™©è¯„ä¼°ç³»ç»Ÿæ˜¯ä¸€æ¬¾è‡ªåŠ¨åŒ–é£é™©è¯„ä¼°ç¨‹åºï¼ŒåŸºäºå›½å†…å¤–æ–½å·¥ç°åœºçš„'ä¸´æ—¶é£é™©è¯„ä¼°'æ•°æ®ä»¥åŠéŸ©å›½é›‡ä½£åŠ³åŠ¨éƒ¨çš„é‡å¤§äº‹æ•…æ¡ˆä¾‹è¿›è¡Œè®­ç»ƒå¼€å‘è€Œæˆã€‚ç”Ÿæˆçš„é£é™©è¯„ä¼°ç»“æœå¿…é¡»ç»è¿‡ä¸´æ—¶é£é™©è¯„ä¼°å®¡è®®å§”å‘˜ä¼šçš„å®¡æ ¸åæ–¹å¯ä½¿ç”¨ã€‚",
        "features_title": "ç³»ç»Ÿç‰¹ç‚¹å’Œç»„ä»¶",
        "phase1_features": """
        #### ç¬¬1é˜¶æ®µï¼šé£é™©è¯„ä¼°è‡ªåŠ¨åŒ–
        - æŒ‰å·¥åºå­¦ä¹ ä¸å·¥ä½œæ´»åŠ¨ç›¸å…³çš„é£é™©è¯„ä¼°æ•°æ®
        - è¾“å…¥å·¥ä½œæ´»åŠ¨æ—¶è‡ªåŠ¨é¢„æµ‹å±å®³å› ç´ 
        - ç›¸ä¼¼æ¡ˆä¾‹æœç´¢å’Œæ˜¾ç¤º
        - åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLM)çš„é£é™©ç­‰çº§ï¼ˆé¢‘ç‡ã€å¼ºåº¦ã€Tå€¼ï¼‰æµ‹é‡
        - è‡ªåŠ¨åˆ†æåŸºäºExcelçš„ç‰¹å®šå·¥åºé£é™©è¯„ä¼°æ•°æ®
        - è‡ªåŠ¨è®¡ç®—é£é™©ç­‰çº§(A-E)
        """,
        "phase2_features": """
        #### ç¬¬2é˜¶æ®µï¼šè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½
        - ä¸ºé£é™©å› ç´ è‡ªåŠ¨ç”Ÿæˆå®šåˆ¶çš„æ”¹è¿›æªæ–½
        - å¤šè¯­è¨€ï¼ˆéŸ©è¯­/è‹±è¯­/ä¸­æ–‡ï¼‰æ”¹è¿›æªæ–½ç”Ÿæˆæ”¯æŒ
        - æ”¹è¿›å‰åé£é™©ç­‰çº§ï¼ˆTå€¼ï¼‰çš„è‡ªåŠ¨æ¯”è¾ƒåˆ†æ
        - é£é™©é™ä½ç‡(RRR)çš„å®šé‡è®¡ç®—
        - å»ºç«‹æŒ‰å·¥ä½œç±»å‹/å·¥åºçš„æœ€ä½³æ”¹è¿›æªæ–½æ•°æ®åº“
        """,
        "phase1_header": "é£é™©è¯„ä¼°è‡ªåŠ¨åŒ– (ç¬¬1é˜¶æ®µ)",
        "api_key_label": "è¾“å…¥OpenAI APIå¯†é’¥ï¼š",
        "dataset_label": "é€‰æ‹©æ•°æ®é›†",
        "load_data_btn": "åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•",
        "api_key_warning": "è¯·è¾“å…¥OpenAI APIå¯†é’¥ä»¥ç»§ç»­ã€‚",
        "data_loading": "æ­£åœ¨åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•...",
        "demo_limit_info": "å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œä»…åµŒå…¥{max_texts}ä¸ªæ–‡æœ¬ã€‚åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œåº”å¤„ç†æ‰€æœ‰æ•°æ®ã€‚",
        "data_load_success": "æ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®å®Œæˆï¼ï¼ˆå…±å¤„ç†{max_texts}ä¸ªé¡¹ç›®ï¼‰",
        "load_first_warning": "è¯·å…ˆç‚¹å‡»[åŠ è½½æ•°æ®å’Œé…ç½®ç´¢å¼•]æŒ‰é’®ã€‚",
        "activity_label": "å·¥ä½œæ´»åŠ¨ï¼š",
        "predict_hazard_btn": "é¢„æµ‹å±å®³",
        "activity_warning": "è¯·è¾“å…¥å·¥ä½œæ´»åŠ¨ã€‚",
        "similar_cases_header": "ç›¸ä¼¼æ¡ˆä¾‹",
        "similar_case_text": """
        <div class="similar-case">
            <strong>æ¡ˆä¾‹ {i}</strong><br>
            <strong>å·¥ä½œæ´»åŠ¨ï¼š</strong> {activity}<br>
            <strong>å±å®³ï¼š</strong> {hazard}<br>
            <strong>é£é™©ç­‰çº§ï¼š</strong> é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, Tå€¼ {t_value} (ç­‰çº§ {grade})
        </div>
        """,
        "result_table_columns": ["é¡¹ç›®", "å€¼"],
        "result_table_rows": ["é¢‘ç‡", "å¼ºåº¦", "Tå€¼", "é£é™©ç­‰çº§"],
        "parsing_error": "æ— æ³•è§£æé£é™©è¯„ä¼°ç»“æœã€‚",
        "phase2_header": "è‡ªåŠ¨ç”Ÿæˆæ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "language_select_label": "é€‰æ‹©æ”¹è¿›æªæ–½çš„è¯­è¨€ï¼š",
        "input_method_label": "é€‰æ‹©è¾“å…¥æ–¹æ³•ï¼š",
        "input_methods": ["ä½¿ç”¨ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ", "ç›´æ¥è¾“å…¥"],
        "phase1_results_header": "ç¬¬1é˜¶æ®µè¯„ä¼°ç»“æœ",
        "risk_level_text": "é£é™©ç­‰çº§: é¢‘ç‡ {freq}, å¼ºåº¦ {intensity}, Tå€¼ {t_value} (ç­‰çº§ {grade})",
        "phase1_first_warning": "è¯·å…ˆåœ¨ç¬¬1é˜¶æ®µè¿›è¡Œé£é™©è¯„ä¼°ã€‚",
        "hazard_label": "å±å®³ï¼š",
        "frequency_label": "é¢‘ç‡ (1-5)ï¼š",
        "intensity_label": "å¼ºåº¦ (1-5)ï¼š",
        "t_value_text": "Tå€¼: {t_value} (ç­‰çº§: {grade})",
        "generate_improvement_btn": "ç”Ÿæˆæ”¹è¿›æªæ–½",
        "generating_improvement": "æ­£åœ¨ç”Ÿæˆæ”¹è¿›æªæ–½...",
        "no_data_warning": "åœ¨ç¬¬1é˜¶æ®µæœªå®Œæˆæ•°æ®åŠ è½½å’Œç´¢å¼•é…ç½®ã€‚ä½¿ç”¨åŸºæœ¬ç¤ºä¾‹ã€‚",
        "improvement_result_header": "æ”¹è¿›æªæ–½ç”Ÿæˆç»“æœ",
        "improvement_plan_header": "æ”¹è¿›æªæ–½",
        "risk_improvement_header": "é£é™©ç­‰çº§æ”¹è¿›ç»“æœ",
        "comparison_columns": ["é¡¹ç›®", "æ”¹è¿›å‰", "æ”¹è¿›å"],
        "risk_reduction_label": "é£é™©é™ä½ç‡ (RRR)",
        "t_value_change_header": "é£é™©ç­‰çº§ (Tå€¼) å˜åŒ–",
        "excel_export": "ğŸ“¥ ä¸‹è½½Excelç»“æœ",
        "risk_classification": "é£é™©åˆ†ç±»",
        "supported_languages": "æ”¯æŒè¯­è¨€",
        "languages_count": "3ç§è¯­è¨€",
        "languages_detail": "éŸ©/è‹±/ä¸­",
        "assessment_phases": "è¯„ä¼°é˜¶æ®µ",
        "phases_count": "2ä¸ªé˜¶æ®µ",
        "phases_detail": "è¯„ä¼°+æ”¹è¿›",
        "risk_grades": "é£é™©ç­‰çº§",
        "grades_count": "5ä¸ªç­‰çº§",
        "grades_detail": "A~E",
        "dataset_options": ["å»ºç­‘", "åœŸæœ¨", "å·¥å‚"],
        "include_similar_cases": "åŒ…æ‹¬ç›¸ä¼¼æ¡ˆä¾‹",
        "result_language": "ç»“æœè¯­è¨€",
        "run_assessment": "ğŸš€ è¿è¡Œé£é™©è¯„ä¼°",
        "performing_assessment": "æ­£åœ¨è¿›è¡Œé£é™©è¯„ä¼°...",
        "phase1_results": "ğŸ“‹ ç¬¬1é˜¶æ®µ: é£é™©è¯„ä¼°ç»“æœ",
        "work_activity": "å·¥ä½œæ´»åŠ¨",
        "predicted_hazard": "é¢„æµ‹çš„å±å®³",
        "risk_grade_display": "é£é™©ç­‰çº§",
        "t_value_display": "Tå€¼",
        "similar_cases_section": "ğŸ” ç›¸ä¼¼æ¡ˆä¾‹",
        "case_number": "æ¡ˆä¾‹",
        "phase2_results": "ğŸ› ï¸ ç¬¬2é˜¶æ®µ: æ”¹è¿›æªæ–½ç»“æœ",
        "improvement_failed": "æœªèƒ½ç”Ÿæˆæ”¹è¿›æªæ–½ã€‚",
        "risk_visualization": "ğŸ“Š é£é™©ç­‰çº§å˜åŒ–å¯è§†åŒ–",
        "before_improvement": "æ”¹è¿›å‰",
        "after_improvement": "æ”¹è¿›å",
        "grade_label": "ç­‰çº§",
        "download_results": "ğŸ’¾ ä¸‹è½½ç»“æœ"
    }
}

# ----------------- í˜ì´ì§€ ìŠ¤íƒ€ì¼ -----------------
st.set_page_config(page_title="Artificial Intelligence Risk Assessment", page_icon="ğŸ› ï¸", layout="wide")
st.markdown(
    """
    <style>
    .main-header{font-size:2.5rem;color:#1E88E5;text-align:center;margin-bottom:1rem}
    .sub-header{font-size:1.8rem;color:#0D47A1;margin-top:2rem;margin-bottom:1rem}
    .metric-container{background-color:#f0f2f6;border-radius:10px;padding:20px;box-shadow:2px 2px 5px rgba(0,0,0,0.1)}
    .result-box{background-color:#f8f9fa;border-radius:10px;padding:15px;margin-top:10px;margin-bottom:10px;border-left:5px solid #4CAF50}
    .phase-badge{background-color:#4CAF50;color:white;padding:5px 10px;border-radius:15px;font-size:0.8rem;margin-right:10px}
    .similar-case{background-color:#f1f8e9;border-radius:8px;padding:12px;margin-bottom:8px;border-left:4px solid #689f38}
    .language-selector{position:absolute;top:10px;right:10px;z-index:1000}
    .calculation-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:15px;margin:20px 0}
    .risk-grade-a{background-color:#ff1744;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-b{background-color:#ff9800;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-c{background-color:#ffc107;color:black;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-d{background-color:#4caf50;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    .risk-grade-e{background-color:#2196f3;color:white;padding:5px 10px;border-radius:5px;font-weight:bold}
    </style>
    """,
    unsafe_allow_html=True
)

# ----------------- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” -----------------
ss = st.session_state
for key, default in {
    "language": "Korean",
    "index": None,
    "embeddings": None,
    "retriever_pool_df": None,
    "last_assessment": None
}.items():
    if key not in ss:
        ss[key] = default

# ----------------- ì–¸ì–´ ì„ íƒ -----------------
col0, colLang = st.columns([6, 1])
with colLang:
    lang = st.selectbox(
        "ì–¸ì–´ ì„ íƒ",
        list(system_texts.keys()),
        index=list(system_texts.keys()).index(ss.language),
        label_visibility="hidden"
    )
    ss.language = lang
texts = system_texts[ss.language]

# ----------------- í—¤ë” -----------------
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# ----------------- íƒ­ êµ¬ì„± -----------------
tabs = st.tabs([texts["tab_overview"], f"{texts['tab_phase1']} & {texts['tab_phase2']}"])

# -----------------------------------------------------------------------------  
# --------------------------- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ -------------------------------------  
# -----------------------------------------------------------------------------  

def determine_grade(value: int):
    """ìœ„í—˜ë„ ë“±ê¸‰ ë¶„ë¥˜"""
    if 16 <= value <= 25:
        return 'A'
    if 10 <= value <= 15:
        return 'B'
    if 5 <= value <= 9:
        return 'C'
    if 3 <= value <= 4:
        return 'D'
    if 1 <= value <= 2:
        return 'E'
    return 'Unknown' if ss.language != 'Korean' else 'ì•Œ ìˆ˜ ì—†ìŒ'

def get_grade_color(grade):
    """ìœ„í—˜ë“±ê¸‰ë³„ ìƒ‰ìƒ ë°˜í™˜"""
    colors = {
        'A': '#ff1744',  # ë¹¨ê°„ìƒ‰
        'B': '#ff9800',  # ì£¼í™©ìƒ‰  
        'C': '#ffc107',  # ë…¸ë€ìƒ‰
        'D': '#4caf50',  # ì´ˆë¡ìƒ‰
        'E': '#2196f3',  # íŒŒë€ìƒ‰
    }
    return colors.get(grade, '#808080')

def compute_rrr(original_t, improved_t):
    """ìœ„í—˜ ê°ì†Œìœ¨ ê³„ì‚°"""
    if original_t == 0:
        return 0.0
    return ((original_t - improved_t) / original_t) * 100

def translate_similar_cases(sim_docs, target_language, api_key):
    """ìœ ì‚¬ ì‚¬ë¡€ë“¤ì„ ëª©í‘œ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ê³  ë°ì´í„°í”„ë ˆì„ ë°˜í™˜"""
    if target_language == "Korean" or not api_key:
        return sim_docs.copy()

    translated_docs = sim_docs.copy()
    for idx, row in sim_docs.iterrows():
        try:
            # ì‘ì—…í™œë™ ë²ˆì—­
            activity_prompt = f"Translate the following construction work activity to {target_language}. Only provide the translation:\n\n{row['ì‘ì—…í™œë™ ë° ë‚´ìš©']}"
            translated_activity = generate_with_gpt(activity_prompt, api_key, target_language, max_retries=1)

            # ìœ í•´ìœ„í—˜ìš”ì¸ ë²ˆì—­  
            hazard_prompt = f"Translate the following construction hazard to {target_language}. Only provide the translation:\n\n{row['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}"
            translated_hazard = generate_with_gpt(hazard_prompt, api_key, target_language, max_retries=1)

            # ê°œì„ ëŒ€ì±… ë²ˆì—­
            if 'ê°œì„ ëŒ€ì±…' in row and pd.notna(row['ê°œì„ ëŒ€ì±…']):
                improvement_prompt = f"Translate the following safety improvement measures to {target_language}. Keep the numbered format. Only provide the translation:\n\n{row['ê°œì„ ëŒ€ì±…']}"
                translated_improvement = generate_with_gpt(improvement_prompt, api_key, target_language, max_retries=1)
                if translated_improvement:
                    translated_docs.at[idx, 'ê°œì„ ëŒ€ì±…'] = translated_improvement

            # ë²ˆì—­ ê²°ê³¼ ì ìš© (ë¹ˆ ê²°ê³¼ë©´ ì›ë³¸ ìœ ì§€)
            if translated_activity:
                translated_docs.at[idx, 'ì‘ì—…í™œë™ ë° ë‚´ìš©'] = translated_activity
            if translated_hazard:
                translated_docs.at[idx, 'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥'] = translated_hazard

        except Exception:
            # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ì›ë³¸ ìœ ì§€
            continue

    return translated_docs

def _extract_improvement_info(row):
    """
    ìœ ì‚¬ ì‚¬ë¡€ í•œ ê±´ì—ì„œ - ê°œì„ ëŒ€ì±… / ê°œì„  í›„ ë¹ˆë„Â·ê°•ë„Â·T ê°’ì„ ì¶”ì¶œ
    """
    plan_cols = [c for c in row.index if re.search(r'ê°œì„ ëŒ€ì±…|Improvement|æ”¹è¿›', c, re.I)]
    plan = row[plan_cols[0]] if plan_cols else ""

    cand_sets = [
        ('ê°œì„  í›„ ë¹ˆë„', 'ê°œì„  í›„ ê°•ë„', 'ê°œì„  í›„ T'),
        ('ê°œì„ ë¹ˆë„', 'ê°œì„ ê°•ë„', 'ê°œì„ T'),
        ('improved_frequency', 'improved_intensity', 'improved_T'),
        ('æ”¹è¿›åé¢‘ç‡', 'æ”¹è¿›åå¼ºåº¦', 'æ”¹è¿›åTê°’'),
    ]
    imp_f, imp_i, imp_t = None, None, None
    for f, i, t in cand_sets:
        if f in row and i in row and t in row:
            imp_f, imp_i, imp_t = int(row[f]), int(row[i]), int(row[t])
            break

    if imp_f is None:
        orig_f, orig_i = int(row['ë¹ˆë„']), int(row['ê°•ë„'])
        imp_f = max(1, orig_f - 1)
        imp_i = max(1, orig_i - 1)
        imp_t = imp_f * imp_i

    return plan, imp_f, imp_i, imp_t

@st.cache_data(show_spinner=False)
def load_data(selected_dataset_name: str):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    try:
        # ì–¸ì–´ë³„ ë°ì´í„°ì…‹ëª…ì„ í•œêµ­ì–´ íŒŒì¼ëª…ìœ¼ë¡œ ë§¤í•‘
        dataset_mapping = {
            "ê±´ì¶•": "ê±´ì¶•", "í† ëª©": "í† ëª©", "í”ŒëœíŠ¸": "í”ŒëœíŠ¸",
            "Architecture": "ê±´ì¶•", "Civil": "í† ëª©", "Plant": "í”ŒëœíŠ¸",
            "å»ºç­‘": "ê±´ì¶•", "åœŸæœ¨": "í† ëª©", "å·¥å‚": "í”ŒëœíŠ¸"
        }

        actual_filename = dataset_mapping.get(selected_dataset_name, selected_dataset_name)

        if os.path.exists(f"{actual_filename}.xlsx"):
            try:
                df = pd.read_excel(f"{actual_filename}.xlsx", engine='openpyxl')
            except Exception as e1:
                try:
                    df = pd.read_excel(f"{actual_filename}.xlsx", engine='xlrd')
                except Exception as e2:
                    try:
                        # .xls íŒŒì¼ë„ ì‹œë„
                        if os.path.exists(f"{actual_filename}.xls"):
                            df = pd.read_excel(f"{actual_filename}.xls", engine='xlrd')
                        else:
                            st.warning(f"Excel íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {actual_filename}.xlsx")
                            st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            return create_sample_data()
                    except Exception as e3:
                        st.warning(f"Excel íŒŒì¼ í˜•ì‹ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤: {e1}")
                        st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        return create_sample_data()
        elif os.path.exists(f"{actual_filename}.xls"):
            try:
                df = pd.read_excel(f"{actual_filename}.xls", engine='xlrd')
            except Exception as e:
                st.warning(f"Excel íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return create_sample_data()
        else:
            st.info(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {actual_filename}.xlsx ë˜ëŠ” {actual_filename}.xls")
            st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return create_sample_data()

        if "ì‚­ì œ Del" in df.columns:
            df.drop(["ì‚­ì œ Del"], axis=1, inplace=True)

        df = df.dropna(how='all')

        column_mapping = {
            "ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect": "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ\nCorrective Action": "ê°œì„ ëŒ€ì±…"
        }
        df.rename(columns=column_mapping, inplace=True)

        numeric_columns = ['ë¹ˆë„', 'ê°•ë„']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        if 'ë¹ˆë„' not in df.columns:
            df['ë¹ˆë„'] = 3
        if 'ê°•ë„' not in df.columns:
            df['ê°•ë„'] = 3

        df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
        df["ë“±ê¸‰"] = df["T"].apply(determine_grade)

        if "ê°œì„ ëŒ€ì±…" not in df.columns:
            alt_cols = [c for c in df.columns if "ê°œì„ " in c or "ëŒ€ì±…" in c or "Corrective" in c]
            if alt_cols:
                df.rename(columns={alt_cols[0]: "ê°œì„ ëŒ€ì±…"}, inplace=True)
            else:
                df["ê°œì„ ëŒ€ì±…"] = "ì•ˆì „ êµìœ¡ ì‹¤ì‹œ ë° ë³´í˜¸êµ¬ ì°©ìš©"

        required_cols = [
            "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥",
            "ë¹ˆë„",
            "ê°•ë„",
            "T",
            "ë“±ê¸‰",
            "ê°œì„ ëŒ€ì±…"
        ]
        final_cols = [col for col in required_cols if col in df.columns]
        df = df[final_cols]

        df = df.fillna({
            "ì‘ì—…í™œë™ ë° ë‚´ìš©": "ì¼ë°˜ ì‘ì—…",
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": "ì¼ë°˜ì  ìœ„í—˜",
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": "ë¶€ìƒ",
            "ê°œì„ ëŒ€ì±…": "ì•ˆì „ ì¡°ì¹˜ ìˆ˜í–‰"
        })

        return df

    except Exception as e:
        st.warning(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return create_sample_data()

def create_sample_data():
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    data = {
        "ì‘ì—…í™œë™ ë° ë‚´ìš©": [
            "ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "ì½˜í¬ë¦¬íŠ¸/CMU ë¸”ë¡ ì„¤ì¹˜ ì‘ì—…",
            "êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…",
            "ê³ ì†Œ ì‘ì—…ëŒ€ë¥¼ ì´ìš©í•œ ì™¸ë²½ ì‘ì—…",
            "ìš©ì ‘ ì‘ì—…"
        ],
        "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": [
            "ë‹¤ì¤‘ ì¸ì–‘ìœ¼ë¡œ ì¸í•œ ì ì¬ë¬¼ ë‚™í•˜",
            "ë¶ˆì¶©ë¶„í•œ ì‘ì—… ë°œíŒìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°",
            "ì•ˆì „ëŒ€ ë¯¸ì°©ìš©ìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "ìš©ì ‘ í„ ë° í™”ì¬ ìœ„í—˜"
        ],
        "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": [
            "íƒ€ë°•ìƒ",
            "ê³¨ì ˆ",
            "ë§¤ëª°",
            "ì¶”ë½ì‚¬",
            "í™”ìƒ"
        ],
        "ë¹ˆë„": [3, 3, 2, 4, 2],
        "ê°•ë„": [5, 4, 5, 5, 3],
        "ê°œì„ ëŒ€ì±…": [
            "1) ë‹¤ìˆ˜ì˜ ì² ê³¨ì¬ë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬ 2) ì¹˜ìˆ˜, ì¤‘ëŸ‰, í˜•ìƒì´ ë‹¤ë¥¸ ì¬ë£Œë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬",
            "1) ë¹„ê³„ëŒ€ ëˆ„ë½ëœ ëª©íŒ ì„¤ì¹˜ 2) ì•ˆì „ëŒ€ ë¶€ì°©ì„¤ë¹„ ì„¤ì¹˜ ë° ì‚¬ìš© 3) ë¹„ê³„ ë³€ê²½ ì‹œ íƒ€ê³µì¢… ì™¸ ì‘ì—…ì ì‘ì—… ê¸ˆì§€",
            "1) ì ì ˆí•œ ì‚¬ë©´ ê¸°ìš¸ê¸° ìœ ì§€ 2) êµ´ì°©ë©´ ë³´ê°• 3) ì •ê¸°ì  ì§€ë°˜ ìƒíƒœ ì ê²€",
            "1) ì•ˆì „ëŒ€ ì°©ìš© ì˜ë¬´í™” 2) ì‘ì—… ì „ ì•ˆì „êµìœ¡ ì‹¤ì‹œ 3) ì¶”ë½ë°©ì§€ë§ ì„¤ì¹˜",
            "1) ì ì ˆí•œ í™˜ê¸°ì‹œì„¤ ì„¤ì¹˜ 2) í™”ì¬ ì˜ˆë°© ì¡°ì¹˜ 3) ë³´í˜¸êµ¬ ì°©ìš©"
        ]
    }
    df = pd.DataFrame(data)
    df["T"] = df["ë¹ˆë„"] * df["ê°•ë„"]
    df["ë“±ê¸‰"] = df["T"].apply(determine_grade)
    return df

def embed_texts_with_openai(texts, api_key, model="text-embedding-3-large"):
    """
    OpenAI ê³µì‹ APIë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    (ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•˜ë„ë¡ ìˆ˜ì •)
    """
    if not api_key:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []

    client = OpenAI(api_key=api_key)
    embeddings = []
    batch_size = 10

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i : i + batch_size]
        processed_texts = [str(txt).replace("\n", " ").strip() for txt in batch_texts]

        try:
            resp = client.embeddings.create(
                model=model,
                input=processed_texts
            )
            for item in resp.data:
                embeddings.append(item.embedding)
        except Exception as e:
            st.error(f"ì„ë² ë”© í˜¸ì¶œ ì‹¤íŒ¨ (ë°°ì¹˜ {i}): {e}")
            for _ in batch_texts:
                embeddings.append([0.0] * 1536)

    return embeddings

def generate_with_gpt(prompt, api_key, language, model="gpt-4o", max_retries=3):
    """OpenAI ê³µì‹ APIë¥¼ ì´ìš©í•œ GPT ìƒì„± í•¨ìˆ˜"""
    if not api_key:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return ""

    client = OpenAI(api_key=api_key)
    sys_prompts = {
        "Korean": "ë‹¹ì‹ ì€ ê±´ì„¤ í˜„ì¥ ìœ„í—˜ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        "English": "You are a construction site risk assessment expert. Provide accurate and practical responses in English.",
        "Chinese": "æ‚¨æ˜¯å»ºç­‘å·¥åœ°é£é™©è¯„ä¼°ä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡æä¾›å‡†ç¡®å®ç”¨çš„å›ç­”ã€‚"
    }

    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_prompts.get(language, sys_prompts["Korean"])},
                    {"role": "user",   "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500,
                top_p=0.9
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            if attempt == max_retries - 1:
                st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                return ""
            else:
                st.warning(f"GPT í˜¸ì¶œ ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                continue

def construct_prompt_phase1_hazard(retrieved_docs, activity_text, language="Korean"):
    """ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ í”„ë¡¬í”„íŠ¸"""
    if retrieved_docs is None or len(retrieved_docs) == 0:
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        basic_prompts = {
            "Korean": f"ë‹¤ìŒ ì‘ì—…í™œë™ì˜ ì£¼ìš” ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”:\n\nì‘ì—…í™œë™: {activity_text}\n\nì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸:",
            "English": f"Please predict the main hazards for the following work activity:\n\nWork Activity: {activity_text}\n\nPredicted Hazard:",
            "Chinese": f"è¯·é¢„æµ‹ä»¥ä¸‹å·¥ä½œæ´»åŠ¨çš„ä¸»è¦å±å®³:\n\nå·¥ä½œæ´»åŠ¨: {activity_text}\n\né¢„æµ‹çš„å±å®³:"
        }
        return basic_prompts.get(language, basic_prompts["Korean"])

    prompt_templates = {
        "Korean": {
            "intro": "ê±´ì„¤ í˜„ì¥ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ë“¤ì´ ìˆìŠµë‹ˆë‹¤:\n\n",
            "example_format": "ì‚¬ë¡€ {i}:\n- ì‘ì—…í™œë™: {activity}\n- ìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\n\n",
            "query_format": "ìœ„ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì‘ì—…í™œë™ì˜ ì£¼ìš” ìœ í•´ìœ„í—˜ìš”ì¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”:\n\nì‘ì—…í™œë™: {activity}\n\nì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸: "
        },
        "English": {
            "intro": "Here are examples of work activities and associated hazards at construction sites:\n\n",
            "example_format": "Case {i}:\n- Work Activity: {activity}\n- Hazard: {hazard}\n\n",
            "query_format": "Based on the above cases, please predict the main hazards for the following work activity:\n\nWork Activity: {activity}\n\nPredicted Hazard: "
        },
        "Chinese": {
            "intro": "ä»¥ä¸‹æ˜¯å»ºç­‘å·¥åœ°å·¥ä½œæ´»åŠ¨å’Œç›¸å…³å±å®³çš„ä¾‹å­:\n\n",
            "example_format": "æ¡ˆä¾‹ {i}:\n- å·¥ä½œæ´»åŠ¨: {activity}\n- å±å®³: {hazard}\n\n",
            "query_format": "æ ¹æ®ä¸Šè¿°æ¡ˆä¾‹ï¼Œè¯·é¢„æµ‹ä»¥ä¸‹å·¥ä½œæ´»åŠ¨çš„ä¸»è¦å±å®³:\n\nå·¥ä½œæ´»åŠ¨: {activity}\n\né¢„æµ‹çš„å±å®³: "
        }
    }

    template = prompt_templates.get(language, prompt_templates["Korean"])
    retrieved_examples = []

    try:
        for _, doc in retrieved_docs.iterrows():
            try:
                activity = doc["ì‘ì—…í™œë™ ë° ë‚´ìš©"]
                hazard = doc["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"]
                if pd.notna(activity) and pd.notna(hazard):
                    retrieved_examples.append((activity, hazard))
            except:
                continue
    except:
        # iterrows ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        basic_prompts = {
            "Korean": f"ë‹¤ìŒ ì‘ì—…í™œë™ì˜ ì£¼ìš” ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”:\n\nì‘ì—…í™œë™: {activity_text}\n\nì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸:",
            "English": f"Please predict the main hazards for the following work activity:\n\nWork Activity: {activity_text}\n\nPredicted Hazard:",
            "Chinese": f"è¯·é¢„æµ‹ä»¥ä¸‹å·¥ä½œæ´»åŠ¨çš„ä¸»è¦å±å®³:\n\nå·¥ä½œæ´»åŠ¨: {activity_text}\n\né¢„æµ‹çš„å±å®³:"
        }
        return basic_prompts.get(language, basic_prompts["Korean"])

    prompt = template["intro"]
    for i, (act, haz) in enumerate(retrieved_examples[:5], 1):
        prompt += template["example_format"].format(i=i, activity=act, hazard=haz)

    prompt += template["query_format"].format(activity=activity_text)
    return prompt

def construct_prompt_phase1_risk(retrieved_docs, activity_text, hazard_text, language="Korean"):
    """ìœ„í—˜ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸"""
    prompt_templates = {
        "Korean": {
            "intro": "ê±´ì„¤ í˜„ì¥ ìœ„í—˜ì„± í‰ê°€ ê¸°ì¤€:\n- ë¹ˆë„(1-5): 1=ë§¤ìš°ë“œë¬¼ê²Œ, 2=ë“œë¬¼ê²Œ, 3=ê°€ë”, 4=ìì£¼, 5=ë§¤ìš°ìì£¼\n- ê°•ë„(1-5): 1=ê²½ë¯¸í•œë¶€ìƒ, 2=ê°€ë²¼ìš´ë¶€ìƒ, 3=ì¤‘ê°„ë¶€ìƒ, 4=ì‹¬ê°í•œë¶€ìƒ, 5=ì‚¬ë§\n- Tê°’ = ë¹ˆë„ Ã— ê°•ë„\n\nì°¸ê³  ì‚¬ë¡€ë“¤:\n\n",
            "example_format": "ì‚¬ë¡€ {i}:\nì…ë ¥: {input}\ní‰ê°€: ë¹ˆë„={freq}, ê°•ë„={intensity}, Tê°’={t_value}\n\n",
            "query_format": "ìœ„ ê¸°ì¤€ê³¼ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒì„ í‰ê°€í•´ì£¼ì„¸ìš”:\n\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”:\n{json_format}"
        },
        "English": {
            "intro": "Construction site risk assessment criteria:\n- Frequency(1-5): 1=Very Rare, 2=Rare, 3=Occasional, 4=Frequent, 5=Very Frequent\n- Intensity(1-5): 1=Minor Injury, 2=Light Injury, 3=Moderate Injury, 4=Serious Injury, 5=Fatality\n- T-value = Frequency Ã— Intensity\n\nReference cases:\n\n",
            "example_format": "Case {i}:\nInput: {input}\nAssessment: Frequency={freq}, Intensity={intensity}, T-value={t_value}\n\n",
            "query_format": "Based on the above criteria and cases, please assess the following:\n\nWork Activity: {activity}\nHazard: {hazard}\n\nRespond in the following JSON format:\n{json_format}"
        },
        "Chinese": {
            "intro": "å»ºç­‘å·¥åœ°é£é™©è¯„ä¼°æ ‡å‡†:\n- é¢‘ç‡(1-5): 1=éå¸¸ç½•è§, 2=ç½•è§, 3=å¶å°”, 4=é¢‘ç¹, 5=éå¸¸é¢‘ç¹\n- å¼ºåº¦(1-5): 1=è½»å¾®ä¼¤å®³, 2=è½»ä¼¤, 3=ä¸­åº¦ä¼¤å®³, 4=ä¸¥é‡ä¼¤å®³, 5=æ­»äº¡\n- Tå€¼ = é¢‘ç‡ Ã— å¼ºåº¦\n\nå‚è€ƒæ¡ˆä¾‹:\n\n",
            "example_format": "æ¡ˆä¾‹ {i}:\nè¾“å…¥: {input}\nè¯„ä¼°: é¢‘ç‡={freq}, å¼ºåº¦={intensity}, Tå€¼={t_value}\n\n",
            "query_format": "æ ¹æ®ä¸Šè¿°æ ‡å‡†å’Œæ¡ˆä¾‹ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹å†…å®¹:\n\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: {hazard}\n\nè¯·ä»¥ä»¥ä¸‹JSONæ ¼å¼å›ç­”:\n{json_format}"
        }
    }

    json_formats = {
        "Korean": '{"ë¹ˆë„": ìˆ«ì, "ê°•ë„": ìˆ«ì, "T": ìˆ«ì}',
        "English": '{"frequency": number, "intensity": number, "T": number}',
        "Chinese": '{"é¢‘ç‡": æ•°å­—, "å¼ºåº¦": æ•°å­—, "T": æ•°å­—}'
    }

    template = prompt_templates.get(language, prompt_templates["Korean"])
    json_format = json_formats.get(language, json_formats["Korean"])

    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            example_input = f"{doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']} - {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}"
            frequency = int(doc["ë¹ˆë„"])
            intensity = int(doc["ê°•ë„"])
            T_value = frequency * intensity
            retrieved_examples.append((example_input, frequency, intensity, T_value))
        except:
            continue

    prompt = template["intro"]
    for i, (inp, freq, intensity, t_val) in enumerate(retrieved_examples[:3], 1):
        prompt += template["example_format"].format(
            i=i, input=inp, freq=freq, intensity=intensity, t_value=t_val
        )

    prompt += template["query_format"].format(
        activity=activity_text, hazard=hazard_text, json_format=json_format
    )
    return prompt

def parse_gpt_output_phase1(gpt_output, language="Korean"):
    """GPT ì¶œë ¥ íŒŒì‹± (Phase 1)"""
    json_patterns = {
        "Korean": r'\{"ë¹ˆë„":\s*([1-5]),\s*"ê°•ë„":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "English": r'\{"frequency":\s*([1-5]),\s*"intensity":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "Chinese": r'\{"é¢‘ç‡":\s*([1-5]),\s*"å¼ºåº¦":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    }

    pattern = json_patterns.get(language, json_patterns["Korean"])
    match = re.search(pattern, gpt_output)
    if match:
        pred_frequency = int(match.group(1))
        pred_intensity = int(match.group(2))
        pred_T = int(match.group(3))
        return pred_frequency, pred_intensity, pred_T

    for lang, pattern in json_patterns.items():
        if lang != language:
            match = re.search(pattern, gpt_output)
            if match:
                pred_frequency = int(match.group(1))
                pred_intensity = int(match.group(2))
                pred_T = int(match.group(3))
                return pred_frequency, pred_intensity, pred_T

    numbers = re.findall(r'\b([1-5])\b', gpt_output)
    if len(numbers) >= 2:
        freq, intensity = int(numbers[0]), int(numbers[1])
        return freq, intensity, freq * intensity

    return None

def construct_prompt_phase2(retrieved_docs, activity_text, hazard_text, freq, intensity, T, target_language="Korean"):
    """ê°œì„ ëŒ€ì±… ìƒì„± í”„ë¡¬í”„íŠ¸"""
    example_section = ""
    examples_added = 0

    field_names = {
        "Korean": {
            "improvement_fields": ['ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ', 'ê°œì„ ëŒ€ì±…', 'ê°œì„ ë°©ì•ˆ'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
        },
        "English": {
            "improvement_fields": ['Improvement Measures', 'Improvement Plan', 'Countermeasures'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
        },
        "Chinese": {
            "improvement_fields": ['æ”¹è¿›æªæ–½', 'æ”¹è¿›è®¡åˆ’', 'å¯¹ç­–'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
            "freq": "ë¹ˆë„",
            "intensity": "ê°•ë„",
        }
    }

    fields = field_names.get(target_language, field_names["Korean"])

    for _, row in retrieved_docs.iterrows():
        try:
            improvement_plan = ""
            for field in fields["improvement_fields"]:
                if field in row and pd.notna(row[field]):
                    improvement_plan = row[field]
                    break

            if not improvement_plan:
                continue

            original_freq = int(row[fields["freq"]]) if fields["freq"] in row else 3
            original_intensity = int(row[fields["intensity"]]) if fields["intensity"] in row else 3
            original_T = original_freq * original_intensity

            improved_freq = max(1, original_freq - 1)
            improved_intensity = max(1, original_intensity - 1)
            improved_T = improved_freq * improved_intensity

            if target_language == "Korean":
                example_section += f"""
ì˜ˆì‹œ {examples_added + 1}:
ì…ë ¥ ì‘ì—…í™œë™: {row[fields['activity']]}
ì…ë ¥ ìœ í•´ìœ„í—˜ìš”ì¸: {row[fields['hazard']]}
ì…ë ¥ ì›ë˜ ë¹ˆë„: {original_freq}
ì…ë ¥ ì›ë˜ ê°•ë„: {original_intensity}
ì…ë ¥ ì›ë˜ Tê°’: {original_T}
ì¶œë ¥ (ê°œì„ ê³„íš ë° ìœ„í—˜ê°ì†Œ) JSON í˜•ì‹:
{{
  "ê°œì„ ëŒ€ì±…": "{improvement_plan}",
  "ê°œì„  í›„ ë¹ˆë„": {improved_freq},
  "ê°œì„  í›„ ê°•ë„": {improved_intensity},
  "ê°œì„  í›„ T": {improved_T},
  "T ê°ì†Œìœ¨": {compute_rrr(original_T, improved_T):.2f}
}}
"""
            elif target_language == "English":
                example_section += f"""
Example {examples_added + 1}:
Input Work Activity: {row[fields['activity']]}
Input Hazard: {row[fields['hazard']]}
Input Original Frequency: {original_freq}
Input Original Intensity: {original_intensity}
Input Original T-value: {original_T}
Output (Improvement Plan and Risk Reduction) JSON format:
{{
  "improvement_plan": "{improvement_plan}",
  "improved_frequency": {improved_freq},
  "improved_intensity": {improved_intensity},
  "improved_T": {improved_T},
  "reduction_rate": {compute_rrr(original_T, improved_T):.2f}
}}
"""
            else:  # Chinese
                example_section += f"""
ç¤ºä¾‹ {examples_added + 1}:
è¾“å…¥å·¥ä½œæ´»åŠ¨: {row[fields['activity']]}
è¾“å…¥å±å®³: {row[fields['hazard']]}
è¾“å…¥åŸé¢‘ç‡: {original_freq}
è¾“å…¥åŸå¼ºåº¦: {original_intensity}
è¾“å…¥åŸTå€¼: {original_T}
è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) JSONæ ¼å¼:
{{
  "æ”¹è¿›æªæ–½": "{improvement_plan}",
  "æ”¹è¿›åé¢‘ç‡": {improved_freq},
  "æ”¹è¿›åå¼ºåº¦": {improved_intensity},
  "æ”¹è¿›åTå€¼": {improved_T},
  "Tå€¼é™ä½ç‡": {compute_rrr(original_T, improved_T):.2f}
}}
"""
            examples_added += 1
            if examples_added >= 3:
                break
        except Exception:
            continue

    if examples_added == 0:
        if target_language == "Korean":
            example_section = """
ì˜ˆì‹œ 1:
ì…ë ¥ ì‘ì—…í™œë™: êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…
ì…ë ¥ ìœ í•´ìœ„í—˜ìš”ì¸: êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°
ì…ë ¥ ì›ë˜ ë¹ˆë„: 3
ì…ë ¥ ì›ë˜ ê°•ë„: 4
ì…ë ¥ ì›ë˜ Tê°’: 12
ì¶œë ¥ (ê°œì„ ê³„íš ë° ìœ„í—˜ê°ì†Œ) JSON í˜•ì‹:
{
  "ê°œì„ ëŒ€ì±…": "1) í† ì–‘ ë¶„ë¥˜ì— ë”°ë¥¸ ì ì ˆí•œ ê²½ì‚¬ ìœ ì§€  
2) êµ´ì°© ë²½ë©´ ë³´ê°•  
3) ì •ê¸°ì ì¸ ì§€ë°˜ ìƒíƒœ ê²€ì‚¬ ì‹¤ì‹œ",
  "ê°œì„  í›„ ë¹ˆë„": 1,
  "ê°œì„  í›„ ê°•ë„": 2,
  "ê°œì„  í›„ T": 2,
  "T ê°ì†Œìœ¨": 83.33
}
"""
        elif target_language == "English":
            example_section = """
Example 1:
Input Work Activity: Excavation and backfilling  
Input Hazard: Collapse of excavation wall  
Input Original Frequency: 3  
Input Original Intensity: 4  
Input Original T-value: 12  
Output (Improvement Plan and Risk Reduction) JSON format:
{
  "improvement_plan": "1) Maintain proper slope according to soil classification  
2) Reinforce excavation walls  
3) Conduct regular ground condition inspections",
  "improved_frequency": 1,
  "improved_intensity": 2,
  "improved_T": 2,
  "reduction_rate": 83.33
}
"""
        else:  # Chinese
            example_section = """
ç¤ºä¾‹ 1:
è¾“å…¥å·¥ä½œæ´»åŠ¨: æŒ–æ˜å’Œå›å¡«ä½œä¸š  
è¾“å…¥å±å®³: æŒ–æ˜å¢™å£å€’å¡Œ  
è¾“å…¥åŸé¢‘ç‡: 3  
è¾“å…¥åŸå¼ºåº¦: 4  
è¾“å…¥åŸTå€¼: 12  
è¾“å‡º (æ”¹è¿›è®¡åˆ’å’Œé£é™©é™ä½) JSONæ ¼å¼:
{
  "æ”¹è¿›æªæ–½": "1) æ ¹æ®åœŸå£¤åˆ†ç±»ç»´æŒé€‚å½“çš„æ–œå¡  
2) åŠ å›ºæŒ–æ˜å¢™å£  
3) å®šæœŸè¿›è¡Œåœ°é¢çŠ¶å†µæ£€æŸ¥",
  "æ”¹è¿›åé¢‘ç‡": 1,
  "æ”¹è¿›åå¼ºåº¦": 2,
  "æ”¹è¿›åTå€¼": 2,
  "Tå€¼é™ä½ç‡": 83.33
}
"""

    json_keys = {
        "Korean": {
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„  í›„ ë¹ˆë„",
            "improved_intensity": "ê°œì„  í›„ ê°•ë„",
            "improved_t": "ê°œì„  í›„ T",
            "reduction_rate": "T ê°ì†Œìœ¨"
        },
        "English": {
            "improvement": "improvement_plan",
            "improved_freq": "improved_frequency",
            "improved_intensity": "improved_intensity",
            "improved_t": "improved_T",
            "reduction_rate": "reduction_rate"
        },
        "Chinese": {
            "improvement": "æ”¹è¿›æªæ–½",
            "improved_freq": "æ”¹è¿›åé¢‘ç‡",
            "improved_intensity": "æ”¹è¿›åå¼ºåº¦",
            "improved_t": "æ”¹è¿›åTå€¼",
            "reduction_rate": "Tå€¼é™ä½ç‡"
        }
    }

    instructions = {
        "Korean": {
            "new_input": "ì´ì œ ìƒˆë¡œìš´ ì…ë ¥ì…ë‹ˆë‹¤:",
            "output_format": "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ê°œì„ ëŒ€ì±…ì„ ì œê³µí•˜ì„¸ìš”:",
            "requirements": "ê°œì„ ëŒ€ì±…ì€ ì‹¤ì œ í˜„ì¥ì—ì„œ ì ìš© ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ë°©ë²•ì„ 3ê°œ ì´ìƒ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ì œì‹œí•˜ì„¸ìš”. ê°œì„  í›„ ë¹ˆë„ì™€ ê°•ë„ëŠ” í•©ë¦¬ì ìœ¼ë¡œ ê°ì†Œëœ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.",
            "output": "ì¶œë ¥:"
        },
        "English": {
            "new_input": "Now here is a new input:",
            "output_format": "Please provide practical and specific improvement measures in the following JSON format:",
            "requirements": "Improvement measures should include at least 3 specific, field-applicable methods in a numbered list. Set improved frequency and intensity to reasonably reduced values.",
            "output": "Output:"
        },
        "Chinese": {
            "new_input": "ç°åœ¨æ˜¯æ–°çš„è¾“å…¥:",
            "output_format": "è¯·ä»¥ä»¥ä¸‹JSONæ ¼å¼æä¾›å®ç”¨ä¸”å…·ä½“çš„æ”¹è¿›æªæ–½:",
            "requirements": "æ”¹è¿›æªæ–½åº”åŒ…æ‹¬è‡³å°‘3ä¸ªå…·ä½“çš„ã€ç°åœºå¯åº”ç”¨çš„æ–¹æ³•ï¼Œä»¥ç¼–å·åˆ—è¡¨å½¢å¼ã€‚å°†æ”¹è¿›åçš„é¢‘ç‡å’Œå¼ºåº¦è®¾ç½®ä¸ºåˆç†é™ä½çš„å€¼ã€‚",
            "output": "è¾“å‡º:"
        }
    }

    keys = json_keys.get(target_language, json_keys["Korean"])
    instr = instructions.get(target_language, instructions["Korean"])

    prompt = f"""{example_section}
{instr['new_input']}
ì…ë ¥ ì‘ì—…í™œë™: {activity_text}
ì…ë ¥ ìœ í•´ìœ„í—˜ìš”ì¸: {hazard_text}
ì…ë ¥ ì›ë˜ ë¹ˆë„: {freq}
ì…ë ¥ ì›ë˜ ê°•ë„: {intensity}
ì…ë ¥ ì›ë˜ Tê°’: {T}

{instr['output_format']}
{{
  "{keys["improvement"]}": "ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ êµ¬ì²´ì  ê°œì„ ëŒ€ì±… ë¦¬ìŠ¤íŠ¸",
  "{keys["improved_freq"]}": (1-5 ì‚¬ì´ì˜ ì •ìˆ˜),
  "{keys["improved_intensity"]}": (1-5 ì‚¬ì´ì˜ ì •ìˆ˜),
  "{keys["improved_t"]}": (ê°œì„  í›„ ë¹ˆë„ Ã— ê°œì„  í›„ ê°•ë„),
  "{keys["reduction_rate"]}": (ìœ„í—˜ ê°ì†Œ ë°±ë¶„ìœ¨)
}}

{instr['requirements']}
{instr['output']}
"""
    return prompt

def parse_gpt_output_phase2(gpt_output, language="Korean"):
    """Phase 2 ì¶œë ¥ íŒŒì‹±"""
    try:
        json_match = re.search(r'```json\s*(.*?)\s*```', gpt_output, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_match = re.search(r'\{.*\}', gpt_output, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                json_str = gpt_output

        import json
        result = json.loads(json_str)

        key_mappings = {
            "Korean": {
                "improvement": ["ê°œì„ ëŒ€ì±…"],
                "improved_freq": ["ê°œì„  í›„ ë¹ˆë„", "ê°œì„ ë¹ˆë„"],
                "improved_intensity": ["ê°œì„  í›„ ê°•ë„", "ê°œì„ ê°•ë„"],
                "improved_t": ["ê°œì„  í›„ T", "ê°œì„ T"],
                "reduction_rate": ["T ê°ì†Œìœ¨", "ê°ì†Œìœ¨", "ìœ„í—˜ ê°ì†Œìœ¨"]
            },
            "English": {
                "improvement": ["improvement_plan", "improvement_measures"],
                "improved_freq": ["improved_frequency", "new_frequency"],
                "improved_intensity": ["improved_intensity", "new_intensity"],
                "improved_t": ["improved_T", "new_T"],
                "reduction_rate": ["reduction_rate", "risk_reduction_rate"]
            },
            "Chinese": {
                "improvement": ["æ”¹è¿›æªæ–½", "æ”¹è¿›è®¡åˆ’"],
                "improved_freq": ["æ”¹è¿›åé¢‘ç‡", "æ–°é¢‘ç‡"],
                "improved_intensity": ["æ”¹è¿›åå¼ºåº¦", "æ–°å¼ºåº¦"],
                "improved_t": ["æ”¹è¿›åTå€¼", "æ–°Tå€¼"],
                "reduction_rate": ["Tå€¼é™ä½ç‡", "é™ä½ç‡"]
            }
        }

        mappings = key_mappings.get(language, key_mappings["Korean"])
        mapped_result = {}

        for result_key, possible_keys in mappings.items():
            for key in possible_keys:
                if key in result:
                    mapped_result[result_key] = result[key]
                    break

        if "improved_freq" not in mapped_result:
            mapped_result["improved_freq"] = 1
        if "improved_intensity" not in mapped_result:
            mapped_result["improved_intensity"] = 1
        if "improved_t" not in mapped_result:
            mapped_result["improved_t"] = mapped_result["improved_freq"] * mapped_result["improved_intensity"]
        if "reduction_rate" not in mapped_result:
            mapped_result["reduction_rate"] = 50.0

        return mapped_result

    except Exception as e:
        st.error(f"ê°œì„ ëŒ€ì±… íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.write("ì›ë³¸ GPT ì‘ë‹µ:", gpt_output)
        return {
            "improvement": "ì•ˆì „ êµìœ¡ ì‹¤ì‹œ ë° ë³´í˜¸êµ¬ ì°©ìš© ì˜ë¬´í™”",
            "improved_freq": 1,
            "improved_intensity": 1,
            "improved_t": 1,
            "reduction_rate": 50.0
        }

# -----------------------------------------------------------------------------  
# --------------------------- Overview íƒ­ -------------------------------------  
# -----------------------------------------------------------------------------  
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)

    col_overview, col_features = st.columns([3, 2])
    with col_overview:
        st.markdown(f"<div class='info-text'>{texts['overview_text']}</div>", unsafe_allow_html=True)
        col_metric1, col_metric2, col_metric3 = st.columns(3)
        with col_metric1:
            st.metric(texts["supported_languages"], texts["languages_count"], texts["languages_detail"])
        with col_metric2:
            st.metric(texts["assessment_phases"], texts["phases_count"], texts["phases_detail"])
        with col_metric3:
            st.metric(texts["risk_grades"], texts["grades_count"], texts["grades_detail"])

    with col_features:
        st.markdown(f"**{texts['features_title']}**")
        st.markdown(texts["phase1_features"])
        st.markdown(texts["phase2_features"])

# -----------------------------------------------------------------------------  
# ---------------------- Risk Assessment íƒ­ -----------------------------------  
# -----------------------------------------------------------------------------  
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["tab_phase1"]} & {texts["tab_phase2"]}</div>', unsafe_allow_html=True)

    col_api, col_dataset = st.columns([2, 1])
    with col_api:
        api_key = st.text_input(texts["api_key_label"], type="password", key="api_key_all")
    with col_dataset:
        dataset_name = st.selectbox(
            texts["dataset_label"],
            texts["dataset_options"],
            key="dataset_all"
        )

    if ss.retriever_pool_df is None or st.button(texts["load_data_btn"], type="primary"):
        if not api_key:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["data_loading"]):
                try:
                    df = load_data(dataset_name)
                    if len(df) > 10:
                        train_df, _ = train_test_split(df, test_size=0.1, random_state=42)
                    else:
                        train_df = df.copy()

                    pool_df = train_df.copy()
                    pool_df["content"] = pool_df.apply(lambda r: " ".join(r.values.astype(str)), axis=1)

                    to_embed = pool_df["content"].tolist()
                    max_texts = min(len(to_embed), 30)

                    st.info(texts["demo_limit_info"].format(max_texts=max_texts))
                    embeds = embed_texts_with_openai(to_embed[:max_texts], api_key=api_key)

                    vecs = np.array(embeds, dtype="float32")
                    dim = vecs.shape[1]
                    index = faiss.IndexFlatL2(dim)
                    index.add(vecs)

                    ss.index = index
                    ss.embeddings = vecs
                    ss.retriever_pool_df = pool_df.iloc[:max_texts]

                    st.success(texts["data_load_success"].format(max_texts=max_texts))
                    with st.expander("ğŸ“Š ë¡œë“œëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                        st.dataframe(df.head(), use_container_width=True)
                except Exception as e:
                    st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    st.divider()
    st.markdown(f"### {texts['performing_assessment'].split('.')[0]}")

    activity = st.text_area(
        texts["activity_label"],
        placeholder={
            "Korean": "ì˜ˆ: ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "English": "e.g.: Unloading steel structural materials using forklift at temporary site storage",
            "Chinese": "ä¾‹: åœ¨ä¸´æ—¶ç°åœºä»“åº“ä½¿ç”¨å‰è½¦å¸è½½é’¢ç»“æ„ææ–™"
        }.get(ss.language, "ì˜ˆ: ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¥¼ ì´ìš©í•œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…"),
        height=100,
        key="user_activity"
    )

    col_options1, col_options2 = st.columns(2)
    with col_options1:
        include_similar_cases = st.checkbox(texts["include_similar_cases"], value=True)
    with col_options2:
        result_language = st.selectbox(
            texts["result_language"],
            ["Korean", "English", "Chinese"],
            index=["Korean", "English", "Chinese"].index(ss.language)
        )

    run_button = st.button(texts["run_assessment"], type="primary", use_container_width=True)

    if run_button:
        if not activity:
            st.warning(texts["activity_warning"])
        elif not api_key:
            st.warning(texts["api_key_warning"])
        elif ss.index is None:
            st.warning(texts["load_first_warning"])
        else:
            with st.spinner(texts["performing_assessment"]):
                try:
                    # === Phase 1: Risk Assessment ===
                    q_emb_list = embed_texts_with_openai([activity], api_key=api_key)
                    if not q_emb_list:
                        st.error(texts["parsing_error"])
                        st.stop()
                    q_emb = q_emb_list[0]

                    # ì¸ë±ìŠ¤ì™€ ë°ì´í„° ì¡´ì¬ í™•ì¸
                    if ss.index is None or ss.retriever_pool_df is None or len(ss.retriever_pool_df) == 0:
                        st.error("ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
                        st.stop()

                    # ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰: ìµœëŒ€ 10ê°œ
                    D, I = ss.index.search(
                        np.array([q_emb], dtype="float32"),
                        k=min(10, len(ss.retriever_pool_df))
                    )

                    # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
                    if I is None or len(I) == 0 or len(I[0]) == 0:
                        st.error("ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    sim_docs = ss.retriever_pool_df.iloc[I[0]]

                    # sim_docs ì•ˆì „ì„± í™•ì¸
                    if sim_docs is None or len(sim_docs) == 0:
                        st.error("ìœ ì‚¬í•œ ì‚¬ë¡€ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    # ì„ íƒëœ ì–¸ì–´ë¡œ ìœ ì‚¬ ì‚¬ë¡€ ë²ˆì—­ (í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°)
                    sim_docs_translated = translate_similar_cases(sim_docs, result_language, api_key)

                    hazard_prompt = construct_prompt_phase1_hazard(sim_docs_translated, activity, result_language)
                    hazard = generate_with_gpt(hazard_prompt, api_key, result_language)
                    if not hazard:
                        st.error(texts["parsing_error"])
                        st.stop()

                    risk_prompt = construct_prompt_phase1_risk(sim_docs_translated, activity, hazard, result_language)
                    risk_json = generate_with_gpt(risk_prompt, api_key, result_language)
                    parse_result = parse_gpt_output_phase1(risk_json, result_language)
                    if not parse_result:
                        st.error(texts["parsing_error"])
                        st.expander("GPT ì›ë¬¸ ì‘ë‹µ").write(risk_json)
                        st.stop()

                    freq, intensity, T = parse_result
                    grade = determine_grade(T)

                    # === Phase 2: Improvement Measures ===
                    improvement_prompt = construct_prompt_phase2(
                        sim_docs_translated, activity, hazard, freq, intensity, T, result_language
                    )
                    improvement_response = generate_with_gpt(improvement_prompt, api_key, result_language)
                    parsed_improvement = parse_gpt_output_phase2(improvement_response, result_language)
                    improvement_plan = parsed_improvement.get("improvement", "")
                    improved_freq = parsed_improvement.get("improved_freq", 1)
                    improved_intensity = parsed_improvement.get("improved_intensity", 1)
                    improved_T = parsed_improvement.get("improved_t", improved_freq * improved_intensity)
                    rrr = compute_rrr(T, improved_T)

                    # === Results Display ===
                    st.markdown(f"## {texts['phase1_results']}")
                    col_result1, col_result2 = st.columns([2, 1])
                    with col_result1:
                        st.markdown(f"**{texts['work_activity']}:** {activity}")
                        st.markdown(f"**{texts['predicted_hazard']}:** {hazard}")
                        result_df = pd.DataFrame({
                            texts["result_table_columns"][0]: texts["result_table_rows"],
                            texts["result_table_columns"][1]: [str(freq), str(intensity), str(T), grade]
                        })
                        st.dataframe(result_df.astype(str), use_container_width=True, hide_index=True)
                    with col_result2:
                        grade_color = get_grade_color(grade)
                        st.markdown(f"""
                        <div style="text-align:center; padding:20px; background-color:{grade_color};
                                    color:white; border-radius:10px; margin:10px 0;">
                            <h2 style="margin:0;">{texts['risk_grade_display']}</h2>
                            <h1 style="margin:10px 0; font-size:3rem;">{grade}</h1>
                            <p style="margin:0;">{texts['t_value_display']}: {T}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    similar_records = []
                    if include_similar_cases and sim_docs_translated is not None and len(sim_docs_translated) > 0:
                        st.markdown(f"### {texts['similar_cases_section']}")
                        # ìµœëŒ€ 10ê°œê¹Œì§€ ì¶œë ¥
                        for i in range(min(len(sim_docs_translated), 10)):
                            try:
                                doc = sim_docs_translated.iloc[i]
                                plan, imp_f, imp_i, imp_t = _extract_improvement_info(doc)

                                # ì–¸ì–´ë³„ ë¼ë²¨
                                activity_label = {"Korean": "ì‘ì—…í™œë™", "English": "Work Activity", "Chinese": "å·¥ä½œæ´»åŠ¨"}.get(result_language, "ì‘ì—…í™œë™")
                                hazard_label = {"Korean": "ìœ í•´ìœ„í—˜ìš”ì¸", "English": "Hazard", "Chinese": "å±å®³"}.get(result_language, "ìœ í•´ìœ„í—˜ìš”ì¸")
                                risk_label = {"Korean": "ìœ„í—˜ë„", "English": "Risk Level", "Chinese": "é£é™©ç­‰çº§"}.get(result_language, "ìœ„í—˜ë„")
                                freq_label = {"Korean": "ë¹ˆë„", "English": "Frequency", "Chinese": "é¢‘ç‡"}.get(result_language, "ë¹ˆë„")
                                intensity_label = {"Korean": "ê°•ë„", "English": "Intensity", "Chinese": "å¼ºåº¦"}.get(result_language, "ê°•ë„")
                                grade_label = {"Korean": "ë“±ê¸‰", "English": "Grade", "Chinese": "ç­‰çº§"}.get(result_language, "ë“±ê¸‰")
                                improvement_label = {"Korean": "ê°œì„ ëŒ€ì±…", "English": "Improvement Measures", "Chinese": "æ”¹è¿›æªæ–½"}.get(result_language, "ê°œì„ ëŒ€ì±…")

                                title_text = str(doc['ì‘ì—…í™œë™ ë° ë‚´ìš©'])
                                if len(title_text) > 30:
                                    title_text = title_text[:30] + "â€¦"

                                with st.expander(f"{texts['case_number']} {i+1}: {title_text}"):
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        st.write(f"**{activity_label}:** {doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']}")
                                        st.write(f"**{hazard_label}:** {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}")
                                        st.write(f"**{risk_label}:** {freq_label} {doc['ë¹ˆë„']}, {intensity_label} {doc['ê°•ë„']}, Tê°’ {doc['T']} ({grade_label} {doc['ë“±ê¸‰']})")
                                    with col2:
                                        st.write(f"**{improvement_label}:**")
                                        formatted_plan = re.sub(r"(\d\))\s*", r"\1  \n", str(plan).strip())
                                        st.markdown(formatted_plan)

                                similar_records.append({
                                    "ì‘ì—…í™œë™": doc["ì‘ì—…í™œë™ ë° ë‚´ìš©"],
                                    "ìœ í•´ìœ„í—˜ìš”ì¸": doc["ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"],
                                    "ë¹ˆë„": doc["ë¹ˆë„"],
                                    "ê°•ë„": doc["ê°•ë„"],
                                    "T": doc["T"],
                                    "ìœ„í—˜ë“±ê¸‰": doc["ë“±ê¸‰"],
                                    "ê°œì„ ëŒ€ì±…": plan
                                })
                            except Exception as e:
                                st.warning(f"ì‚¬ë¡€ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                continue

                    st.markdown(f"## {texts['phase2_results']}")
                    col_improvement1, col_improvement2 = st.columns([3, 2])
                    with col_improvement1:
                        st.markdown(f"### {texts['improvement_plan_header']}")
                        if improvement_plan:
                            formatted_plan = re.sub(r"\s*\n\s*", r"<br>", improvement_plan.strip())
                            st.markdown(formatted_plan, unsafe_allow_html=True)
                        else:
                            st.write(texts["improvement_failed"])

                    with col_improvement2:
                        st.markdown(f"### {texts['risk_improvement_header']}")
                        comparison_df = pd.DataFrame({
                            texts["comparison_columns"][0]: texts["result_table_rows"],
                            texts["comparison_columns"][1]: [str(freq), str(intensity), str(T), grade],
                            texts["comparison_columns"][2]: [str(improved_freq), str(improved_intensity), str(improved_T), determine_grade(improved_T)]
                        })
                        st.dataframe(comparison_df.astype(str), use_container_width=True, hide_index=True)
                        st.metric(
                            label=texts["risk_reduction_label"],
                            value=f"{rrr:.1f}%",
                            delta=f"-{T - improved_T} Tê°’"
                        )

                    st.markdown(f"### {texts['risk_visualization']}")
                    col_vis1, col_vis2 = st.columns(2)
                    with col_vis1:
                        st.markdown(f"**{texts['before_improvement']}**")
                        grade_color = get_grade_color(grade)
                        st.markdown(f"""
                        <div style="background-color:{grade_color}; color:white; padding:15px; 
                                    border-radius:10px; text-align:center; margin:10px 0;">
                            <h3 style="margin:0;">{texts['grade_label']} {grade}</h3>
                            <p style="margin:5px 0; font-size:1.2em;">{texts['t_value_display']}: {T}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    with col_vis2:
                        st.markdown(f"**{texts['after_improvement']}**")
                        improved_grade = determine_grade(improved_T)
                        improved_grade_color = get_grade_color(improved_grade)
                        st.markdown(f"""
                        <div style="background-color:{improved_grade_color}; color:white; padding:15px; 
                                    border-radius:10px; text-align:center; margin:10px 0;">
                            <h3 style="margin:0;">{texts['grade_label']} {improved_grade}</h3>
                            <p style="margin:5px 0; font-size:1.2em;">{texts['t_value_display']}: {improved_T}</p>
                        </div>
                        """, unsafe_allow_html=True)

                    ss.last_assessment = {
                        "activity": activity,
                        "hazard": hazard,
                        "freq": freq,
                        "intensity": intensity,
                        "T": T,
                        "grade": grade,
                        "improvement_plan": improvement_plan,
                        "improved_freq": improved_freq,
                        "improved_intensity": improved_intensity,
                        "improved_T": improved_T,
                        "rrr": rrr,
                        "similar_cases": similar_records
                    }

                    st.markdown(f"### {texts['download_results']}")
                    def create_excel_download():
                        output = io.BytesIO()
                        try:
                            with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                                workbook = writer.book
                                red_fmt = workbook.add_format({
                                    "font_color": "#FF0000",
                                    "text_wrap": True
                                })

                                # â”€â”€â”€ Phase 1 ê²°ê³¼ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                phase1_df = pd.DataFrame({
                                    "í•­ëª©": ["ì‘ì—…í™œë™", "ìœ í•´ìœ„í—˜ìš”ì¸", "ë¹ˆë„", "ê°•ë„", "Tê°’", "ìœ„í—˜ë“±ê¸‰"],
                                    "ê°’": [activity, hazard, freq, intensity, T, grade]
                                })
                                phase1_df.to_excel(writer, sheet_name="Phase1_ê²°ê³¼", index=False)
                                ws1 = writer.sheets["Phase1_ê²°ê³¼"]
                                for col_idx in range(len(phase1_df.columns)):
                                    ws1.set_column(col_idx, col_idx, 20, red_fmt)

                                # â”€â”€â”€ Phase 2 ê²°ê³¼ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                phase2_df = pd.DataFrame({
                                    "í•­ëª©": ["ê°œì„ ëŒ€ì±…", "ê°œì„  í›„ ë¹ˆë„", "ê°œì„  í›„ ê°•ë„", "ê°œì„  í›„ Tê°’", "ê°œì„  í›„ ë“±ê¸‰", "ìœ„í—˜ ê°ì†Œìœ¨"],
                                    "ê°’": [improvement_plan, improved_freq, improved_intensity, improved_T, determine_grade(improved_T), f"{rrr:.2f}%"]
                                })
                                phase2_df.to_excel(writer, sheet_name="Phase2_ê²°ê³¼", index=False)
                                ws2 = writer.sheets["Phase2_ê²°ê³¼"]
                                for col_idx in range(len(phase2_df.columns)):
                                    ws2.set_column(col_idx, col_idx, 20, red_fmt)

                                # â”€â”€â”€ ë¹„êµ ë¶„ì„ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                comparison_detail_df = pd.DataFrame({
                                    "í•­ëª©": ["ë¹ˆë„", "ê°•ë„", "Tê°’", "ìœ„í—˜ë“±ê¸‰"],
                                    "ê°œì„  ì „": [freq, intensity, T, grade],
                                    "ê°œì„  í›„": [improved_freq, improved_intensity, improved_T, determine_grade(improved_T)],
                                    "ê°œì„ ìœ¨": [
                                        f"{(freq - improved_freq) / freq * 100:.1f}%" if freq > 0 else "0%",
                                        f"{(intensity - improved_intensity) / intensity * 100:.1f}%" if intensity > 0 else "0%",
                                        f"{rrr:.1f}%",
                                        f"{grade} â†’ {determine_grade(improved_T)}"
                                    ]
                                })
                                comparison_detail_df.to_excel(writer, sheet_name="ë¹„êµë¶„ì„", index=False)
                                ws3 = writer.sheets["ë¹„êµë¶„ì„"]
                                for col_idx in range(len(comparison_detail_df.columns)):
                                    ws3.set_column(col_idx, col_idx, 20, red_fmt)

                                # â”€â”€â”€ ìœ ì‚¬ì‚¬ë¡€ ì‹œíŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                if similar_records:
                                    sim_df = pd.DataFrame(similar_records)
                                    sim_df["ê°œì„  í›„ ë¹ˆë„"] = sim_df["ë¹ˆë„"].astype(int).apply(lambda x: max(1, x - 1))
                                    sim_df["ê°œì„  í›„ ê°•ë„"] = sim_df["ê°•ë„"].astype(int).apply(lambda x: max(1, x - 1))
                                    export_df = pd.DataFrame({
                                        "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence":      sim_df["ì‘ì—…í™œë™"],
                                        "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazardous Factors": sim_df["ìœ í•´ìœ„í—˜ìš”ì¸"],
                                        "ìœ„í—˜ì„± Risk â€“ ë¹ˆë„ likelihood":     sim_df["ë¹ˆë„"],
                                        "ìœ„í—˜ì„± Risk â€“ ê°•ë„ severity":      sim_df["ê°•ë„"],
                                        "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures":    sim_df["ê°œì„ ëŒ€ì±…"],
                                        "ìœ„í—˜ì„± Risk (ê°œì„  í›„) â€“ ë¹ˆë„ likelihood": sim_df["ê°œì„  í›„ ë¹ˆë„"],
                                        "ìœ„í—˜ì„± Risk (ê°œì„  í›„) â€“ ê°•ë„ severity":  sim_df["ê°œì„  í›„ ê°•ë„"],
                                    })
                                    export_df.to_excel(writer, sheet_name="ìœ ì‚¬ì‚¬ë¡€", index=False)
                                    ws4 = writer.sheets["ìœ ì‚¬ì‚¬ë¡€"]
                                    for col_idx in range(len(export_df.columns)):
                                        ws4.set_column(col_idx, col_idx, 20, red_fmt)

                            return output.getvalue()
                        except ImportError:
                            # xlsxwriterê°€ ì—†ìœ¼ë©´ CSVë¡œ ëŒ€ì²´
                            st.warning("Excel ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
                            csv_data = pd.DataFrame({
                                "ì‘ì—…í™œë™": [activity],
                                "ìœ í•´ìœ„í—˜ìš”ì¸": [hazard],
                                "ë¹ˆë„": [freq],
                                "ê°•ë„": [intensity],
                                "Tê°’": [T],
                                "ìœ„í—˜ë“±ê¸‰": [grade],
                                "ê°œì„ ëŒ€ì±…": [improvement_plan],
                                "ê°œì„ í›„ë¹ˆë„": [improved_freq],
                                "ê°œì„ í›„ê°•ë„": [improved_intensity],
                                "ê°œì„ í›„Tê°’": [improved_T],
                                "ìœ„í—˜ê°ì†Œìœ¨": [f"{rrr:.2f}%"]
                            })
                            return csv_data.to_csv(index=False).encode('utf-8-sig')

                    excel_bytes = create_excel_download()
                    st.download_button(
                        label=texts["excel_export"],
                        data=excel_bytes,
                        file_name="risk_assessment_results.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                except Exception as e:
                    st.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
                    st.stop()

# ------------------- í‘¸í„° ------------------------
st.markdown('<hr style="margin-top: 3rem;">', unsafe_allow_html=True)

footer_col1, footer_col2, footer_col3 = st.columns([1, 1, 1])

with footer_col1:
    if os.path.exists("cau.png"):
        st.image("cau.png", width=140)

with footer_col2:
    st.markdown(
        """
        <div style="text-align: center; padding: 20px;">
            <h4>ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°</h4>
            <p>AI ê¸°ë°˜ ìœ„í—˜ì„± í‰ê°€ ì‹œìŠ¤í…œ</p>
            <p style="font-size: 0.8rem; color: #666;">
                Â© 2025 Doosan Enerbility. All rights reserved.
            </p>
        </div>
        """,
        unsafe_allow_html=True
    )

with footer_col3:
    if os.path.exists("doosan.png"):
        st.image("doosan.png", width=160)
