import streamlit as st
import pandas as pd
import numpy as np
import faiss
import openai
import re
import os
from PIL import Image
from sklearn.model_selection import train_test_split
import json
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime

# ì–¸ì–´ ì„¤ì • í…ìŠ¤íŠ¸ ì •ì˜
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_phase1": "ìœ„í—˜ì„± í‰ê°€ (Phase 1)",
        "tab_phase2": "ê°œì„ ëŒ€ì±… ìƒì„± (Phase 2)",
        "tab_history": "í‰ê°€ ì´ë ¥",
        "tab_statistics": "í†µê³„ ë¶„ì„",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": """
        LLM(Large Language Model)ì„ í™œìš©í•œ ìœ„í—˜ì„±í‰ê°€ ìë™í™” ì‹œìŠ¤í…œì€ ê±´ì„¤ í˜„ì¥ì˜ ì•ˆì „ ê´€ë¦¬ë¥¼ í˜ì‹ ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤:
        
        1. <span class="highlight">ì‘ì—… ë‚´ìš© ì…ë ¥ ì‹œ ìƒì„±í˜• AIë¥¼ í†µí•œ 'ìœ í•´ìœ„í—˜ìš”ì¸' ìë™ ì˜ˆì¸¡ ë° ìœ„í—˜ ë“±ê¸‰ ì‚°ì •</span> <span class="phase-badge">Phase 1</span>
        2. <span class="highlight">ìœ„í—˜ë„ ê°ì†Œë¥¼ ìœ„í•œ ê°œì„ ëŒ€ì±… ìë™ ìƒì„± ë° ê°ì†Œìœ¨ ì˜ˆì¸¡</span> <span class="phase-badge">Phase 2</span>
        3. AIëŠ” ê±´ì„¤í˜„ì¥ì˜ ê¸°ì¡´ ìœ„í—˜ì„±í‰ê°€ë¥¼ ê³µì •ë³„ë¡œ êµ¬ë¶„í•˜ê³ , í•´ë‹¹ ìœ í•´ìœ„í—˜ìš”ì¸ì„ í•™ìŠµ
        4. ìë™ ìƒì„± ê¸°ìˆ  ê°œë°œ ì™„ë£Œ í›„ ìœ„í—˜ë„ ê¸°ë°˜ ì‚¬ê³ ìœ„í—˜ì„± ë¶„ì„ ë° ê°œì„ ëŒ€ì±… ìƒì„±
        
        ì´ ì‹œìŠ¤í…œì€ PIMS ë° ì•ˆì „ì§€í‚´ì´ ë“± EHS í”Œë«í¼ì— AI ê¸°ìˆ  íƒ‘ì¬ë¥¼ í†µí•´ í†µí•© ì‚¬ê³  ì˜ˆì¸¡ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ë°œì „ ì˜ˆì •ì…ë‹ˆë‹¤.
        """,
        "process_title": "AI ìœ„í—˜ì„±í‰ê°€ í”„ë¡œì„¸ìŠ¤",
        "process_steps": ["ì‘ì—…ë‚´ìš© ì…ë ¥", "AI ìœ„í—˜ë¶„ì„", "ìœ í•´ìš”ì¸ ì˜ˆì¸¡", "ìœ„í—˜ë“±ê¸‰ ì‚°ì •", "ê°œì„ ëŒ€ì±… ìë™ìƒì„±", "ì•ˆì „ì¡°ì¹˜ ì ìš©"],
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§• ë° êµ¬ì„±ìš”ì†Œ",
        "phase1_features": """
        #### Phase 1: ìœ„í—˜ì„± í‰ê°€ ìë™í™”
        - ê³µì •ë³„ ì‘ì—…í™œë™ì— ë”°ë¥¸ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° í•™ìŠµ
        - ì‘ì—…í™œë™ ì…ë ¥ ì‹œ ìœ í•´ìœ„í—˜ìš”ì¸ ìë™ ì˜ˆì¸¡
        - ìœ ì‚¬ ìœ„í—˜ìš”ì¸ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ
        - ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ê¸°ë°˜ ìœ„í—˜ë„(ë¹ˆë„, ê°•ë„, T) ì¸¡ì •
        - Excel ê¸°ë°˜ ê³µì •ë³„ ìœ„í—˜ì„±í‰ê°€ ë°ì´í„° ìë™ ë¶„ì„
        - ìœ„í—˜ë“±ê¸‰(A-E) ìë™ ì‚°ì •
        """,
        "phase2_features": """
        #### Phase 2: ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ìœ„í—˜ìš”ì†Œë³„ ë§ì¶¤í˜• ê°œì„ ëŒ€ì±… ìë™ ìƒì„±
        - ë‹¤êµ­ì–´(í•œ/ì˜/ì¤‘) ê°œì„ ëŒ€ì±… ìƒì„± ì§€ì›
        - ê°œì„  ì „í›„ ìœ„í—˜ë„(T) ìë™ ë¹„êµ ë¶„ì„
        - ìœ„í—˜ ê°ì†Œìœ¨(RRR) ì •ëŸ‰ì  ì‚°ì¶œ
        - ê³µì¢…/ê³µì •ë³„ ìµœì  ê°œì„ ëŒ€ì±… ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        """,
        "phase1_header": "ìœ„í—˜ì„± í‰ê°€ ìë™í™” (Phase 1)",
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ",
        "load_data_label": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "ê³„ì†í•˜ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "í˜„ì¬ {total_rows}ê°œì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! (ì´ {total_rows}ê°œ í•­ëª© ì²˜ë¦¬)",
        "hazard_prediction_header": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡",
        "load_first_warning": "ë¨¼ì € [ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.",
        "activity_label": "ì‘ì—…í™œë™:",
        "predict_hazard_btn": "ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡í•˜ê¸°",
        "activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "predicting_hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” ì¤‘...",
        "similar_cases_header": "ìœ ì‚¬í•œ ì‚¬ë¡€",
        "similar_case_text": """
        <div class="similar-case">
            <strong>ì‚¬ë¡€ {i}</strong><br>
            <strong>ì‘ì—…í™œë™:</strong> {activity}<br>
            <strong>ìœ í•´ìœ„í—˜ìš”ì¸:</strong> {hazard}<br>
            <strong>ìœ„í—˜ë„:</strong> ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})
        </div>
        """,
        "prediction_result_header": "ì˜ˆì¸¡ ê²°ê³¼",
        "activity_result": "ì‘ì—…í™œë™: {activity}",
        "hazard_result": "ì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸: {hazard}",
        "result_table_columns": ["í•­ëª©", "ê°’"],
        "result_table_rows": ["ë¹ˆë„", "ê°•ë„", "T ê°’", "ìœ„í—˜ë“±ê¸‰"],
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "gpt_response": "GPT ì›ë¬¸ ì‘ë‹µ: {response}",
        "phase2_header": "ê°œì„ ëŒ€ì±… ìë™ ìƒì„± (Phase 2)",
        "language_select_label": "ê°œì„ ëŒ€ì±… ì–¸ì–´ ì„ íƒ:",
        "input_method_label": "ì…ë ¥ ë°©ì‹ ì„ íƒ:",
        "input_methods": ["Phase 1 í‰ê°€ ê²°ê³¼ ì‚¬ìš©", "ì§ì ‘ ì…ë ¥"],
        "phase1_results_header": "Phase 1 í‰ê°€ ê²°ê³¼",
        "risk_level_text": "ìœ„í—˜ë„: ë¹ˆë„ {freq}, ê°•ë„ {intensity}, Tê°’ {t_value} (ë“±ê¸‰ {grade})",
        "phase1_first_warning": "ë¨¼ì € Phase 1ì—ì„œ ìœ„í—˜ì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.",
        "hazard_label": "ìœ í•´ìœ„í—˜ìš”ì¸:",
        "frequency_label": "ë¹ˆë„ (1-5):",
        "intensity_label": "ê°•ë„ (1-5):",
        "t_value_text": "Tê°’: {t_value} (ë“±ê¸‰: {grade})",
        "generate_improvement_btn": "ê°œì„ ëŒ€ì±… ìƒì„±",
        "generating_improvement": "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•˜ëŠ” ì¤‘...",
        "no_data_warning": "Phase 1ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±ì„ ì™„ë£Œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "improvement_result_header": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼",
        "improvement_plan_header": "ê°œì„ ëŒ€ì±…",
        "risk_improvement_header": "ìœ„í—˜ë„ ê°œì„  ê²°ê³¼",
        "comparison_columns": ["í•­ëª©", "ê°œì„  ì „", "ê°œì„  í›„"],
        "risk_reduction_label": "ìœ„í—˜ ê°ì†Œìœ¨ (RRR)",
        "t_value_change_header": "ìœ„í—˜ë„(Tê°’) ë³€í™”",
        "before_improvement": "ê°œì„  ì „ Tê°’:",
        "after_improvement": "ê°œì„  í›„ Tê°’:",
        "parsing_error_improvement": "ê°œì„ ëŒ€ì±… ìƒì„± ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "save_assessment": "í‰ê°€ ê²°ê³¼ ì €ì¥",
        "assessment_saved": "í‰ê°€ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "export_excel": "Excelë¡œ ë‚´ë³´ë‚´ê¸°",
        "export_pdf": "PDFë¡œ ë‚´ë³´ë‚´ê¸°",
        "history_header": "í‰ê°€ ì´ë ¥",
        "statistics_header": "í†µê³„ ë¶„ì„",
        "risk_distribution": "ìœ„í—˜ë“±ê¸‰ ë¶„í¬",
        "monthly_trend": "ì›”ë³„ í‰ê°€ ì¶”ì´",
        "work_type_analysis": "ì‘ì—…ìœ í˜•ë³„ ìœ„í—˜ë„ ë¶„ì„",
        "confidence_score": "ì‹ ë¢°ë„ ì ìˆ˜: {score}%",
        "data_insights": "ë°ì´í„° ì¸ì‚¬ì´íŠ¸",
        "total_assessments": "ì´ í‰ê°€ ê±´ìˆ˜",
        "high_risk_count": "ê³ ìœ„í—˜ (Aë“±ê¸‰) ê±´ìˆ˜",
        "avg_risk_score": "í‰ê·  ìœ„í—˜ë„(Tê°’)",
        "improvement_rate": "ê°œì„ ìœ¨"
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_phase1": "Risk Assessment (Phase 1)",
        "tab_phase2": "Improvement Measures (Phase 2)",
        "tab_history": "Assessment History",
        "tab_statistics": "Statistical Analysis",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": """
        The risk assessment automation system using LLM (Large Language Model) innovatively improves safety management at construction sites:
        
        1. <span class="highlight">Automatic prediction of 'hazards' and risk level calculation through generative AI</span> <span class="phase-badge">Phase 1</span>
        2. <span class="highlight">Automatic generation of improvement measures and reduction rate prediction to reduce risk level</span> <span class="phase-badge">Phase 2</span>
        3. AI learns existing risk assessments at construction sites by process and their hazard factors
        4. After the development of automatic generation technology, risk analysis and improvement measures based on risk level
        
        This system is expected to evolve into an integrated accident prediction program through the incorporation of AI technology into EHS platforms such as PIMS and Safety Guardian.
        """,
        # ... (ì˜ì–´ í…ìŠ¤íŠ¸ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ)
    },
    "Chinese": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ç³»ç»Ÿæ¦‚è¿°",
        "tab_phase1": "é£é™©è¯„ä¼° (ç¬¬1é˜¶æ®µ)",
        "tab_phase2": "æ”¹è¿›æªæ–½ (ç¬¬2é˜¶æ®µ)",
        "tab_history": "è¯„ä¼°å†å²",
        "tab_statistics": "ç»Ÿè®¡åˆ†æ",
        # ... (ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ)
    }
}

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Artificial Intelligence Risk Assessment",
    page_icon="ğŸ› ï¸",
    layout="wide"
)

# ê°œì„ ëœ ìŠ¤íƒ€ì¼ ì ìš©
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
        font-weight: bold;
    }
    .sub-header {
        font-size: 1.8rem;
        color: #0D47A1;
        margin-top: 2rem;
        margin-bottom: 1rem;
        border-bottom: 2px solid #E3F2FD;
        padding-bottom: 0.5rem;
    }
    .metric-container {
        background: linear-gradient(135deg, #f0f2f6 0%, #e8eaf6 100%);
        border-radius: 15px;
        padding: 20px;
        box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        margin: 10px 0;
    }
    .info-text {
        font-size: 1rem;
        color: #424242;
        margin-bottom: 1rem;
        line-height: 1.6;
    }
    .highlight {
        background: linear-gradient(120deg, #e3f2fd 0%, #bbdefb 100%);
        padding: 8px 12px;
        border-radius: 8px;
        font-weight: 500;
    }
    .result-box {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        border-radius: 15px;
        padding: 20px;
        margin: 15px 0;
        border-left: 5px solid #4CAF50;
        box-shadow: 0 3px 10px rgba(0,0,0,0.1);
    }
    .phase-badge {
        background: linear-gradient(45deg, #4CAF50, #45a049);
        color: white;
        padding: 8px 15px;
        border-radius: 20px;
        font-size: 0.85rem;
        margin-right: 10px;
        font-weight: 600;
        box-shadow: 0 2px 5px rgba(76, 175, 80, 0.3);
    }
    .similar-case {
        background: linear-gradient(135deg, #f1f8e9 0%, #e8f5e8 100%);
        border-radius: 12px;
        padding: 15px;
        margin-bottom: 12px;
        border-left: 4px solid #689f38;
        box-shadow: 0 2px 8px rgba(0,0,0,0.05);
    }
    .language-selector {
        position: absolute;
        top: 10px;
        right: 10px;
        z-index: 1000;
    }
    .success-message {
        background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%);
        border: 1px solid #c3e6cb;
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        color: #155724;
    }
    .warning-message {
        background: linear-gradient(135deg, #fff3cd 0%, #ffeaa7 100%);
        border: 1px solid #ffeaa7;
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        color: #856404;
    }
    .confidence-badge {
        background: linear-gradient(45deg, #2196F3, #1976D2);
        color: white;
        padding: 5px 12px;
        border-radius: 15px;
        font-size: 0.8rem;
        font-weight: 600;
        display: inline-block;
        margin-left: 10px;
    }
    .data-card {
        background: white;
        border-radius: 12px;
        padding: 20px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        margin: 15px 0;
        border: 1px solid #e0e0e0;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
        background-color: #f8f9fa;
        border-radius: 10px 10px 0 0;
        margin-right: 5px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #1976D2;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ê°œì„ ëœ ë²„ì „)
if "index" not in st.session_state:
    st.session_state.index = None
if "embeddings" not in st.session_state:
    st.session_state.embeddings = None
if "retriever_pool_df" not in st.session_state:
    st.session_state.retriever_pool_df = None
if "language" not in st.session_state:
    st.session_state.language = "Korean"
if "assessment_history" not in st.session_state:
    st.session_state.assessment_history = []
if "last_assessment" not in st.session_state:
    st.session_state.last_assessment = None
if "data_loaded" not in st.session_state:
    st.session_state.data_loaded = False
if "current_dataset" not in st.session_state:
    st.session_state.current_dataset = None

# ìƒë‹¨ì— ì–¸ì–´ ì„ íƒê¸° ì¶”ê°€
col1, col2 = st.columns([6, 1])
with col2:
    selected_language = st.selectbox(
        "",
        options=list(system_texts.keys()),
        index=list(system_texts.keys()).index(st.session_state.language) if st.session_state.language in system_texts else 0,
        key="language_selector"
    )
    st.session_state.language = selected_language

# í˜„ì¬ ì–¸ì–´ì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
texts = system_texts[st.session_state.language]

# í—¤ë” í‘œì‹œ
st.markdown(f'<div class="main-header">{texts["title"]}</div>', unsafe_allow_html=True)

# íƒ­ ì„¤ì • (ê°œì„ ëœ ë²„ì „ - ì´ë ¥ ë° í†µê³„ íƒ­ ì¶”ê°€)
tabs = st.tabs([
    texts["tab_overview"], 
    texts["tab_phase1"], 
    texts["tab_phase2"],
    texts.get("tab_history", "Assessment History"),
    texts.get("tab_statistics", "Statistical Analysis")
])

# ------------------ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „) ------------------

def determine_grade(value):
    """ë¹ˆë„*ê°•ë„ ê²°ê³¼ Tì— ë”°ë¥¸ ë“±ê¸‰ ê²°ì • í•¨ìˆ˜."""
    if 16 <= value <= 25:
        return 'A'
    elif 10 <= value <= 15:
        return 'B'
    elif 5 <= value <= 9:
        return 'C'
    elif 3 <= value <= 4:
        return 'D'
    elif 1 <= value <= 2:
        return 'E'
    else:
        return 'ì•Œ ìˆ˜ ì—†ìŒ' if st.session_state.language == 'Korean' else 'Unknown'

def calculate_confidence_score(retrieved_docs, similarity_scores=None):
    """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
    if similarity_scores is not None:
        # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
        avg_similarity = np.mean(similarity_scores) if len(similarity_scores) > 0 else 0.5
        confidence = min(100, avg_similarity * 100)
    else:
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ì™€ ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì¶”ì •
        doc_count = len(retrieved_docs)
        if doc_count >= 3:
            confidence = 85
        elif doc_count >= 2:
            confidence = 75
        else:
            confidence = 65
    
    return round(confidence, 1)

def load_data(selected_dataset_name):
    """ê°œì„ ëœ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ - ë” ë§ì€ í•„ë“œ ì²˜ë¦¬"""
    try:
        # ì‹¤ì œ Excel íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        df = pd.read_excel(f"{selected_dataset_name}.xlsx")
        
        # ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„ 
        # í—¤ë” ì •ë¦¬
        df.columns = df.columns.str.strip()
        
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
        columns_to_drop = ['ì‚­ì œ Del', 'Unnamed: 0'] if any(col in df.columns for col in ['ì‚­ì œ Del', 'Unnamed: 0']) else []
        if columns_to_drop:
            df = df.drop(columns_to_drop, axis=1)
        
        # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì¸ ê²½ìš° ì²˜ë¦¬
        if df.iloc[0].isna().sum() > len(df.columns) * 0.5:
            df = df.iloc[1:]
        
        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
        column_mapping = {
            'ì‘ì—…í™œë™ ë° ë‚´ìš©\nWork & Contents': 'ì‘ì—…í™œë™ ë° ë‚´ìš©',
            'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥\nHazard & Risk': 'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥',
            'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥\nDamage & Effect': 'í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥'
        }
        df = df.rename(columns=column_mapping)
        
        # ë¹ˆë„, ê°•ë„ ì»¬ëŸ¼ ì°¾ê¸° ë° ì´ë¦„ ë³€ê²½
        freq_cols = [col for col in df.columns if 'ë¹ˆë„' in str(col) or 'Frequency' in str(col)]
        intensity_cols = [col for col in df.columns if 'ê°•ë„' in str(col) or 'Severity' in str(col) or 'Intensity' in str(col)]
        
        if freq_cols:
            df = df.rename(columns={freq_cols[0]: 'ë¹ˆë„'})
        if intensity_cols:
            df = df.rename(columns={intensity_cols[0]: 'ê°•ë„'})
        
        # ìˆ«ìí˜• ë³€í™˜ ë° Tê°’ ê³„ì‚°
        if 'ë¹ˆë„' in df.columns and 'ê°•ë„' in df.columns:
            df['ë¹ˆë„'] = pd.to_numeric(df['ë¹ˆë„'], errors='coerce').fillna(3)
            df['ê°•ë„'] = pd.to_numeric(df['ê°•ë„'], errors='coerce').fillna(3)
            df['T'] = df['ë¹ˆë„'] * df['ê°•ë„']
        else:
            # ê¸°ë³¸ê°’ ì„¤ì •
            df['ë¹ˆë„'] = 3
            df['ê°•ë„'] = 3
            df['T'] = 9
        
        # ë“±ê¸‰ ê³„ì‚°
        df['ë“±ê¸‰'] = df['T'].apply(determine_grade)
        
        # ë¹ˆ í–‰ ì œê±°
        df = df.dropna(subset=['ì‘ì—…í™œë™ ë° ë‚´ìš©', 'ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥'], how='all')
        
        # ê°œì„ ëŒ€ì±… ì»¬ëŸ¼ í™•ì¸
        improvement_cols = [col for col in df.columns if any(keyword in str(col) for keyword in ['ê°œì„ ', 'Improvement', 'Corrective', 'ëŒ€ì±…'])]
        if improvement_cols:
            df['ê°œì„ ëŒ€ì±…'] = df[improvement_cols[0]]
        
        return df
        
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        # ë” í¬ê´„ì ì¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        st.warning("Excel íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ í™•ì¥ëœ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        
        sample_data = {
            "ì‘ì—…í™œë™ ë° ë‚´ìš©": [
                "Unloading of steel structure materials using forklift at temporary site storage area",
                "Installation of Concrete / CMU blocks", 
                "Excavation and backfill work",
                "Steel reinforcement installation",
                "Concrete pouring and finishing",
                "Scaffolding assembly and dismantling",
                "Electrical wiring and installation",
                "Welding operations",
                "Heavy equipment operation",
                "Material handling and transportation"
            ],
            "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥": [
                "Fall of load due to multiple lifting",
                "Fall due to insufficient working platform",
                "Cave-in due to unstable soil conditions",
                "Cut injury from rebar handling",
                "Chemical exposure from concrete additives",
                "Fall from height during assembly",
                "Electric shock from live wires",
                "Fire and explosion from welding",
                "Struck by moving equipment",
                "Musculoskeletal injury from manual handling"
            ],
            "í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥": [
                "Injury from falling objects",
                "Fall injury",
                "Burial and crushing",
                "Laceration",
                "Chemical burns",
                "Fall from height",
                "Electrocution",
                "Burns and fire",
                "Impact injury",
                "Strain and sprain"
            ],
            "ë¹ˆë„": [3, 3, 4, 2, 2, 4, 3, 2, 3, 4],
            "ê°•ë„": [5, 5, 4, 3, 4, 5, 5, 4, 4, 2],
            "ê°œì„ ëŒ€ì±…": [
                "1) Install proper rigging equipment 2) Conduct pre-lift safety checks 3) Maintain clear communication",
                "1) Install missing scaffold planks 2) Use full body harness 3) Install safety railings",
                "1) Proper soil analysis 2) Install shoring system 3) Regular inspection",
                "1) Use proper PPE 2) Safe handling procedures 3) Tool maintenance",
                "1) Proper ventilation 2) Use appropriate PPE 3) Material safety procedures",
                "1) Competent person supervision 2) Fall protection system 3) Regular inspection",
                "1) LOTO procedures 2) Qualified electrician 3) Proper insulation",
                "1) Fire watch personnel 2) Proper ventilation 3) Hot work permits",
                "1) Designated traffic routes 2) Spotters 3) Equipment maintenance",
                "1) Mechanical aids 2) Proper lifting techniques 3) Team lifting"
            ]
        }
        
        df = pd.DataFrame(sample_data)
        df['T'] = df['ë¹ˆë„'] * df['ê°•ë„']
        df['ë“±ê¸‰'] = df['T'].apply(determine_grade)
        
        return df

def embed_texts_with_openai(texts, model="text-embedding-3-large", api_key=None):
    """ê°œì„ ëœ OpenAI ì„ë² ë”© ìƒì„± í•¨ìˆ˜"""
    if api_key:
        openai.api_key = api_key
    
    embeddings = []
    progress_bar = st.progress(0)
    total = len(texts)
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ê°œì„ 
    batch_size = 10
    for i in range(0, total, batch_size):
        batch_texts = texts[i:i+batch_size]
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_texts = [str(text).replace("\n", " ").strip() for text in batch_texts]
            
            response = openai.Embedding.create(
                model=model, 
                input=processed_texts
            )
            
            batch_embeddings = [item["embedding"] for item in response["data"]]
            embeddings.extend(batch_embeddings)
            
        except Exception as e:
            st.error(f"ë°°ì¹˜ {i//batch_size + 1} ì„ë² ë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ë²¡í„°ë¡œ ëŒ€ì²´
            for _ in batch_texts:
                embeddings.append([0.0] * 1536)
        
        progress_bar.progress(min(1.0, (i + batch_size) / total))
    
    return embeddings

def generate_with_gpt(prompt, api_key=None, model="gpt-4o-mini", language="Korean"):
    """ê°œì„ ëœ GPT í˜¸ì¶œ í•¨ìˆ˜"""
    if api_key:
        openai.api_key = api_key
    
    system_prompts = {
        "Korean": "ìœ„í—˜ì„± í‰ê°€ ë° ê°œì„ ëŒ€ì±… ìƒì„±ì„ ë•ëŠ” ì „ë¬¸ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ì•ˆì „ ê´€ë¦¬ ì¡°ì¹˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.",
        "English": "I am a professional assistant helping with risk assessment and improvement measures. I provide accurate and specific safety management recommendations.",
        "Chinese": "æˆ‘æ˜¯ä¸€ä¸ªååŠ©è¿›è¡Œé£é™©è¯„ä¼°å’Œæ”¹è¿›æªæ–½çš„ä¸“ä¸šåŠ©æ‰‹ã€‚æˆ‘æä¾›å‡†ç¡®å’Œå…·ä½“çš„å®‰å…¨ç®¡ç†å»ºè®®ã€‚"
    }
    
    system_prompt = system_prompts.get(language, system_prompts["Korean"])
    
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì¶¤
            max_tokens=500,   # ë” ìƒì„¸í•œ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€
            presence_penalty=0.1,
            frequency_penalty=0.1
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        st.error(f"GPT API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def save_assessment_to_history(assessment_data):
    """í‰ê°€ ê²°ê³¼ë¥¼ ì´ë ¥ì— ì €ì¥"""
    assessment_data['timestamp'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    assessment_data['id'] = len(st.session_state.assessment_history) + 1
    st.session_state.assessment_history.append(assessment_data)

def export_to_excel(data, filename="risk_assessment_results.xlsx"):
    """Excel íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    try:
        df = pd.DataFrame(data)
        df.to_excel(filename, index=False, engine='openpyxl')
        return filename
    except Exception as e:
        st.error(f"Excel ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {str(e)}")
        return None

def create_risk_visualization(assessment_history):
    """ìœ„í—˜ë„ ì‹œê°í™” ì°¨íŠ¸ ìƒì„±"""
    if not assessment_history:
        return None
    
    df = pd.DataFrame(assessment_history)
    
    # ìœ„í—˜ë“±ê¸‰ ë¶„í¬ ì°¨íŠ¸
    grade_counts = df['grade'].value_counts()
    fig_grade = px.pie(
        values=grade_counts.values, 
        names=grade_counts.index,
        title="ìœ„í—˜ë“±ê¸‰ ë¶„í¬",
        color_discrete_map={
            'A': '#FF4444', 'B': '#FF8800', 'C': '#FFCC00', 
            'D': '#88CC00', 'E': '#44CC44'
        }
    )
    
    # ì›”ë³„ ì¶”ì´ ì°¨íŠ¸
    df['date'] = pd.to_datetime(df['timestamp'])
    df['month'] = df['date'].dt.to_period('M')
    monthly_counts = df.groupby('month').size()
    
    fig_trend = px.line(
        x=monthly_counts.index.astype(str), 
        y=monthly_counts.values,
        title="ì›”ë³„ í‰ê°€ ê±´ìˆ˜ ì¶”ì´",
        labels={'x': 'ì›”', 'y': 'í‰ê°€ ê±´ìˆ˜'}
    )
    
    return fig_grade, fig_trend

# Phase 1 ê´€ë ¨ í•¨ìˆ˜ë“¤
def construct_prompt_phase1_hazard(retrieved_docs, activity_text, language="Korean"):
    """ì‘ì—…í™œë™ìœ¼ë¡œë¶€í„° ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
    prompt_templates = {
        "Korean": {
            "intro": "ë‹¤ìŒì€ ê±´ì„¤ í˜„ì¥ì˜ ì‘ì—…í™œë™ê³¼ ê·¸ì— ë”°ë¥¸ ìœ í•´ìœ„í—˜ìš”ì¸ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n",
            "example_format": "ì˜ˆì‹œ {i}:\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\nìœ„í—˜ë„: T={t_value} (ë“±ê¸‰ {grade})\n\n",
            "query_format": "ì´ì œ ë‹¤ìŒ ì‘ì—…í™œë™ì— ëŒ€í•œ ìœ í•´ìœ„í—˜ìš”ì¸ì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”. êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ì ì¸ ìœ„í—˜ìš”ì¸ì„ ì œì‹œí•˜ì„¸ìš”:\nì‘ì—…í™œë™: {activity}\n\nì˜ˆì¸¡ëœ ìœ í•´ìœ„í—˜ìš”ì¸: "
        },
        "English": {
            "intro": "The following are examples of work activities at construction sites and their associated hazards:\n\n",
            "example_format": "Example {i}:\nWork Activity: {activity}\nHazard: {hazard}\nRisk Level: T={t_value} (Grade {grade})\n\n",
            "query_format": "Now, please predict the hazard for the following work activity. Provide specific and practical risk factors:\nWork Activity: {activity}\n\nPredicted Hazard: "
        },
        "Chinese": {
            "intro": "ä»¥ä¸‹æ˜¯å»ºç­‘å·¥åœ°çš„å·¥ä½œæ´»åŠ¨åŠå…¶ç›¸å…³å±å®³çš„ä¾‹å­:\n\n",
            "example_format": "ä¾‹å­ {i}:\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: {hazard}\né£é™©ç­‰çº§: T={t_value} (ç­‰çº§ {grade})\n\n",
            "query_format": "ç°åœ¨ï¼Œè¯·é¢„æµ‹ä»¥ä¸‹å·¥ä½œæ´»åŠ¨çš„å±å®³ã€‚è¯·æä¾›å…·ä½“å’Œå®ç”¨çš„é£é™©å› ç´ :\nå·¥ä½œæ´»åŠ¨: {activity}\n\né¢„æµ‹çš„å±å®³: "
        }
    }
    
    template = prompt_templates.get(language, prompt_templates["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            activity = doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']
            hazard = doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']
            t_value = doc.get('T', 0)
            grade = doc.get('ë“±ê¸‰', 'C')
            retrieved_examples.append((activity, hazard, t_value, grade))
        except:
            continue
    
    prompt = template["intro"]
    for i, (activity, hazard, t_value, grade) in enumerate(retrieved_examples, 1):
        prompt += template["example_format"].format(
            i=i, activity=activity, hazard=hazard, t_value=t_value, grade=grade
        )
    
    prompt += template["query_format"].format(activity=activity_text)
    
    return prompt

def construct_prompt_phase1_risk(retrieved_docs, activity_text, hazard_text, language="Korean"):
    """ë¹ˆë„ì™€ ê°•ë„ ì˜ˆì¸¡ì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸"""
    prompt_templates = {
        "Korean": {
            "intro": "ë‹¤ìŒì€ ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ì— ë”°ë¥¸ ìœ„í—˜ë„ í‰ê°€ ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n",
            "example_format": "ì˜ˆì‹œ {i}:\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\në¹ˆë„: {freq} (1=ë§¤ìš° ë“œë­„, 2=ë“œë­„, 3=ë³´í†µ, 4=ìì£¼, 5=ë§¤ìš° ìì£¼)\nê°•ë„: {intensity} (1=ê²½ë¯¸, 2=ì•½ê°„, 3=ë³´í†µ, 4=ì‹¬ê°, 5=ì¹˜ëª…ì )\nTê°’: {t_value}\n\n",
            "query_format": "ë‹¤ìŒ ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ì— ëŒ€í•´ ë¹ˆë„(1-5)ì™€ ê°•ë„(1-5)ë¥¼ í‰ê°€í•˜ì„¸ìš”:\n\nì‘ì—…í™œë™: {activity}\nìœ í•´ìœ„í—˜ìš”ì¸: {hazard}\n\ní‰ê°€ ê¸°ì¤€:\n- ë¹ˆë„: í•´ë‹¹ ìœ„í—˜ì´ ë°œìƒí•  ê°€ëŠ¥ì„± (1=ë§¤ìš° ë“œë­„ ~ 5=ë§¤ìš° ìì£¼)\n- ê°•ë„: ì‚¬ê³  ë°œìƒ ì‹œ í”¼í•´ ì •ë„ (1=ê²½ë¯¸ ~ 5=ì¹˜ëª…ì )\n\në‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•˜ì„¸ìš”:\n{json_format}\n\nì‘ë‹µ:"
        },
        "English": {
            "intro": "The following are examples of risk assessment based on work activities and hazards:\n\n",
            "example_format": "Example {i}:\nWork Activity: {activity}\nHazard: {hazard}\nFrequency: {freq} (1=Very Rare, 2=Rare, 3=Moderate, 4=Frequent, 5=Very Frequent)\nSeverity: {intensity} (1=Minor, 2=Slight, 3=Moderate, 4=Serious, 5=Fatal)\nT-value: {t_value}\n\n",
            "query_format": "Please evaluate the frequency (1-5) and severity (1-5) for the following work activity and hazard:\n\nWork Activity: {activity}\nHazard: {hazard}\n\nEvaluation Criteria:\n- Frequency: Likelihood of the risk occurring (1=Very Rare ~ 5=Very Frequent)\n- Severity: Degree of harm if accident occurs (1=Minor ~ 5=Fatal)\n\nPlease respond exactly in the following JSON format:\n{json_format}\n\nResponse:"
        },
        "Chinese": {
            "intro": "ä»¥ä¸‹æ˜¯åŸºäºå·¥ä½œæ´»åŠ¨å’Œå±å®³çš„é£é™©è¯„ä¼°ç¤ºä¾‹:\n\n",
            "example_format": "ç¤ºä¾‹ {i}:\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: {hazard}\né¢‘ç‡: {freq} (1=éå¸¸ç½•è§, 2=ç½•è§, 3=ä¸­ç­‰, 4=é¢‘ç¹, 5=éå¸¸é¢‘ç¹)\nä¸¥é‡ç¨‹åº¦: {intensity} (1=è½»å¾®, 2=è½»åº¦, 3=ä¸­ç­‰, 4=ä¸¥é‡, 5=è‡´å‘½)\nTå€¼: {t_value}\n\n",
            "query_format": "è¯·è¯„ä¼°ä»¥ä¸‹å·¥ä½œæ´»åŠ¨å’Œå±å®³çš„é¢‘ç‡(1-5)å’Œä¸¥é‡ç¨‹åº¦(1-5):\n\nå·¥ä½œæ´»åŠ¨: {activity}\nå±å®³: {hazard}\n\nè¯„ä¼°æ ‡å‡†:\n- é¢‘ç‡: é£é™©å‘ç”Ÿçš„å¯èƒ½æ€§ (1=éå¸¸ç½•è§ ~ 5=éå¸¸é¢‘ç¹)\n- ä¸¥é‡ç¨‹åº¦: äº‹æ•…å‘ç”Ÿæ—¶çš„ä¼¤å®³ç¨‹åº¦ (1=è½»å¾® ~ 5=è‡´å‘½)\n\nè¯·å®Œå…¨æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›ç­”:\n{json_format}\n\nå›ç­”:"
        }
    }
    
    json_formats = {
        "Korean": '{"ë¹ˆë„": ìˆ«ì, "ê°•ë„": ìˆ«ì, "T": ìˆ«ì}',
        "English": '{"frequency": number, "intensity": number, "T": number}',
        "Chinese": '{"é¢‘ç‡": æ•°å­—, "å¼ºåº¦": æ•°å­—, "T": æ•°å­—}'
    }
    
    template = prompt_templates.get(language, prompt_templates["Korean"])
    json_format = json_formats.get(language, json_formats["Korean"])
    
    retrieved_examples = []
    for _, doc in retrieved_docs.iterrows():
        try:
            activity = doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']
            hazard = doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']
            frequency = int(doc['ë¹ˆë„'])
            intensity = int(doc['ê°•ë„'])
            T_value = frequency * intensity
            retrieved_examples.append((activity, hazard, frequency, intensity, T_value))
        except:
            continue
    
    prompt = template["intro"]
    for i, (activity, hazard, freq, intensity, t_value) in enumerate(retrieved_examples[:3], 1):
        prompt += template["example_format"].format(
            i=i, activity=activity, hazard=hazard, 
            freq=freq, intensity=intensity, t_value=t_value
        )
    
    prompt += template["query_format"].format(
        activity=activity_text, 
        hazard=hazard_text,
        json_format=json_format
    )
    
    return prompt

def parse_gpt_output_phase1(gpt_output, language="Korean"):
    """GPT ì¶œë ¥ íŒŒì‹± ê°œì„  ë²„ì „"""
    json_patterns = {
        "Korean": r'\{"ë¹ˆë„":\s*([1-5]),\s*"ê°•ë„":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "English": r'\{"frequency":\s*([1-5]),\s*"intensity":\s*([1-5]),\s*"T":\s*([0-9]+)\}',
        "Chinese": r'\{"é¢‘ç‡":\s*([1-5]),\s*"å¼ºåº¦":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    }
    
    # ìš°ì„  í˜„ì¬ ì–¸ì–´ íŒ¨í„´ìœ¼ë¡œ ì‹œë„
    pattern = json_patterns.get(language, json_patterns["Korean"])
    match = re.search(pattern, gpt_output)
    
    if match:
        pred_frequency = int(match.group(1))
        pred_intensity = int(match.group(2))
        pred_T = int(match.group(3))
        return pred_frequency, pred_intensity, pred_T
    
    # ë‹¤ë¥¸ ì–¸ì–´ íŒ¨í„´ë“¤ë„ ì‹œë„
    for lang, pattern in json_patterns.items():
        if lang != language:
            match = re.search(pattern, gpt_output)
            if match:
                pred_frequency = int(match.group(1))
                pred_intensity = int(match.group(2))
                pred_T = int(match.group(3))
                return pred_frequency, pred_intensity, pred_T
    
    # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ìˆ«ìë§Œ ì¶”ì¶œ ì‹œë„
    numbers = re.findall(r'\b([1-5])\b', gpt_output)
    if len(numbers) >= 2:
        pred_frequency = int(numbers[0])
        pred_intensity = int(numbers[1])
        pred_T = pred_frequency * pred_intensity
        return pred_frequency, pred_intensity, pred_T
    
    return None

# Phase 2 ê´€ë ¨ í•¨ìˆ˜ë“¤
def construct_prompt_phase2(retrieved_docs, activity_text, hazard_text, freq, intensity, T, target_language="Korean"):
    """ê°œì„ ëŒ€ì±… ìƒì„±ì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸"""
    
    example_section = ""
    examples_added = 0
    
    field_names = {
        "Korean": {
            "improvement_fields": ['ê°œì„ ëŒ€ì±…', 'ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ', 'ê°œì„ ë°©ì•ˆ', 'Corrective Action'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"
        },
        "English": {
            "improvement_fields": ['Improvement Measures', 'Improvement Plan', 'Countermeasures', 'ê°œì„ ëŒ€ì±…'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©", 
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"
        },
        "Chinese": {
            "improvement_fields": ['æ”¹è¿›æªæ–½', 'æ”¹è¿›è®¡åˆ’', 'å¯¹ç­–', 'ê°œì„ ëŒ€ì±…'],
            "activity": "ì‘ì—…í™œë™ ë° ë‚´ìš©",
            "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"
        }
    }
    
    fields = field_names.get(target_language, field_names["Korean"])
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì˜ˆì‹œ ìƒì„±
    for _, row in retrieved_docs.iterrows():
        try:
            improvement_plan = ""
            for field in fields["improvement_fields"]:
                if field in row and pd.notna(row[field]) and str(row[field]).strip():
                    improvement_plan = str(row[field]).strip()
                    break
            
            if not improvement_plan:
                continue
                
            original_freq = int(row['ë¹ˆë„']) if 'ë¹ˆë„' in row else 3
            original_intensity = int(row['ê°•ë„']) if 'ê°•ë„' in row else 3
            original_T = original_freq * original_intensity
            
            # ê°œì„  í›„ ê°’ ì¶”ì • (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)
            improved_freq = max(1, original_freq - 1)
            improved_intensity = max(1, original_intensity - 1)
            improved_T = improved_freq * improved_intensity
            
            example_section += f"""
ì˜ˆì‹œ {examples_added + 1}:
ì‘ì—…í™œë™: {row[fields['activity']]}
ìœ í•´ìœ„í—˜ìš”ì¸: {row[fields['hazard']]}
ì›ë˜ ìœ„í—˜ë„: ë¹ˆë„ {original_freq}, ê°•ë„ {original_intensity}, T={original_T}
ê°œì„ ëŒ€ì±…: {improvement_plan}
ê°œì„  í›„ ìœ„í—˜ë„: ë¹ˆë„ {improved_freq}, ê°•ë„ {improved_intensity}, T={improved_T}
ìœ„í—˜ ê°ì†Œìœ¨: {((original_T - improved_T) / original_T * 100):.1f}%

"""
            examples_added += 1
            if examples_added >= 2:
                break
                
        except Exception as e:
            continue
    
    # ê¸°ë³¸ ì˜ˆì‹œ (ì–¸ì–´ë³„)
    if examples_added == 0:
        if target_language == "Korean":
            example_section = """
ì˜ˆì‹œ 1:
ì‘ì—…í™œë™: êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…
ìœ í•´ìœ„í—˜ìš”ì¸: ë¶€ì ì ˆí•œ ê²½ì‚¬ë¡œ ì¸í•œ êµ´ì°©ë²½ ë¶•ê´´
ì›ë˜ ìœ„í—˜ë„: ë¹ˆë„ 3, ê°•ë„ 4, T=12
ê°œì„ ëŒ€ì±…: 1) í† ì–‘ ë¶„ë¥˜ì— ë”°ë¥¸ ì ì ˆí•œ ê²½ì‚¬ ìœ ì§€ 2) êµ´ì°© ë²½ë©´ ë³´ê°• ì‹œì„¤ ì„¤ì¹˜ 3) ì •ê¸°ì ì¸ ì§€ë°˜ ìƒíƒœ ì ê²€ ì‹¤ì‹œ 4) ì‘ì—…ì ì•ˆì „êµìœ¡ ê°•í™”
ê°œì„  í›„ ìœ„í—˜ë„: ë¹ˆë„ 1, ê°•ë„ 2, T=2
ìœ„í—˜ ê°ì†Œìœ¨: 83.3%

ì˜ˆì‹œ 2:
ì‘ì—…í™œë™: ì¤‘ì¥ë¹„ë¥¼ ì´ìš©í•œ ìì¬ ìš´ë°˜
ìœ í•´ìœ„í—˜ìš”ì¸: ìš´ë°˜ ì¤‘ ìì¬ ë‚™í•˜ë¡œ ì¸í•œ ì¶©ëŒ
ì›ë˜ ìœ„í—˜ë„: ë¹ˆë„ 2, ê°•ë„ 5, T=10
ê°œì„ ëŒ€ì±…: 1) ì ì ˆí•œ ë¦¬ê¹… ì¥ë¹„ ì‚¬ìš© 2) ì‘ì—… ì „ ì•ˆì „ì ê²€ ì‹¤ì‹œ 3) ì‹ í˜¸ìˆ˜ ë°°ì¹˜ 4) ì•ˆì „êµ¬ì—­ ì„¤ì • ë° ì¶œì…í†µì œ
ê°œì„  í›„ ìœ„í—˜ë„: ë¹ˆë„ 1, ê°•ë„ 2, T=2
ìœ„í—˜ ê°ì†Œìœ¨: 80.0%

"""
        elif target_language == "English":
            example_section = """
Example 1:
Work Activity: Excavation and backfilling
Hazard: Collapse of excavation wall due to improper sloping
Original Risk: Frequency 3, Intensity 4, T=12
Improvement Measures: 1) Maintain proper slope according to soil classification 2) Install excavation wall reinforcement 3) Conduct regular ground condition inspections 4) Enhance worker safety training
Improved Risk: Frequency 1, Intensity 2, T=2
Risk Reduction Rate: 83.3%

Example 2:
Work Activity: Material transportation using heavy equipment
Hazard: Material fall causing collision during transport
Original Risk: Frequency 2, Intensity 5, T=10
Improvement Measures: 1) Use appropriate rigging equipment 2) Conduct pre-work safety inspections 3) Deploy signal personnel 4) Establish safety zones and access control
Improved Risk: Frequency 1, Intensity 2, T=2
Risk Reduction Rate: 80.0%

"""
    
    # ì–¸ì–´ë³„ JSON í‚¤ì™€ ì§€ì‹œì‚¬í•­
    json_keys = {
        "Korean": {
            "improvement": "ê°œì„ ëŒ€ì±…",
            "improved_freq": "ê°œì„ _í›„_ë¹ˆë„", 
            "improved_intensity": "ê°œì„ _í›„_ê°•ë„",
            "improved_t": "ê°œì„ _í›„_T",
            "reduction_rate": "ìœ„í—˜_ê°ì†Œìœ¨"
        },
        "English": {
            "improvement": "improvement_measures",
            "improved_freq": "improved_frequency",
            "improved_intensity": "improved_intensity", 
            "improved_t": "improved_T",
            "reduction_rate": "risk_reduction_rate"
        },
        "Chinese": {
            "improvement": "æ”¹è¿›æªæ–½",
            "improved_freq": "æ”¹è¿›åé¢‘ç‡",
            "improved_intensity": "æ”¹è¿›åå¼ºåº¦",
            "improved_t": "æ”¹è¿›åTå€¼", 
            "reduction_rate": "é£é™©é™ä½ç‡"
        }
    }
    
    instructions = {
        "Korean": {
            "task": "ë‹¤ìŒ ì‘ì—…í™œë™ê³¼ ìœ í•´ìœ„í—˜ìš”ì¸ì— ëŒ€í•œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„ ëŒ€ì±…ì„ ì œì‹œí•˜ê³ , ê°œì„  í›„ ìœ„í—˜ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”:",
            "guidelines": """
ê°œì„ ëŒ€ì±… ì‘ì„± ê°€ì´ë“œë¼ì¸:
- ìµœì†Œ 4ê°œ ì´ìƒì˜ êµ¬ì²´ì ì¸ ê°œì„ ì¡°ì¹˜ë¥¼ ì œì‹œí•˜ì„¸ìš”
- ê¸°ìˆ ì  ëŒ€ì±…, ê´€ë¦¬ì  ëŒ€ì±…, ê°œì¸ë³´í˜¸êµ¬ ëŒ€ì±…ì„ ê· í˜•ìˆê²Œ í¬í•¨í•˜ì„¸ìš”
- ì‹¤ì œ í˜„ì¥ì—ì„œ ì ìš© ê°€ëŠ¥í•œ í˜„ì‹¤ì ì¸ ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”
- ê° ëŒ€ì±…ì€ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”

ìœ„í—˜ë„ í‰ê°€ ê¸°ì¤€:
- ê°œì„  í›„ ë¹ˆë„ëŠ” ì›ë˜ ë¹ˆë„ë³´ë‹¤ 1-2ë‹¨ê³„ ë‚®ê²Œ í‰ê°€
- ê°œì„  í›„ ê°•ë„ëŠ” ëŒ€ì±…ì˜ íš¨ê³¼ì„±ì— ë”°ë¼ ì¡°ì •
- í˜„ì‹¤ì ì¸ ê°œì„  íš¨ê³¼ë¥¼ ë°˜ì˜í•˜ì„¸ìš”""",
            "output_instruction": "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•˜ì„¸ìš”:"
        },
        "English": {
            "task": "Provide specific and actionable improvement measures for the following work activity and hazard, and evaluate the post-improvement risk level:",
            "guidelines": """
Improvement Measures Guidelines:
- Provide at least 4 specific improvement actions
- Include a balanced mix of technical, administrative, and PPE measures
- Suggest realistic solutions applicable in actual field conditions
- Clearly distinguish each measure with numbering

Risk Assessment Criteria:
- Post-improvement frequency should be 1-2 levels lower than original
- Post-improvement intensity should be adjusted based on measure effectiveness
- Reflect realistic improvement effects""",
            "output_instruction": "Please respond exactly in the following JSON format:"
        },
        "Chinese": {
            "task": "ä¸ºä»¥ä¸‹å·¥ä½œæ´»åŠ¨å’Œå±å®³æä¾›å…·ä½“å¯è¡Œçš„æ”¹è¿›æªæ–½ï¼Œå¹¶è¯„ä¼°æ”¹è¿›åçš„é£é™©ç­‰çº§ï¼š",
            "guidelines": """
æ”¹è¿›æªæ–½æŒ‡å¯¼åŸåˆ™:
- æä¾›è‡³å°‘4é¡¹å…·ä½“çš„æ”¹è¿›è¡ŒåŠ¨
- åŒ…æ‹¬æŠ€æœ¯æªæ–½ã€ç®¡ç†æªæ–½å’Œä¸ªäººé˜²æŠ¤è®¾å¤‡æªæ–½çš„å¹³è¡¡ç»„åˆ
- å»ºè®®åœ¨å®é™…ç°åœºæ¡ä»¶ä¸‹å¯åº”ç”¨çš„ç°å®è§£å†³æ–¹æ¡ˆ
- ç”¨ç¼–å·æ¸…æ™°åŒºåˆ†æ¯é¡¹æªæ–½

é£é™©è¯„ä¼°æ ‡å‡†:
- æ”¹è¿›åé¢‘ç‡åº”æ¯”åŸå§‹é¢‘ç‡ä½1-2ä¸ªç­‰çº§
- æ”¹è¿›åå¼ºåº¦åº”æ ¹æ®æªæ–½æœ‰æ•ˆæ€§è¿›è¡Œè°ƒæ•´
- åæ˜ ç°å®çš„æ”¹è¿›æ•ˆæœ""",
            "output_instruction": "è¯·å®Œå…¨æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼š"
        }
    }
    
    keys = json_keys.get(target_language, json_keys["Korean"])
    instr = instructions.get(target_language, instructions["Korean"])
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""{example_section}

{instr['task']}

ì‘ì—…í™œë™: {activity_text}
ìœ í•´ìœ„í—˜ìš”ì¸: {hazard_text}
í˜„ì¬ ìœ„í—˜ë„: ë¹ˆë„ {freq}, ê°•ë„ {intensity}, T={T}

{instr['guidelines']}

{instr['output_instruction']}
{{
    "{keys['improvement']}": "êµ¬ì²´ì ì¸ ê°œì„ ëŒ€ì±… ëª©ë¡ (ìµœì†Œ 4ê°œ í•­ëª©)",
    "{keys['improved_freq']}": ìˆ«ì (1-5),
    "{keys['improved_intensity']}": ìˆ«ì (1-5),
    "{keys['improved_t']}": ìˆ«ì,
    "{keys['reduction_rate']}": ìˆ«ì (ë°±ë¶„ìœ¨)
}}

ì‘ë‹µ:"""
    
    return prompt

def parse_gpt_output_phase2(gpt_output, language="Korean"):
    """ê°œì„ ëœ Phase 2 GPT ì¶œë ¥ íŒŒì‹±"""
    try:
        # JSON ë¸”ë¡ ì¶”ì¶œ
        json_match = re.search(r'```json\s*(.*?)\s*```', gpt_output, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # JSON ë¸”ë¡ í‘œì‹œê°€ ì—†ëŠ” ê²½ìš° ì¤‘ê´„í˜¸ ë‚´ìš© ì¶”ì¶œ
            brace_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', gpt_output, re.DOTALL)
            if brace_match:
                json_str = brace_match.group(0)
            else:
                json_str = gpt_output.strip()
        
        # JSON íŒŒì‹±
        result = json.loads(json_str)
        
        # ì–¸ì–´ë³„ í‚¤ ë§¤í•‘
        key_mappings = {
            "Korean": {
                "improvement": ["ê°œì„ ëŒ€ì±…", "ê°œì„ ë°©ì•ˆ", "ê°œì„ ì¡°ì¹˜"],
                "improved_freq": ["ê°œì„ _í›„_ë¹ˆë„", "ê°œì„ í›„ë¹ˆë„", "ê°œì„  í›„ ë¹ˆë„"],
                "improved_intensity": ["ê°œì„ _í›„_ê°•ë„", "ê°œì„ í›„ê°•ë„", "ê°œì„  í›„ ê°•ë„"],
                "improved_t": ["ê°œì„ _í›„_T", "ê°œì„ í›„T", "ê°œì„  í›„ T"],
                "reduction_rate": ["ìœ„í—˜_ê°ì†Œìœ¨", "ê°ì†Œìœ¨", "ìœ„í—˜ê°ì†Œìœ¨"]
            },
            "English": {
                "improvement": ["improvement_measures", "improvement_plan", "improvements"],
                "improved_freq": ["improved_frequency", "new_frequency"],
                "improved_intensity": ["improved_intensity", "new_intensity"],
                "improved_t": ["improved_T", "new_T"],
                "reduction_rate": ["risk_reduction_rate", "reduction_rate"]
            },
            "Chinese": {
                "improvement": ["æ”¹è¿›æªæ–½", "æ”¹è¿›è®¡åˆ’"],
                "improved_freq": ["æ”¹è¿›åé¢‘ç‡", "æ–°é¢‘ç‡"],
                "improved_intensity": ["æ”¹è¿›åå¼ºåº¦", "æ–°å¼ºåº¦"],
                "improved_t": ["æ”¹è¿›åTå€¼", "æ–°Tå€¼"],
                "reduction_rate": ["é£é™©é™ä½ç‡", "é™ä½ç‡"]
            }
        }
        
        # ê²°ê³¼ ë§¤í•‘
        mapped_result = {}
        mappings = key_mappings.get(language, key_mappings["Korean"])
        
        for result_key, possible_keys in mappings.items():
            for key in possible_keys:
                if key in result:
                    mapped_result[result_key] = result[key]
                    break
            # í‚¤ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            if result_key not in mapped_result:
                if result_key == "improved_freq":
                    mapped_result[result_key] = 2
                elif result_key == "improved_intensity":
                    mapped_result[result_key] = 2
                elif result_key == "improved_t":
                    mapped_result[result_key] = 4
                elif result_key == "reduction_rate":
                    mapped_result[result_key] = 50.0
                elif result_key == "improvement":
                    mapped_result[result_key] = "ê°œì„ ëŒ€ì±…ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        return mapped_result
        
    except Exception as e:
        st.error(f"JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# ë°ì´í„°ì…‹ ì˜µì…˜ (í™•ì¥ëœ ë²„ì „)
dataset_options = {
    "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)": "SWRO ê±´ì¶•ê³µì • (ê±´ì¶•)",
    "Civil (í† ëª©)": "Civil (í† ëª©)", 
    "Marine (í† ëª©)": "Marine (í† ëª©)",
    "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)": "SWRO ê¸°ê³„ê³µì‚¬ (í”ŒëœíŠ¸)",
    "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)": "SWRO ì „ê¸°ì‘ì—…í‘œì¤€ (í”ŒëœíŠ¸)",
    "ìƒ˜í”Œ ë°ì´í„°": "sample_data"
}

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘
# ----- ì‹œìŠ¤í…œ ê°œìš” íƒ­ -----
with tabs[0]:
    st.markdown(f'<div class="sub-header">{texts["overview_header"]}</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([3, 2])
    
    with col1:
        st.markdown(f"""
        <div class="info-text">
        {texts["overview_text"]}
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        # AI ìœ„í—˜ì„±í‰ê°€ í”„ë¡œì„¸ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ (ê°œì„ ëœ ë²„ì „)
        st.markdown(f'<div class="data-card">', unsafe_allow_html=True)
        st.markdown(f'<div style="text-align: center; margin-bottom: 15px; font-weight: bold; font-size: 1.1rem;">{texts["process_title"]}</div>', unsafe_allow_html=True)
        
        steps = texts["process_steps"]
        
        for i, step in enumerate(steps):
            phase_badge = '<span class="phase-badge">Phase 1</span>' if i < 4 else '<span class="phase-badge">Phase 2</span>'
            arrow = " â†“" if i < len(steps)-1 else ""
            st.markdown(f"**{i+1}. {step}** {phase_badge}{arrow}", unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ì‹œìŠ¤í…œ íŠ¹ì§• (ê°œì„ ëœ ë ˆì´ì•„ì›ƒ)
    st.markdown(f'<div class="sub-header">{texts["features_title"]}</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown('<div class="data-card">', unsafe_allow_html=True)
        st.markdown(texts["phase1_features"], unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
    
    with col2:
        st.markdown('<div class="data-card">', unsafe_allow_html=True)
        st.markdown(texts["phase2_features"], unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

# ----- Phase 1: ìœ„í—˜ì„± í‰ê°€ íƒ­ (ê°œì„ ëœ ë²„ì „) -----
with tabs[1]:
    st.markdown(f'<div class="sub-header">{texts["phase1_header"]}</div>', unsafe_allow_html=True)
    
    # ì„¤ì • ì„¹ì…˜
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # API í‚¤ ì…ë ¥
        api_key = st.text_input(texts["api_key_label"], type="password", key="api_key_phase1")
    
    with col2:
        # ë°ì´í„°ì…‹ ì„ íƒ
        selected_dataset_name = st.selectbox(
            texts["dataset_label"],
            options=list(dataset_options.keys()),
            key="dataset_selector_phase1"
        )
    
    # ë°ì´í„° ë¡œë“œ ì„¹ì…˜ (ê°œì„ ëœ UI)
    st.markdown("### " + texts['load_data_label'])
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        if st.button(texts["load_data_btn"], key="load_data_phase1", type="primary"):
            if not api_key:
                st.warning(texts["api_key_warning"])
            else:
                with st.spinner(texts["data_loading"]):
                    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
                    df = load_data(dataset_options[selected_dataset_name])
                    
                    if df is not None:
                        # ë°ì´í„° ì •ë³´ í‘œì‹œ
                        st.session_state.current_dataset = selected_dataset_name
                        total_rows = len(df)
                        
                        # Train/Test ë¶„í• 
                        if total_rows > 10:
                            train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
                        else:
                            train_df = df
                            test_df = df.sample(min(2, len(df)))
                        
                        # ë¦¬íŠ¸ë¦¬ë²„ í’€ êµ¬ì„±
                        retriever_pool_df = train_df.copy()
                        retriever_pool_df['content'] = retriever_pool_df.apply(
                            lambda row: ' '.join([
                                str(row.get('ì‘ì—…í™œë™ ë° ë‚´ìš©', '')),
                                str(row.get('ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥', '')),
                                str(row.get('í”¼í•´í˜•íƒœ ë° í™˜ê²½ì˜í–¥', ''))
                            ]), axis=1
                        )
                        
                        texts_to_embed = retriever_pool_df['content'].tolist()
                        
                        # ì„ë² ë”© ìƒì„±
                        with st.status("í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì¤‘...", expanded=True) as status:
                            st.write(f"ì´ {len(texts_to_embed)}ê°œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘...")
                            
                            openai.api_key = api_key
                            embeddings = embed_texts_with_openai(texts_to_embed, api_key=api_key)
                            
                            st.write("FAISS ì¸ë±ìŠ¤ êµ¬ì„± ì¤‘...")
                            # FAISS ì¸ë±ìŠ¤ êµ¬ì„±
                            embeddings_array = np.array(embeddings, dtype='float32')
                            dimension = embeddings_array.shape[1]
                            faiss_index = faiss.IndexFlatL2(dimension)
                            faiss_index.add(embeddings_array)
                            
                            status.update(label="ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ!", state="complete")
                        
                        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                        st.session_state.index = faiss_index
                        st.session_state.embeddings = embeddings_array
                        st.session_state.retriever_pool_df = retriever_pool_df
                        st.session_state.test_df = test_df
                        st.session_state.data_loaded = True
                        
                        # ì„±ê³µ ë©”ì‹œì§€
                        st.markdown(f"""
                        <div class="success-message">
                        âœ… {texts["data_load_success"].format(total_rows=total_rows)}
                        <br>ğŸ“Š ë°ì´í„°ì…‹: {selected_dataset_name}
                        <br>ğŸ” ì„ë² ë”© ì°¨ì›: {dimension}
                        </div>
                        """, unsafe_allow_html=True)
    
    with col2:
        if st.session_state.data_loaded:
            st.metric("ë°ì´í„° ìƒíƒœ", "âœ… ë¡œë“œë¨", f"{len(st.session_state.retriever_pool_df)}ê°œ í•­ëª©")
    
    with col3:
        if st.session_state.data_loaded:
            st.metric("ì¸ë±ìŠ¤ ìƒíƒœ", "âœ… êµ¬ì„±ë¨", f"{st.session_state.embeddings.shape[1]}ì°¨ì›")
    
    # ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ ì„¹ì…˜ (ê°œì„ ëœ UI)
    st.markdown("### " + texts['hazard_prediction_header'])
    
    if st.session_state.index is None:
        st.markdown(f"""
        <div class="warning-message">
        âš ï¸ {texts["load_first_warning"]}
        </div>
        """, unsafe_allow_html=True)
    else:
        with st.form("user_input_form"):
            col1, col2 = st.columns([3, 1])
            
            with col1:
                user_work = st.text_area(
                    texts["activity_label"], 
                    height=100,
                    placeholder="ì˜ˆ: êµ´ì°©ê¸°ë¥¼ ì´ìš©í•œ í† ì‚¬ êµ´ì°© ì‘ì—…",
                    key="form_user_work"
                )
            
            with col2:
                st.markdown("<br>", unsafe_allow_html=True)
                submitted = st.form_submit_button(
                    texts["predict_hazard_btn"], 
                    type="primary",
                    use_container_width=True
                )
            
        if submitted:
            if not user_work.strip():
                st.warning(texts["activity_warning"])
            else:
                with st.spinner(texts["predicting_hazard"]):
                    # ì¿¼ë¦¬ ì„ë² ë”©
                    query_embedding = embed_texts_with_openai([user_work], api_key=api_key)[0]
                    query_embedding_array = np.array([query_embedding], dtype='float32')
                    
                    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
                    k_similar = min(5, len(st.session_state.retriever_pool_df))
                    distances, indices = st.session_state.index.search(query_embedding_array, k_similar)
                    retrieved_docs = st.session_state.retriever_pool_df.iloc[indices[0]]
                    
                    # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
                    similarity_scores = 1 / (1 + distances[0])  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    confidence = calculate_confidence_score(retrieved_docs, similarity_scores)
                    
                    # ê²°ê³¼ í‘œì‹œ ì„¹ì…˜
                    col1, col2 = st.columns([3, 2])
                    
                    with col1:
                        # ìœ ì‚¬í•œ ì‚¬ë¡€ í‘œì‹œ (ê°œì„ ëœ UI)
                        st.markdown(f"#### {texts['similar_cases_header']}")
                        
                        for i, (_, doc) in enumerate(retrieved_docs.iterrows(), 1):
                            similarity_pct = similarity_scores[i-1] * 100
                            
                            st.markdown(f"""
                            <div class="similar-case">
                                <div style="display: flex; justify-content: between; align-items: center;">
                                    <strong>ì‚¬ë¡€ {i}</strong>
                                    <span class="confidence-badge">ìœ ì‚¬ë„: {similarity_pct:.1f}%</span>
                                </div>
                                <strong>ì‘ì—…í™œë™:</strong> {doc['ì‘ì—…í™œë™ ë° ë‚´ìš©']}<br>
                                <strong>ìœ í•´ìœ„í—˜ìš”ì¸:</strong> {doc['ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥']}<br>
                                <strong>ìœ„í—˜ë„:</strong> ë¹ˆë„ {doc['ë¹ˆë„']}, ê°•ë„ {doc['ê°•ë„']}, Tê°’ {doc['T']} (ë“±ê¸‰ {doc['ë“±ê¸‰']})
                            </div>
                            """, unsafe_allow_html=True)
                    
                    with col2:
                        # GPT ì˜ˆì¸¡ ê²°ê³¼
                        st.markdown(f"#### {texts['prediction_result_header']}")
                        
                        # ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡
                        hazard_prompt = construct_prompt_phase1_hazard(
                            retrieved_docs, user_work, language=st.session_state.language
                        )
                        hazard_prediction = generate_with_gpt(
                            hazard_prompt, api_key=api_key, language=st.session_state.language
                        )
                        
                        # ë¹ˆë„ì™€ ê°•ë„ ì˜ˆì¸¡
                        risk_prompt = construct_prompt_phase1_risk(
                            retrieved_docs, user_work, hazard_prediction, language=st.session_state.language
                        )
                        risk_prediction = generate_with_gpt(
                            risk_prompt, api_key=api_key, language=st.session_state.language
                        )
                        
                        # ê²°ê³¼ ë°•ìŠ¤
                        st.markdown('<div class="result-box">', unsafe_allow_html=True)
                        
                        st.markdown(f"**{texts['activity_label']}** {user_work}")
                        st.markdown(f"**{texts['hazard_label']}** {hazard_prediction}")
                        
                        # ì‹ ë¢°ë„ í‘œì‹œ
                        st.markdown(f"""
                        <div style="text-align: right;">
                            <span class="confidence-badge">{texts.get('confidence_score', 'Confidence: {score}%').format(score=confidence)}</span>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # ìœ„í—˜ë„ íŒŒì‹± ë° í‘œì‹œ
                        parse_result = parse_gpt_output_phase1(risk_prediction, language=st.session_state.language)
                        if parse_result is not None:
                            f_val, i_val, t_val = parse_result
                            grade = determine_grade(t_val)
                            
                            # ë©”íŠ¸ë¦­ìœ¼ë¡œ í‘œì‹œ
                            col_a, col_b = st.columns(2)
                            with col_a:
                                st.metric("ë¹ˆë„", f_val)
                                st.metric("Tê°’", t_val)
                            with col_b:
                                st.metric("ê°•ë„", i_val)
                                st.metric("ìœ„í—˜ë“±ê¸‰", grade)
                            
                            # ìœ„í—˜ë“±ê¸‰ì— ë”°ë¥¸ ìƒ‰ìƒ í‘œì‹œ
                            grade_colors = {'A': '#FF4444', 'B': '#FF8800', 'C': '#FFCC00', 'D': '#88CC00', 'E': '#44CC44'}
                            grade_color = grade_colors.get(grade, '#888888')
                            
                            st.markdown(f"""
                            <div style="background-color: {grade_color}; color: white; padding: 10px; border-radius: 5px; text-align: center; font-weight: bold; margin-top: 10px;">
                                ìœ„í—˜ë“±ê¸‰: {grade}
                            </div>
                            """, unsafe_allow_html=True)
                            
                            # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
                            assessment_data = {
                                'activity': user_work,
                                'hazard': hazard_prediction,
                                'frequency': f_val,
                                'intensity': i_val,
                                'T': t_val,
                                'grade': grade,
                                'confidence': confidence,
                                'dataset': st.session_state.current_dataset
                            }
                            st.session_state.last_assessment = assessment_data
                            
                            # ì €ì¥ ë²„íŠ¼
                            if st.button("ğŸ“Š " + texts.get("save_assessment", "Save Assessment"), key="save_phase1"):
                                save_assessment_to_history(assessment_data.copy())
                                st.success("âœ… " + texts.get("assessment_saved", "Assessment saved!"))
                        
                        else:
                            st.error(texts["parsing_error"])
                            with st.expander("GPT ì›ë¬¸ ì‘ë‹µ ë³´ê¸°"):
                                st.write(risk_prediction)
                        
                        st.markdown('</div>', unsafe_allow_html=True)

# ----- Phase 2: ê°œì„ ëŒ€ì±… ìƒì„± íƒ­ (ê°œì„ ëœ ë²„ì „) -----
with tabs[2]:
    st.markdown(f'<div class="sub-header">{texts["phase2_header"]}</div>', unsafe_allow_html=True)
    
    # ì„¤ì • ì„¹ì…˜
    col1, col2, col3 = st.columns(3)
    
    with col1:
        api_key_phase2 = st.text_input(texts["api_key_label"], type="password", key="api_key_phase2")
    
    with col2:
        target_language = st.selectbox(
            texts["language_select_label"],
            options=list(system_texts.keys()),
            index=list(system_texts.keys()).index(st.session_state.language),
            key="target_language"
        )
    
    with col3:
        input_method = st.radio(
            texts["input_method_label"],
            options=texts["input_methods"],
            index=0,
            key="input_method",
            horizontal=True
        )
    
    # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
    if input_method == texts["input_methods"][0]:  # Phase 1 ê²°ê³¼ ì‚¬ìš©
        if hasattr(st.session_state, 'last_assessment') and st.session_state.last_assessment:
            last_assessment = st.session_state.last_assessment
            
            # Phase 1 ê²°ê³¼ í‘œì‹œ
            st.markdown("### " + texts['phase1_results_header'])
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown('<div class="data-card">', unsafe_allow_html=True)
                st.markdown(f"**{texts['activity_label']}** {last_assessment['activity']}")
                st.markdown(f"**{texts['hazard_label']}** {last_assessment['hazard']}")
                st.markdown(f"**ìœ„í—˜ë„:** ë¹ˆë„ {last_assessment['frequency']}, ê°•ë„ {last_assessment['intensity']}, Tê°’ {last_assessment['T']} (ë“±ê¸‰ {last_assessment['grade']})")
                st.markdown('</div>', unsafe_allow_html=True)
            
            with col2:
                # ìœ„í—˜ë“±ê¸‰ ì‹œê°í™”
                grade_colors = {'A': '#FF4444', 'B': '#FF8800', 'C': '#FFCC00', 'D': '#88CC00', 'E': '#44CC44'}
                grade_color = grade_colors.get(last_assessment['grade'], '#888888')
                
                st.markdown(f"""
                <div class="metric-container" style="text-align: center;">
                    <div style="font-size: 2rem; color: {grade_color}; font-weight: bold;">
                        {last_assessment['grade']}
                    </div>
                    <div style="font-size: 0.9rem; color: #666;">ìœ„í—˜ë“±ê¸‰</div>
                    <div style="font-size: 1.2rem; font-weight: bold; margin-top: 10px;">
                        T = {last_assessment['T']}
                    </div>
                </div>
                """, unsafe_allow_html=True)
            
            activity_text = last_assessment['activity']
            hazard_text = last_assessment['hazard']
            frequency = last_assessment['frequency']
            intensity = last_assessment['intensity']
            T_value = last_assessment['T']
            
        else:
            st.markdown(f"""
            <div class="warning-message">
            âš ï¸ {texts["phase1_first_warning"]}
            </div>
            """, unsafe_allow_html=True)
            activity_text = hazard_text = None
            frequency = intensity = T_value = None
    
    else:  # ì§ì ‘ ì…ë ¥
        st.markdown("### ì§ì ‘ ì…ë ¥")
        
        col1, col2 = st.columns(2)
        
        with col1:
            activity_text = st.text_area(texts["activity_label"], height=100, key="direct_activity")
            hazard_text = st.text_area(texts["hazard_label"], height=100, key="direct_hazard")
        
        with col2:
            frequency = st.slider(texts["frequency_label"], min_value=1, max_value=5, value=3, key="direct_freq")
            intensity = st.slider(texts["intensity_label"], min_value=1, max_value=5, value=3, key="direct_intensity")
            T_value = frequency * intensity
            
            st.markdown(f"""
            <div class="metric-container" style="text-align: center;">
                <div style="font-size: 1.5rem; font-weight: bold;">T = {T_value}</div>
                <div style="font-size: 0.9rem; color: #666;">ë“±ê¸‰: {determine_grade(T_value)}</div>
            </div>
            """, unsafe_allow_html=True)
    
    # ê°œì„ ëŒ€ì±… ìƒì„± ì„¹ì…˜
    if activity_text and hazard_text and frequency and intensity and T_value:
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown("### " + texts.get("improvement_plan_header", "Improvement Measures"))
        
        with col2:
            generate_button = st.button(
                "ğŸš€ " + texts["generate_improvement_btn"], 
                key="generate_improvement",
                type="primary",
                use_container_width=True
            )
        
        if generate_button:
            if not api_key_phase2:
                st.warning(texts["api_key_warning"])
            else:
                with st.spinner(texts["generating_improvement"]):
                    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤€ë¹„
                    if st.session_state.retriever_pool_df is not None and st.session_state.index is not None:
                        # Phase 1ì—ì„œ êµ¬ì„±ëœ ë°ì´í„° ì‚¬ìš©
                        retriever_pool_df = st.session_state.retriever_pool_df
                        
                        # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
                        query_text = f"{activity_text} {hazard_text}"
                        query_embedding = embed_texts_with_openai([query_text], api_key=api_key_phase2)[0]
                        query_embedding_array = np.array([query_embedding], dtype='float32')
                        
                        k_similar = min(3, len(retriever_pool_df))
                        distances, indices = st.session_state.index.search(query_embedding_array, k_similar)
                        retrieved_docs = retriever_pool_df.iloc[indices[0]]
                    else:
                        # ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©
                        st.info(texts["no_data_warning"])
                        df = load_data("sample_data")
                        retrieved_docs = df.sample(min(3, len(df)))
                    
                    # ê°œì„ ëŒ€ì±… ìƒì„± í”„ë¡¬í”„íŠ¸
                    prompt = construct_prompt_phase2(
                        retrieved_docs, 
                        activity_text, 
                        hazard_text, 
                        frequency, 
                        intensity, 
                        T_value, 
                        target_language
                    )
                    
                    # GPT í˜¸ì¶œ
                    generated_output = generate_with_gpt(
                        prompt, 
                        api_key=api_key_phase2, 
                        language=target_language
                    )
                    
                    # ê²°ê³¼ íŒŒì‹±
                    parsed_result = parse_gpt_output_phase2(generated_output, language=target_language)
                    
                    if parsed_result:
                        # ê²°ê³¼ í‘œì‹œ
                        improvement_plan = parsed_result.get("improvement", "")
                        improved_freq = parsed_result.get("improved_freq", 1)
                        improved_intensity = parsed_result.get("improved_intensity", 1)
                        improved_T = parsed_result.get("improved_t", improved_freq * improved_intensity)
                        rrr = parsed_result.get("reduction_rate", ((T_value - improved_T) / T_value * 100) if T_value > 0 else 0)
                        
                        # ê²°ê³¼ ë ˆì´ì•„ì›ƒ
                        col1, col2 = st.columns([3, 2])
                        
                        with col1:
                            # ê°œì„ ëŒ€ì±…
                            st.markdown('<div class="result-box">', unsafe_allow_html=True)
                            st.markdown(f"#### ğŸ“‹ {texts['improvement_plan_header']}")
                            st.markdown(improvement_plan.replace('1)', '\n1)').replace('2)', '\n2)').replace('3)', '\n3)').replace('4)', '\n4)'))
                            st.markdown('</div>', unsafe_allow_html=True)
                        
                        with col2:
                            # ìœ„í—˜ë„ ê°œì„  ê²°ê³¼
                            st.markdown('<div class="result-box">', unsafe_allow_html=True)
                            st.markdown(f"#### ğŸ“Š {texts['risk_improvement_header']}")
                            
                            # ê°œì„  ì „í›„ ë¹„êµ ì°¨íŠ¸
                            comparison_data = {
                                'Before': [frequency, intensity, T_value],
                                'After': [improved_freq, improved_intensity, improved_T]
                            }
                            comparison_df = pd.DataFrame(
                                comparison_data, 
                                index=['ë¹ˆë„', 'ê°•ë„', 'Tê°’']
                            )
                            
                            st.bar_chart(comparison_df)
                            
                            # ìœ„í—˜ ê°ì†Œìœ¨
                            st.metric(
                                label="ğŸ¯ " + texts["risk_reduction_label"],
                                value=f"{rrr:.1f}%",
                                delta=f"-{T_value - improved_T}"
                            )
                            
                            # ë“±ê¸‰ ë³€í™”
                            before_grade = determine_grade(T_value)
                            after_grade = determine_grade(improved_T)
                            
                            col_a, col_b = st.columns(2)
                            with col_a:
                                st.markdown(f"**ê°œì„  ì „:** {before_grade}")
                            with col_b:
                                st.markdown(f"**ê°œì„  í›„:** {after_grade}")
                            
                            st.markdown('</div>', unsafe_allow_html=True)
                        
                        # ì „ì²´ ê°œì„  ê²°ê³¼ ìš”ì•½
                        st.markdown('<div class="result-box">', unsafe_allow_html=True)
                        st.markdown("#### ğŸ“ˆ ê°œì„  íš¨ê³¼ ìš”ì•½")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric("ìœ„í—˜ë„(T)", improved_T, f"{improved_T - T_value}")
                        with col2:
                            st.metric("ìœ„í—˜ë“±ê¸‰", after_grade, f"{before_grade}â†’{after_grade}")
                        with col3:
                            st.metric("ê°ì†Œìœ¨", f"{rrr:.1f}%")
                        with col4:
                            if rrr >= 70:
                                effectiveness = "ë§¤ìš° íš¨ê³¼ì "
                                color = "#4CAF50"
                            elif rrr >= 50:
                                effectiveness = "íš¨ê³¼ì "
                                color = "#FF9800"
                            else:
                                effectiveness = "ë³´í†µ"
                                color = "#f44336"
                            
                            st.markdown(f"""
                            <div style="background-color: {color}; color: white; padding: 10px; border-radius: 5px; text-align: center;">
                                {effectiveness}
                            </div>
                            """, unsafe_allow_html=True)
                        
                        st.markdown('</div>', unsafe_allow_html=True)
                        
                        # ì €ì¥ ë° ë‚´ë³´ë‚´ê¸° ë²„íŠ¼
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            if st.button("ğŸ’¾ ê²°ê³¼ ì €ì¥", key="save_improvement"):
                                improvement_data = {
                                    'activity': activity_text,
                                    'hazard': hazard_text,
                                    'original_freq': frequency,
                                    'original_intensity': intensity,
                                    'original_T': T_value,
                                    'original_grade': before_grade,
                                    'improvement_plan': improvement_plan,
                                    'improved_freq': improved_freq,
                                    'improved_intensity': improved_intensity,
                                    'improved_T': improved_T,
                                    'improved_grade': after_grade,
                                    'reduction_rate': rrr,
                                    'language': target_language
                                }
                                save_assessment_to_history(improvement_data)
                                st.success("âœ… ê°œì„ ëŒ€ì±…ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
                        with col2:
                            # Excel ë‚´ë³´ë‚´ê¸° ë²„íŠ¼
                            st.download_button(
                                label="ğŸ“„ Excel ë‹¤ìš´ë¡œë“œ",
                                data=pd.DataFrame([improvement_data]).to_csv(index=False),
                                file_name=f"improvement_plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                                mime="text/csv"
                            )
                        
                        with col3:
                            # ìƒˆ í‰ê°€ ë²„íŠ¼
                            if st.button("ğŸ”„ ìƒˆ í‰ê°€", key="new_assessment"):
                                st.session_state.last_assessment = None
                                st.experimental_rerun()
                    
                    else:
                        st.error(texts["parsing_error_improvement"])
                        with st.expander("GPT ì›ë¬¸ ì‘ë‹µ ë³´ê¸°"):
                            st.write(generated_output)

# ----- í‰ê°€ ì´ë ¥ íƒ­ -----
with tabs[3]:
    st.markdown(f'<div class="sub-header">{texts.get("history_header", "Assessment History")}</div>', unsafe_allow_html=True)
    
    if st.session_state.assessment_history:
        # ì´ë ¥ í†µê³„
        col1, col2, col3, col4 = st.columns(4)
        
        history_df = pd.DataFrame(st.session_state.assessment_history)
        
        with col1:
            st.metric(
                texts.get("total_assessments", "Total Assessments"), 
                len(st.session_state.assessment_history)
            )
        
        with col2:
            if 'grade' in history_df.columns:
                high_risk_count = len(history_df[history_df['grade'] == 'A'])
                st.metric(
                    texts.get("high_risk_count", "High Risk (A Grade)"), 
                    high_risk_count
                )
        
        with col3:
            if 'T' in history_df.columns:
                avg_risk = history_df['T'].mean()
                st.metric(
                    texts.get("avg_risk_score", "Average Risk Score"), 
                    f"{avg_risk:.1f}"
                )
        
        with col4:
            if 'reduction_rate' in history_df.columns:
                avg_improvement = history_df['reduction_rate'].mean()
                st.metric(
                    texts.get("improvement_rate", "Improvement Rate"), 
                    f"{avg_improvement:.1f}%"
                )
        
        # ì´ë ¥ í…Œì´ë¸”
        st.markdown("### í‰ê°€ ì´ë ¥ ìƒì„¸")
        
        # ë°ì´í„° ì •ë¦¬
        display_columns = ['timestamp', 'activity', 'hazard', 'T', 'grade']
        if 'reduction_rate' in history_df.columns:
            display_columns.append('reduction_rate')
        
        display_df = history_df[display_columns].copy()
        display_df.columns = ['ì‹œê°„', 'ì‘ì—…í™œë™', 'ìœ í•´ìœ„í—˜ìš”ì¸', 'Tê°’', 'ë“±ê¸‰', 'ê°œì„ ìœ¨(%)'][:len(display_columns)]
        
        # í…Œì´ë¸” í‘œì‹œ (í˜ì´ì§€ë„¤ì´ì…˜)
        page_size = 10
        total_pages = (len(display_df) - 1) // page_size + 1
        
        if total_pages > 1:
            page = st.selectbox("í˜ì´ì§€ ì„ íƒ", range(1, total_pages + 1))
            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size
            display_df = display_df.iloc[start_idx:end_idx]
        
        st.dataframe(display_df, use_container_width=True)
        
        # ì´ë ¥ ë‹¤ìš´ë¡œë“œ
        col1, col2 = st.columns([1, 4])
        with col1:
            st.download_button(
                label="ğŸ“Š ì´ë ¥ ë‹¤ìš´ë¡œë“œ",
                data=pd.DataFrame(st.session_state.assessment_history).to_csv(index=False),
                file_name=f"assessment_history_{datetime.now().strftime('%Y%m%d')}.csv",
                mime="text/csv"
            )
    
    else:
        st.markdown("""
        <div style="text-align: center; padding: 50px; color: #666;">
            <h3>ì•„ì§ ì €ì¥ëœ í‰ê°€ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤</h3>
            <p>Phase 1 ë˜ëŠ” Phase 2ì—ì„œ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ì €ì¥í•´ë³´ì„¸ìš”.</p>
        </div>
        """, unsafe_allow_html=True)

# ----- í†µê³„ ë¶„ì„ íƒ­ -----
with tabs[4]:
    st.markdown(f'<div class="sub-header">{texts.get("statistics_header", "Statistical Analysis")}</div>', unsafe_allow_html=True)
    
    if st.session_state.assessment_history:
        # ì‹œê°í™” ìƒì„±
        charts = create_risk_visualization(st.session_state.assessment_history)
        
        if charts:
            fig_grade, fig_trend = charts
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ“Š ìœ„í—˜ë“±ê¸‰ ë¶„í¬")
                st.plotly_chart(fig_grade, use_container_width=True)
            
            with col2:
                st.markdown("#### ğŸ“ˆ ì›”ë³„ í‰ê°€ ì¶”ì´")
                st.plotly_chart(fig_trend, use_container_width=True)
        
        # ìƒì„¸ ë¶„ì„
        history_df = pd.DataFrame(st.session_state.assessment_history)
        
        st.markdown("### ğŸ“‹ ë°ì´í„° ì¸ì‚¬ì´íŠ¸")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown('<div class="data-card">', unsafe_allow_html=True)
            st.markdown("#### ìœ„í—˜ë„ ë¶„ì„")
            
            if 'T' in history_df.columns:
                # Tê°’ ë¶„í¬
                t_values = history_df['T']
                st.markdown(f"- **í‰ê·  Tê°’:** {t_values.mean():.2f}")
                st.markdown(f"- **ìµœê³  Tê°’:** {t_values.max()}")
                st.markdown(f"- **ìµœì € Tê°’:** {t_values.min()}")
                st.markdown(f"- **í‘œì¤€í¸ì°¨:** {t_values.std():.2f}")
                
                # Tê°’ íˆìŠ¤í† ê·¸ë¨
                fig_hist = px.histogram(
                    history_df, x='T', 
                    title="Tê°’ ë¶„í¬",
                    nbins=10
                )
                st.plotly_chart(fig_hist, use_container_width=True)
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col2:
            st.markdown('<div class="data-card">', unsafe_allow_html=True)
            st.markdown("#### ê°œì„  íš¨ê³¼ ë¶„ì„")
            
            if 'reduction_rate' in history_df.columns:
                improvement_data = history_df.dropna(subset=['reduction_rate'])
                
                if len(improvement_data) > 0:
                    reduction_rates = improvement_data['reduction_rate']
                    st.markdown(f"- **í‰ê·  ê°œì„ ìœ¨:** {reduction_rates.mean():.1f}%")
                    st.markdown(f"- **ìµœê³  ê°œì„ ìœ¨:** {reduction_rates.max():.1f}%")
                    st.markdown(f"- **ìµœì € ê°œì„ ìœ¨:** {reduction_rates.min():.1f}%")
                    
                    # ê°œì„ ìœ¨ ë¶„í¬
                    fig_improvement = px.box(
                        improvement_data, y='reduction_rate',
                        title="ê°œì„ ìœ¨ ë¶„í¬"
                    )
                    st.plotly_chart(fig_improvement, use_container_width=True)
                else:
                    st.markdown("ê°œì„ ëŒ€ì±… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            st.markdown('</div>', unsafe_allow_html=True)
        
        # ì‘ì—…ìœ í˜•ë³„ ë¶„ì„
        if 'activity' in history_df.columns:
            st.markdown("### ğŸ—ï¸ ì‘ì—…ìœ í˜•ë³„ ìœ„í—˜ë„ ë¶„ì„")
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë¶„ë¥˜
            def classify_work_type(activity):
                activity_lower = str(activity).lower()
                if any(word in activity_lower for word in ['êµ´ì°©', 'excavation', 'dig']):
                    return 'êµ´ì°©ì‘ì—…'
                elif any(word in activity_lower for word in ['ìš©ì ‘', 'welding', 'weld']):
                    return 'ìš©ì ‘ì‘ì—…'
                elif any(word in activity_lower for word in ['ìš´ë°˜', 'transport', 'carry']):
                    return 'ìš´ë°˜ì‘ì—…'
                elif any(word in activity_lower for word in ['ì„¤ì¹˜', 'install', 'assembly']):
                    return 'ì„¤ì¹˜ì‘ì—…'
                elif any(word in activity_lower for word in ['í•´ì²´', 'demolition', 'dismantle']):
                    return 'í•´ì²´ì‘ì—…'
                else:
                    return 'ê¸°íƒ€ì‘ì—…'
            
            history_df['work_type'] = history_df['activity'].apply(classify_work_type)
            
            # ì‘ì—…ìœ í˜•ë³„ í†µê³„
            work_type_stats = history_df.groupby('work_type').agg({
                'T': ['mean', 'max', 'count'],
                'grade': lambda x: (x == 'A').sum()
            }).round(2)
            
            work_type_stats.columns = ['í‰ê· _Tê°’', 'ìµœëŒ€_Tê°’', 'í‰ê°€_ê±´ìˆ˜', 'Aë“±ê¸‰_ê±´ìˆ˜']
            
            st.dataframe(work_type_stats, use_container_width=True)
            
            # ì‘ì—…ìœ í˜•ë³„ ìœ„í—˜ë„ ì°¨íŠ¸
            if len(work_type_stats) > 1:
                fig_worktype = px.bar(
                    x=work_type_stats.index,
                    y=work_type_stats['í‰ê· _Tê°’'],
                    title="ì‘ì—…ìœ í˜•ë³„ í‰ê·  ìœ„í—˜ë„",
                    labels={'x': 'ì‘ì—…ìœ í˜•', 'y': 'í‰ê·  Tê°’'}
                )
                st.plotly_chart(fig_worktype, use_container_width=True)
        
        # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        st.markdown("### ğŸ“¤ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # ì „ì²´ ì´ë ¥ ë‹¤ìš´ë¡œë“œ
            st.download_button(
                label="ğŸ“Š ì „ì²´ ì´ë ¥ CSV",
                data=history_df.to_csv(index=False),
                file_name=f"full_assessment_history_{datetime.now().strftime('%Y%m%d')}.csv",
                mime="text/csv"
            )
        
        with col2:
            # ê³ ìœ„í—˜ ë°ì´í„°ë§Œ ë‹¤ìš´ë¡œë“œ
            if 'grade' in history_df.columns:
                high_risk_df = history_df[history_df['grade'].isin(['A', 'B'])]
                if len(high_risk_df) > 0:
                    st.download_button(
                        label="âš ï¸ ê³ ìœ„í—˜ ë°ì´í„° CSV",
                        data=high_risk_df.to_csv(index=False),
                        file_name=f"high_risk_assessments_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv"
                    )
        
        with col3:
            # ê°œì„ ëŒ€ì±… ë°ì´í„°ë§Œ ë‹¤ìš´ë¡œë“œ
            if 'improvement_plan' in history_df.columns:
                improvement_df = history_df.dropna(subset=['improvement_plan'])
                if len(improvement_df) > 0:
                    st.download_button(
                        label="ğŸ’¡ ê°œì„ ëŒ€ì±… CSV",
                        data=improvement_df.to_csv(index=False),
                        file_name=f"improvement_plans_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv"
                    )
    
    else:
        st.markdown("""
        <div style="text-align: center; padding: 50px; color: #666;">
            <h3>í†µê³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤</h3>
            <p>ëª‡ ê±´ì˜ í‰ê°€ë¥¼ ìˆ˜í–‰í•œ í›„ ì´ íƒ­ì—ì„œ í†µê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
        </div>
        """, unsafe_allow_html=True)

# ----- í‘¸í„° ì„¹ì…˜ -----
st.markdown('<hr style="margin-top: 50px; border: 1px solid #e0e0e0;">', unsafe_allow_html=True)

# ì‹œìŠ¤í…œ ì •ë³´ ë° ë¡œê³ 
col1, col2, col3 = st.columns([2, 2, 2])

with col1:
    st.markdown("""
    <div style="text-align: center; padding: 20px;">
        <h4 style="color: #1976D2;">ğŸ—ï¸ ê±´ì„¤ ì•ˆì „ AI</h4>
        <p style="color: #666; font-size: 0.9rem;">
            LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€<br>
            ìë™í™” ì‹œìŠ¤í…œ
        </p>
    </div>
    """, unsafe_allow_html=True)

with col2:
    # ë¡œê³  í‘œì‹œ (ì‹¤ì œ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
    if os.path.exists("cau.png"):
        cau_logo = Image.open("cau.png")
        st.image(cau_logo, width=120)
    else:
        st.markdown("""
        <div style="text-align: center; padding: 20px;">
            <div style="background: #f5f5f5; border-radius: 10px; padding: 20px; margin: 10px;">
                <strong>ì¤‘ì•™ëŒ€í•™êµ</strong><br>
                <small>ê±´ì„¤í™˜ê²½í”ŒëœíŠ¸ê³µí•™ê³¼</small>
            </div>
        </div>
        """, unsafe_allow_html=True)

with col3:
    if os.path.exists("doosan.png"):
        doosan_logo = Image.open("doosan.png")
        st.image(doosan_logo, width=150)
    else:
        st.markdown("""
        <div style="text-align: center; padding: 20px;">
            <div style="background: #f5f5f5; border-radius: 10px; padding: 20px; margin: 10px;">
                <strong>ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°</strong><br>
                <small>EHS ë””ì§€í„¸í˜ì‹ </small>
            </div>
        </div>
        """, unsafe_allow_html=True)

# ë²„ì „ ì •ë³´
st.markdown("""
<div style="text-align: center; color: #999; font-size: 0.8rem; margin-top: 20px;">
    AI Risk Assessment System v2.0 | ê°œì„ ëœ ë²„ì „ | Last Updated: 2025-05-30
</div>
""", unsafe_allow_html=True)

# ê°œë°œì ë…¸íŠ¸ (ì„ íƒì  í‘œì‹œ)
with st.expander("ğŸ”§ ê°œë°œì ë…¸íŠ¸ ë° ê°œì„ ì‚¬í•­"):
    st.markdown("""
    ### ì£¼ìš” ê°œì„ ì‚¬í•­
    
    âœ… **UI/UX ê°œì„ **
    - í˜„ëŒ€ì ì´ê³  ì§ê´€ì ì¸ ì¸í„°í˜ì´ìŠ¤ ë””ìì¸
    - ê·¸ë¼ë°ì´ì…˜ê³¼ ê·¸ë¦¼ì íš¨ê³¼ë¡œ ì‹œê°ì  í’ˆì§ˆ í–¥ìƒ
    - ë°˜ì‘í˜• ë ˆì´ì•„ì›ƒìœ¼ë¡œ ë‹¤ì–‘í•œ í™”ë©´ í¬ê¸° ì§€ì›
    
    âœ… **ê¸°ëŠ¥ í™•ì¥**
    - í‰ê°€ ì´ë ¥ ê´€ë¦¬ ë° í†µê³„ ë¶„ì„ íƒ­ ì¶”ê°€
    - ì‹ ë¢°ë„ ì ìˆ˜ í‘œì‹œë¡œ AI ì˜ˆì¸¡ í’ˆì§ˆ ê°€ì‹œí™”
    - ë‹¤ì–‘í•œ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì˜µì…˜ ì œê³µ
    
    âœ… **ì„±ëŠ¥ ìµœì í™”**
    - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„ë² ë”© ìƒì„± íš¨ìœ¨ì„± ê°œì„ 
    - ë” ì •í™•í•œ JSON íŒŒì‹± ë¡œì§
    - ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì‚¬ìš©ì í”¼ë“œë°± ê°•í™”
    
    âœ… **ë°ì´í„° ì²˜ë¦¬ ê°œì„ **
    - ë” í¬ê´„ì ì¸ Excel íŒŒì¼ ì²˜ë¦¬
    - ìë™ ì»¬ëŸ¼ëª… ë§¤í•‘ ë° ë°ì´í„° ì •ê·œí™”
    - ëˆ„ë½ ë°ì´í„°ì— ëŒ€í•œ ê²¬ê³ í•œ ì²˜ë¦¬
    
    âœ… **ì‹œê°í™” ê°•í™”**
    - Plotlyë¥¼ í™œìš©í•œ ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸
    - ìœ„í—˜ë“±ê¸‰ë³„ ìƒ‰ìƒ ì½”ë”©
    - ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ í‘œì‹œ
    
    ### ê¸°ìˆ ì  íŠ¹ì§•
    - **ë©€í‹° ì–¸ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´
    - **ì‹¤ì‹œê°„ AI ë¶„ì„**: OpenAI GPT-4 ê¸°ë°˜
    - **ì˜ë¯¸ë¡ ì  ê²€ìƒ‰**: FAISS ë²¡í„° ì¸ë±ì‹±
    - **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ëª¨ë“ˆí™”ëœ í•¨ìˆ˜ êµ¬ì¡°
    """)

# ë””ë²„ê·¸ ì •ë³´ (ê°œë°œ ëª¨ë“œì—ì„œë§Œ í‘œì‹œ)
if st.sidebar.checkbox("ğŸ› ë””ë²„ê·¸ ëª¨ë“œ", key="debug_mode"):
    st.sidebar.markdown("### ì„¸ì…˜ ìƒíƒœ")
    st.sidebar.json({
        "ë°ì´í„° ë¡œë“œë¨": st.session_state.data_loaded,
        "í˜„ì¬ ë°ì´í„°ì…‹": st.session_state.current_dataset,
        "ì–¸ì–´": st.session_state.language,
        "í‰ê°€ ì´ë ¥ ìˆ˜": len(st.session_state.assessment_history),
        "ë§ˆì§€ë§‰ í‰ê°€": bool(st.session_state.last_assessment)
    })
