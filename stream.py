import streamlit as st
import pandas as pd
import numpy as np
import faiss
import re
import os
import io
from PIL import Image
from sklearn.model_selection import train_test_split
import openai

# -------------------------------------------------
# OpenAI ê³µì‹ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
# -------------------------------------------------

# ------------- ì‹œìŠ¤í…œ í…ìŠ¤íŠ¸ -----------------
# ë‚´ë¶€ ì²˜ë¦¬ëŠ” ëª¨ë‘ ì˜ì–´ë¡œ ì´ë£¨ì–´ì§€ê³ , ê²°ê³¼ëŠ” ì»¬ëŸ¼ ì´ë¦„ì„ í•œ/ì˜ ë³‘ê¸° í˜•íƒœë¡œ ì¶œë ¥í•˜ë„ë¡ êµ¬ì„±í•©ë‹ˆë‹¤.
system_texts = {
    "Korean": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "ì‹œìŠ¤í…œ ê°œìš”",
        "tab_assessment": "ìœ„í—˜ì„± í‰ê°€ & ê°œì„ ëŒ€ì±…",
        "overview_header": "LLM ê¸°ë°˜ ìœ„í—˜ì„±í‰ê°€ ì‹œìŠ¤í…œ",
        "overview_text": (
            "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹° AI Risk AssessmentëŠ” êµ­ë‚´ ë° í•´ì™¸ ê±´ì„¤í˜„ì¥ 'ìˆ˜ì‹œìœ„í—˜ì„±í‰ê°€' "
            "ë° 'ë…¸ë™ë¶€ ì¤‘ëŒ€ì¬í•´ ì‚¬ë¡€'ë¥¼ í•™ìŠµí•˜ì—¬ ê°œë°œëœ ìë™ ìœ„í—˜ì„±í‰ê°€ í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. "
            "ìƒì„±ëœ ê²°ê³¼ëŠ” ê²€ì¦ í›„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        ),
        "features_title": "ì‹œìŠ¤í…œ íŠ¹ì§•",
        "phase1_features": (
            "- ì‘ì—…í™œë™ì„ ì˜ì–´ë¡œ ë³€í™˜í•˜ì—¬ LLMì— ì…ë ¥\n"
            "- English ë‚´ë¶€ ì²˜ë¦¬ í›„, ê²°ê³¼ë¥¼ í•œ/ì˜ ë³‘ê¸° ì»¬ëŸ¼ìœ¼ë¡œ ì¶œë ¥\n"
            "- ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ë° í‘œì‹œ\n"
            "- ìœ„í—˜ë„ ê³„ì‚° ë° ê°œì„ ëŒ€ì±… ìƒì„±"
        ),
        "api_key_label": "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        "dataset_label": "ë°ì´í„°ì…‹ ì„ íƒ:",
        "load_data_btn": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±",
        "api_key_warning": "API í‚¤ë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.",
        "data_loading": "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì¤‘...",
        "demo_limit_info": "ë°ëª¨ìš©ìœ¼ë¡œ {max_texts}ê°œ í•­ëª©ë§Œ ì„ë² ë”©í•©ë‹ˆë‹¤.",
        "data_load_success": "ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„± ì™„ë£Œ! ({max_texts}ê°œ í•­ëª© ì²˜ë¦¬ë¨)",
        "activity_label": "ì‘ì—…í™œë™ (í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´):",
        "include_similar_label": "ìœ ì‚¬ ì‚¬ë¡€ í¬í•¨",
        "result_language_label": "ì¶œë ¥ ì–¸ì–´:",
        "run_button": "ìœ„í—˜ì„± í‰ê°€ ì‹¤í–‰",
        "no_activity_warning": "ì‘ì—…í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”.",
        "no_index_warning": "ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ë¥¼ êµ¬ì„±í•˜ì„¸ìš”.",
        "parsing_error": "ìœ„í—˜ì„± í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "parsing_error_improvement": "ê°œì„ ëŒ€ì±… ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
        "download_excel": "ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ"
    },
    "English": {
        "title": "Artificial Intelligence Risk Assessment",
        "tab_overview": "System Overview",
        "tab_assessment": "Assessment & Improvement",
        "overview_header": "LLM-based Risk Assessment System",
        "overview_text": (
            "Doosan Enerbility AI Risk Assessment is an automated program trained on "
            "on-demand risk-assessment reports and major-accident cases. "
            "Please review and validate all generated outputs before use."
        ),
        "features_title": "Features",
        "phase1_features": (
            "- Convert work activity to English before LLM input\n"
            "- Internally process in English, then output bilingual columns\n"
            "- Retrieve and display similar cases\n"
            "- Compute risk and generate improvement measures"
        ),
        "api_key_label": "Enter OpenAI API Key:",
        "dataset_label": "Select Dataset:",
        "load_data_btn": "Load Data & Build Index",
        "api_key_warning": "Please enter an API key.",
        "data_loading": "Loading data and building index...",
        "demo_limit_info": "Embedding only {max_texts} items for demo.",
        "data_load_success": "Data loaded & index built! ({max_texts} items processed)",
        "activity_label": "Work Activity (Korean or English):",
        "include_similar_label": "Include Similar Cases",
        "result_language_label": "Output Language:",
        "run_button": "Run Assessment",
        "no_activity_warning": "Please enter a work activity.",
        "no_index_warning": "Load data and build index first.",
        "parsing_error": "Cannot parse risk assessment output.",
        "parsing_error_improvement": "Cannot parse improvement measures output.",
        "download_excel": "ğŸ“¥ Download Excel"
    }
}
# -----------------------------------------------------------------------------  
# ------------------------ í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ -----------------------------  
# -----------------------------------------------------------------------------  
st.set_page_config(page_title="AI Risk Assessment", page_icon="ğŸ› ï¸", layout="wide")
ss = st.session_state

for key, default in {
    "language": "Korean",
    "index": None,
    "embeddings": None,
    "retriever_pool_df": None,
    "last_assessment": None
}.items():
    if key not in ss:
        ss[key] = default

# ì–¸ì–´ ì„ íƒ
col0, colLang = st.columns([6, 1])
with colLang:
    lang = st.selectbox(
        "ì–¸ì–´ ì„ íƒ",
        ["Korean", "English"],
        index=["Korean", "English"].index(ss.language),
        label_visibility="hidden"
    )
    ss.language = lang
texts = system_texts[ss.language]

# í—¤ë”
st.markdown(f"<h1 style='text-align:center; color:#1E88E5;'>{texts['title']}</h1>", unsafe_allow_html=True)

# íƒ­ ìƒì„±
tab1, tab2 = st.tabs([texts["tab_overview"], texts["tab_assessment"]])

# -----------------------------------------------------------------------------  
# ----------------------------- Overview íƒ­ -----------------------------------  
# -----------------------------------------------------------------------------  
with tab1:
    st.markdown(f"## {texts['overview_header']}")
    st.markdown(texts["overview_text"])
    st.markdown(f"### {texts['features_title']}")
    st.markdown(texts["phase1_features"])

# -----------------------------------------------------------------------------  
# --------------------------- Assessment íƒ­ -----------------------------------  
# -----------------------------------------------------------------------------  
with tab2:
    st.markdown(f"## {texts['tab_assessment']}")

    col_api, col_dataset = st.columns([2, 1])
    with col_api:
        api_key = st.text_input(texts["api_key_label"], type="password", key="api_key_all")
    with col_dataset:
        dataset_name = st.selectbox(
            texts["dataset_label"],
            ["ê±´ì¶•", "í† ëª©", "í”ŒëœíŠ¸"],
            key="dataset_select"
        )

    # ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ êµ¬ì„±
    if ss.retriever_pool_df is None or st.button(texts["load_data_btn"], type="primary"):
        if not api_key:
            st.warning(texts["api_key_warning"])
        else:
            with st.spinner(texts["data_loading"]):
                try:
                    df = load_data(dataset_name)  # load_dataëŠ” ì•„ë˜ì— ì •ì˜
                    if len(df) > 10:
                        train_df, _ = train_test_split(df, test_size=0.1, random_state=42)
                    else:
                        train_df = df.copy()

                    pool_df = train_df.copy()
                    pool_df["content_en"] = pool_df["work_sequence_en"].tolist()

                    to_embed = pool_df["content_en"].tolist()
                    max_texts = min(len(to_embed), 30)
                    st.info(texts["demo_limit_info"].format(max_texts=max_texts))

                    embeds = embed_texts_with_openai(to_embed[:max_texts], api_key)
                    vecs = np.array(embeds, dtype="float32")
                    dim = vecs.shape[1]
                    index = faiss.IndexFlatL2(dim)
                    index.add(vecs)

                    ss.index = index
                    ss.embeddings = vecs
                    ss.retriever_pool_df = pool_df.iloc[:max_texts]
                    st.success(texts["data_load_success"].format(max_texts=max_texts))

                except Exception as e:
                    st.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")

    st.divider()
    st.markdown("### ğŸ” í‰ê°€ ì‹¤í–‰")

    activity_input = st.text_input(texts["activity_label"], key="user_activity")

    col_opt1, col_opt2 = st.columns([2, 1])
    with col_opt1:
        include_similar = st.checkbox(texts["include_similar_label"], value=True)
    with col_opt2:
        output_lang = st.selectbox(texts["result_language_label"], ["Korean", "English"], index=["Korean", "English"].index(ss.language))

    run_button = st.button(texts["run_button"], type="primary", use_container_width=True)

if run_button:
    if not activity_input:
        st.warning(texts["no_activity_warning"])
    elif ss.index is None:
        st.warning(texts["no_index_warning"])
    else:
        with st.spinner("ì²˜ë¦¬ ì¤‘..."):
            try:
                # === 1) ì…ë ¥ í™œë™ì„ ì˜ì–´ë¡œ ë³€í™˜ ===
                translate_to_en_prompt = f"Translate the following work activity into clear English:\n\n{activity_input.strip()}"
                activity_en = generate_with_gpt(translate_to_en_prompt, api_key, "English")

                # === 2) ì„ë² ë”© ë° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ===
                q_emb_list = embed_texts_with_openai([activity_en], api_key)
                q_emb = q_emb_list[0]
                D, I = ss.index.search(np.array([q_emb], dtype="float32"), k=min(10, len(ss.retriever_pool_df)))
                sim_docs = ss.retriever_pool_df.iloc[I[0]]

                # === 3) ì£¼ìš” ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ (ì˜ì–´ ë‚´ë¶€ ì²˜ë¦¬) ===
                hazard_prompt_en = construct_prompt_hazard_en(sim_docs, activity_en)
                hazard_en = generate_with_gpt(hazard_prompt_en, api_key, "English")

                # === 4) ìœ„í—˜ë„ í‰ê°€ (ì˜ì–´ ë‚´ë¶€ ì²˜ë¦¬) ===
                risk_prompt_en = construct_prompt_risk_en(sim_docs, activity_en, hazard_en)
                risk_json = generate_with_gpt(risk_prompt_en, api_key, "English")
                parse_result = parse_gpt_output_risk(risk_json)
                if not parse_result:
                    st.error(texts["parsing_error"])
                    st.stop()
                freq_en, intensity_en, T_en = parse_result
                grade_en = determine_grade(T_en)

                # === 5) ê°œì„ ëŒ€ì±… ìƒì„± (ì˜ì–´ ë‚´ë¶€ ì²˜ë¦¬) ===
                improvement_prompt_en = construct_prompt_improvement_en(sim_docs, activity_en, hazard_en, freq_en, intensity_en, T_en)
                improvement_json = generate_with_gpt(improvement_prompt_en, api_key, "English")
                parsed_imp = parse_gpt_output_improvement(improvement_json)
                if not parsed_imp:
                    st.error(texts["parsing_error_improvement"])
                    st.stop()
                improvement_plan_en = parsed_imp["improvement_plan"]

                # === 6) ê²°ê³¼ë¥¼ ì¶œë ¥ ì–¸ì–´ë¡œ ë²ˆì—­ ===
                if output_lang == "Korean":
                    translate_columns = {
                        "activity": ("Work Sequence", "ì‘ì—…í™œë™ ë° ë‚´ìš©"),
                        "hazard": ("Hazarous Factors", "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥"),
                        "risk": ("Risk", "ìœ„í—˜ì„±"),
                        "improvement": ("Control Measures", "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ")
                    }
                    col_labels = {
                        "work": "ì‘ì—…í™œë™ ë° ë‚´ìš© Work Sequence",
                        "hazard": "ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥ Hazarous Factors",
                        "EHS": "EHS|",
                        "risk": "ìœ„í—˜ì„± Risk |",
                        "control": "ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ Control Measures |",
                        "in_charge": "ê°œì„ ë‹´ë‹¹ì In Charge",
                        "due_date": "ê°œì„ ì¼ì Correction Due Date"
                    }
                    # ë²ˆì—­
                    hazard_ko = generate_with_gpt(f"Translate to Korean:\n\n{hazard_en}", api_key, "Korean")
                    improvement_ko = generate_with_gpt(f"Translate to Korean preserving line breaks:\n\n{improvement_plan_en}", api_key, "Korean")
                    # ìœ„í—˜ë„ í•„ë“œ ìì²´ëŠ” ìˆ«ìì´ë¯€ë¡œ ë²ˆì—­ ë¶ˆí•„ìš”
                    activity_ko = activity_input.strip()
                else:
                    # English ì¶œë ¥ì´ë¼ë©´, ì˜ì–´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    col_labels = {
                        "work": "Work Sequence ì‘ì—…í™œë™ ë° ë‚´ìš©",
                        "hazard": "Hazarous Factors ìœ í•´ìœ„í—˜ìš”ì¸ ë° í™˜ê²½ì¸¡ë©´ ì˜í–¥",
                        "EHS": "EHS|",
                        "risk": "Risk | ìœ„í—˜ì„±",
                        "control": "Control Measures ê°œì„ ëŒ€ì±… ë° ì„¸ë¶€ê´€ë¦¬ë°©ì•ˆ |",
                        "in_charge": "In Charge ê°œì„ ë‹´ë‹¹ì",
                        "due_date": "Correction Due Date ê°œì„ ì¼ì"
                    }
                    activity_ko = activity_en
                    hazard_ko = hazard_en
                    improvement_ko = improvement_plan_en

                # === 7) í™”ë©´ ì¶œë ¥ ===
                st.markdown("## ğŸ“‹ ê²°ê³¼")
                # ì£¼ìš” ê²°ê³¼ ìš”ì•½
                col1, col2 = st.columns([3, 2])
                with col1:
                    st.markdown(f"**{col_labels['work']}**  \n{activity_ko}")
                    st.markdown(f"**{col_labels['hazard']}**  \n{hazard_ko}")
                with col2:
                    st.markdown(f"**{col_labels['risk']}**")
                    st.markdown(f"â€¢ ë¹ˆë„ (Likelihood): {freq_en}  \nâ€¢ ê°•ë„ (Severity): {intensity_en}  \nâ€¢ T-value: {T_en} (Grade {grade_en})")

                st.markdown("### ğŸ” ìœ ì‚¬ ì‚¬ë¡€")
                if include_similar:
                    for i, row in sim_docs.iterrows():
                        work_i = row["work_sequence_en"]
                        hazard_i = row["hazard_en"]
                        freq_i = int(row["frequency"])
                        int_i = int(row["severity"])
                        T_i = freq_i * int_i
                        grade_i = determine_grade(T_i)
                        plan_i = row["control_en"]
                        # ê²°ê³¼ ì–¸ì–´ì— ë§ì¶° ë²ˆì—­ (ê°„ë‹¨í•˜ê²Œ, í•œê¸€ ëª¨ë“œë¼ë©´ í•œ/ì˜ ë³‘ê¸° í˜•íƒœë¡œ ì¶œë ¥)
                        if output_lang == "Korean":
                            work_disp = row["work_sequence_ko"]
                            hazard_disp = row["hazard_ko"]
                            plan_disp = row["control_ko"]
                        else:
                            work_disp = work_i
                            hazard_disp = hazard_i
                            plan_disp = plan_i

                        with st.expander(f"ìœ ì‚¬ ì‚¬ë¡€ {i + 1}"):
                            st.write(f"**{col_labels['work']}**  \n{work_disp}")
                            st.write(f"**{col_labels['hazard']}**  \n{hazard_disp}")
                            st.write(f"**{col_labels['risk']}**  \nâ€¢ ë¹ˆë„: {freq_i}  \nâ€¢ ê°•ë„: {int_i}  \nâ€¢ T: {T_i} (Grade {grade_i})")
                            st.write(f"**{col_labels['control']}**  \n{plan_disp}")

                st.markdown("## ğŸ› ï¸ ê°œì„ ëŒ€ì±…")
                st.write(f"**{col_labels['control']}**  \n{improvement_ko}")
                st.write(f"**{col_labels['in_charge']}**  \n(ë¯¸ì§€ì •)")
                st.write(f"**{col_labels['due_date']}**  \n(ë¯¸ì§€ì •)")

                # === 8) ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ===
                def create_excel():
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                        # ì‹œíŠ¸ì— ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œì™€ ì´ë¦„ ì‚¬ìš©
                        columns = [
                            col_labels["work"],
                            col_labels["hazard"],
                            col_labels["EHS"],
                            col_labels["risk"],
                            col_labels["control"],
                            col_labels["in_charge"],
                            col_labels["due_date"]
                        ]
                        # ìœ„í—˜ì„± ì•„ë˜ rowì— ë¹ˆë„/ê°•ë„ ë³„ë„ í–‰ìœ¼ë¡œ êµ¬ì„±
                        risk_cell = (
                            f"ë¹ˆë„ likelihood: {freq_en}\n"
                            f"ê°•ë„ severity: {intensity_en}"
                        )
                        row_data = {
                            columns[0]: activity_ko,
                            columns[1]: hazard_ko,
                            columns[2]: "",  # EHS í•„ë“œëŠ” ë¹ˆ ê°’ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
                            columns[3]: risk_cell,
                            columns[4]: improvement_ko,
                            columns[5]: "",  # ë‹´ë‹¹ì ë¯¸ì§€ì •
                            columns[6]: ""   # ì¼ì ë¯¸ì§€ì •
                        }
                        df_out = pd.DataFrame([row_data], columns=columns)
                        df_out.to_excel(writer, sheet_name="Results", index=False)

                        # ìœ ì‚¬ì‚¬ë¡€ ì‹œíŠ¸
                        if include_similar and not sim_docs.empty:
                            sim_rows = []
                            for i, row in sim_docs.iterrows():
                                freq_i = int(row["frequency"])
                                int_i = int(row["severity"])
                                T_i = freq_i * int_i
                                grade_i = determine_grade(T_i)
                                if output_lang == "Korean":
                                    work_disp = row["work_sequence_ko"]
                                    hazard_disp = row["hazard_ko"]
                                    plan_disp = row["control_ko"]
                                else:
                                    work_disp = row["work_sequence_en"]
                                    hazard_disp = row["hazard_en"]
                                    plan_disp = row["control_en"]
                                risk_cell_i = f"ë¹ˆë„ likelihood: {freq_i}\nê°•ë„ severity: {int_i}"
                                sim_rows.append({
                                    columns[0]: work_disp,
                                    columns[1]: hazard_disp,
                                    columns[2]: "",
                                    columns[3]: risk_cell_i,
                                    columns[4]: plan_disp,
                                    columns[5]: "",
                                    columns[6]: ""
                                })
                            df_sim = pd.DataFrame(sim_rows, columns=columns)
                            df_sim.to_excel(writer, sheet_name="Similar Cases", index=False)

                    return output.getvalue()

                excel_bytes = create_excel()
                st.download_button(
                    label=texts["download_excel"],
                    data=excel_bytes,
                    file_name="risk_assessment.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.stop()

# -----------------------------------------------------------------------------  
# --------------------------- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ --------------------------------  
# -----------------------------------------------------------------------------  

def determine_grade(value: int):
    """ìœ„í—˜ë„ ë“±ê¸‰ ê³„ì‚° (ì˜ì–´ ë‚´ë¶€ ê¸°ì¤€)"""
    if 16 <= value <= 25:
        return 'A'
    if 10 <= value <= 15:
        return 'B'
    if 5 <= value <= 9:
        return 'C'
    if 3 <= value <= 4:
        return 'D'
    if 1 <= value <= 2:
        return 'E'
    return 'Unknown'

@st.cache_data(show_spinner=False)
def load_data(selected_dataset_name: str):
    """
    ë°ì´í„° ë¡œë“œí•¨ìˆ˜.
    ê° ì»¬ëŸ¼ì„ ì˜ì–´/í•œêµ­ì–´ ì–‘ìª½ìœ¼ë¡œ ì¤€ë¹„í•´ ë‘¡ë‹ˆë‹¤.
    """
    # ì‹¤ì œë¡œëŠ” xlsxë¥¼ ë¡œë“œí•˜ê² ì§€ë§Œ, ì˜ˆì‹œìš©ìœ¼ë¡œ ê°„ë‹¨íˆ ìƒ˜í”Œ ìƒì„±
    data = {
        "work_sequence_en": [
            "Unload steel beams using forklift at temporary site storage",
            "Install concrete/CMU blocks",
            "Excavation and backfilling work",
            "Exterior wall work on elevated platform",
            "Welding operations"
        ],
        "work_sequence_ko": [
            "ì„ì‹œ í˜„ì¥ ì €ì¥ì†Œì—ì„œ í¬í¬ë¦¬í”„íŠ¸ë¡œ ì² ê³¨ êµ¬ì¡°ì¬ í•˜ì—­ì‘ì—…",
            "ì½˜í¬ë¦¬íŠ¸/CMU ë¸”ë¡ ì„¤ì¹˜ ì‘ì—…",
            "êµ´ì°© ë° ë˜ë©”ìš°ê¸° ì‘ì—…",
            "ê³ ì†Œ ì‘ì—…ëŒ€ë¥¼ ì´ìš©í•œ ì™¸ë²½ ì‘ì—…",
            "ìš©ì ‘ ì‘ì—…"
        ],
        "hazard_en": [
            "Falling loads due to multiple lifting",
            "Fall due to insufficient work platform",
            "Burial from excavation wall collapse",
            "Fall due to missing safety harness",
            "Welding fumes and fire risk"
        ],
        "hazard_ko": [
            "ë‹¤ì¤‘ ì¸ì–‘ìœ¼ë¡œ ì¸í•œ ì ì¬ë¬¼ ë‚™í•˜",
            "ë¶ˆì¶©ë¶„í•œ ì‘ì—… ë°œíŒìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "êµ´ì°©ë²½ ë¶•ê´´ë¡œ ì¸í•œ ë§¤ëª°",
            "ì•ˆì „ëŒ€ ë¯¸ì°©ìš©ìœ¼ë¡œ ì¸í•œ ì¶”ë½",
            "ìš©ì ‘ í„ ë° í™”ì¬ ìœ„í—˜"
        ],
        "frequency": [3, 3, 2, 4, 2],
        "severity": [5, 4, 5, 5, 3],
        "control_en": [
            "1) Do not lift multiple steel beams together. 2) Manage dimensions and weights.",
            "1) Install missing planks on scaffolding. 2) Equip safety harness anchorage.",
            "1) Maintain proper slope. 2) Reinforce excavation walls. 3) Inspect ground regularly.",
            "1) Enforce safety harness use. 2) Conduct pre-work safety training. 3) Install fall arrest nets.",
            "1) Provide proper ventilation. 2) Implement fire prevention measures. 3) Mandate PPE."
        ],
        "control_ko": [
            "1) ë‹¤ìˆ˜ì˜ ì² ê³¨ì¬ë¥¼ í•¨ê»˜ ì¸ì–‘í•˜ì§€ ì•Šë„ë¡ ê´€ë¦¬\n2) ì¹˜ìˆ˜ ë° ì¤‘ëŸ‰ ê´€ë¦¬",
            "1) ë¹„ê³„ì— ëˆ„ë½ëœ ëª©íŒ ì„¤ì¹˜\n2) ì•ˆì „ëŒ€ ë¶€ì°© ì„¤ë¹„ ì‚¬ìš©",
            "1) ì ì ˆí•œ ì‚¬ë©´ ìœ ì§€\n2) êµ´ì°© ë²½ë©´ ë³´ê°•\n3) ì •ê¸°ì  ì§€ë°˜ ì ê²€",
            "1) ì•ˆì „ëŒ€ ì°©ìš© ì˜ë¬´í™”\n2) ì‘ì—… ì „ ì•ˆì „êµìœ¡ ì‹¤ì‹œ\n3) ì¶”ë½ ë°©ì§€ë§ ì„¤ì¹˜",
            "1) ì ì ˆí•œ í™˜ê¸°\n2) í™”ì¬ ì˜ˆë°© ì¡°ì¹˜\n3) ë³´í˜¸êµ¬ ì°©ìš©"
        ]
    }
    df = pd.DataFrame(data)
    df["T"] = df["frequency"] * df["severity"]
    df["grade"] = df["T"].apply(determine_grade)
    return df

def embed_texts_with_openai(texts, api_key, model="text-embedding-3-large"):
    """
    OpenAI Embedding í˜¸ì¶œ (ì˜ì–´ ë‚´ë¶€ ì²˜ë¦¬)
    """
    if not api_key:
        return []

    openai.api_key = api_key
    embeddings = []
    batch_size = 10
    for i in range(0, len(texts), batch_size):
        batch = texts[i : i + batch_size]
        processed = [t.replace("\n", " ").strip() for t in batch]
        try:
            resp = openai.Embedding.create(model=model, input=processed)
            embeddings.extend([item["embedding"] for item in resp["data"]])
        except Exception as e:
            st.error(f"Embedding error: {e}")
            embeddings.extend([[0] * 1536] * len(batch))
    return embeddings

def generate_with_gpt(prompt, api_key, language, model="gpt-4o", max_retries=3):
    """
    OpenAI ChatCompletion í˜¸ì¶œ
    """
    if not api_key:
        return ""
    openai.api_key = api_key

    sys_prompt = "You are a construction site risk assessment expert. Provide clear, practical English responses."
    # í•œêµ­ì–´ë‚˜ ì¤‘êµ­ì–´ ì¶œë ¥ ì‹œ, ë‚´ë¶€ì—ì„œ ë²ˆì—­í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ì—¬ê¸°ì„œëŠ” ì˜ì–´ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

    for attempt in range(max_retries):
        try:
            resp = openai.ChatCompletion.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=512,
                top_p=0.9
            )
            return resp["choices"][0]["message"]["content"].strip()
        except Exception as e:
            if attempt == max_retries - 1:
                st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                return ""
            else:
                continue

def construct_prompt_hazard_en(sim_docs, activity_en: str):
    """
    ì£¼ìš” ìœ í•´ìœ„í—˜ìš”ì¸ ì˜ˆì¸¡ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸
    """
    intro = "Here are examples of work activities and associated hazards:\n\n"
    example_fmt = "Case {i}:\n- Work Activity: {act}\n- Hazard: {haz}\n\n"
    for i, row in enumerate(sim_docs.itertuples(), start=1):
        intro += example_fmt.format(i=i, act=row.work_sequence_en, haz=row.hazard_en)
        if i >= 5:
            break
    query = f"Based on the above cases, predict the main hazards for the following work activity:\n\nWork Activity: {activity_en}\n\nHazard:"
    return intro + query

def construct_prompt_risk_en(sim_docs, activity_en: str, hazard_en: str):
    """
    ìœ„í—˜ë„ í‰ê°€ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸
    """
    intro = (
        "Risk assessment criteria:\n"
        "- Frequency (1-5): 1=Very Rare, 2=Rare, 3=Occasional, 4=Frequent, 5=Very Frequent\n"
        "- Severity (1-5): 1=Minor Injury, 2=Light Injury, 3=Moderate Injury, 4=Serious Injury, 5=Fatality\n"
        "- T-value = Frequency x Severity\n\nReference cases:\n\n"
    )
    example_fmt = "Case {i}:\nInput: {inp}\nAssessment: Frequency={freq}, Severity={sev}, T-value={tval}\n\n"
    for i, row in enumerate(sim_docs.itertuples(), start=1):
        inp = f"{row.work_sequence_en} - {row.hazard_en}"
        freq = row.frequency
        sev = row.severity
        tval = freq * sev
        intro += example_fmt.format(i=i, inp=inp, freq=freq, sev=sev, tval=tval)
        if i >= 3:
            break
    query = (
        f"{intro}Please assess the following:\n\n"
        f"Work Activity: {activity_en}\nHazard: {hazard_en}\n\n"
        'Respond in JSON as: {"frequency": number, "severity": number, "T": number}'
    )
    return query

def parse_gpt_output_risk(gpt_output: str):
    """
    GPT ìœ„í—˜ë„ JSON íŒŒì‹±
    """
    pattern = r'\{"frequency":\s*([1-5]),\s*"severity":\s*([1-5]),\s*"T":\s*([0-9]+)\}'
    match = re.search(pattern, gpt_output)
    if match:
        return int(match.group(1)), int(match.group(2)), int(match.group(3))
    return None

def construct_prompt_improvement_en(sim_docs, activity_en: str, hazard_en: str, freq: int, sev: int, T: int):
    """
    ê°œì„ ëŒ€ì±… ìƒì„±ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸
    """
    intro = "Provide at least 3 practical improvement measures (numbered) that reduce both frequency and severity.\n\n"
    example_fmt = (
        "Example {i}:\n"
        "Input Work Activity: {act}\n"
        "Input Hazard: {haz}\n"
        "Original Frequency: {freq}, Severity: {sev}, T-value: {tval}\n"
        "Output:\n{{\n"
        '  "improvement_plan": "{plan}",\n'
        "  \"improved_frequency\": {ifreq},\n"
        "  \"improved_severity\": {isev},\n"
        "  \"improved_T\": {itval},\n"
        '  "reduction_rate": {rrr}\n'
        "}}\n\n"
    )
    examples = ""
    count = 0
    for row in sim_docs.itertuples():
        orig_freq = row.frequency
        orig_sev = row.severity
        orig_t = orig_freq * orig_sev
        imp_freq = max(1, orig_freq - 1)
        imp_sev = max(1, orig_sev - 1)
        imp_t = imp_freq * imp_sev
        plan = row.control_en.replace("\n", " ")
        rrr = round(((orig_t - imp_t) / orig_t) * 100, 2) if orig_t else 0.0
        examples += example_fmt.format(
            i=count + 1,
            act=row.work_sequence_en,
            haz=row.hazard_en,
            freq=orig_freq,
            sev=orig_sev,
            tval=orig_t,
            plan=plan,
            ifreq=imp_freq,
            isev=imp_sev,
            itval=imp_t,
            rrr=rrr
        )
        count += 1
        if count >= 2:
            break

    query = (
        f"{examples}"
        f"Now assess for:\n\n"
        f"Work Activity: {activity_en}\nHazard: {hazard_en}\n"
        f"Original Frequency: {freq}, Severity: {sev}, T: {T}\n\n"
        'Output in JSON as: {"improvement_plan": "...", "improved_frequency": number, '
        '"improved_severity": number, "improved_T": number, "reduction_rate": number}'
    )
    return query

def parse_gpt_output_improvement(gpt_output: str):
    """
    ê°œì„ ëŒ€ì±… JSON íŒŒì‹±
    """
    # JSON ì¶”ì¶œ
    match = re.search(r'\{.*\}', gpt_output, re.DOTALL)
    if not match:
        return None
    try:
        import json
        data = json.loads(match.group(0))
        return {
            "improvement_plan": data.get("improvement_plan", "").replace("\\n", "\n"),
            "improved_frequency": data.get("improved_frequency", 1),
            "improved_severity": data.get("improved_severity", 1),
            "improved_T": data.get("improved_T", data.get("improved_frequency", 1) * data.get("improved_severity", 1)),
            "reduction_rate": data.get("reduction_rate", 0.0)
        }
    except json.JSONDecodeError:
        return None
